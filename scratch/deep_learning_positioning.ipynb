{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from src.utils.data_loader import LoadDataSet\n",
    "from src.positioning.toa import toa_positioning\n",
    "from src.utils.models import DenseNet\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the feature data is (150000, 4, 6)\n",
      "The shape of the target data is: (150000, 3)\n",
      "Random shuffling state: 3\n",
      "The shape of the training data is: (75000, 4, 6)\n",
      "The shape of the testing data is: (75000, 4, 6)\n",
      "new training shape: (75000, 24)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "dir_name = \"Omni Spatial and Temporal Data/\"\n",
    "min_delay_dataset = LoadDataSet(dir_name)\n",
    "print(f\"Random shuffling state: {min_delay_dataset.rnd_state}\")\n",
    "scaler = StandardScaler()\n",
    "train_set, test_set = min_delay_dataset.get_datasets(scale=True, scaler=scaler)\n",
    "train_dl = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "test_dl = torch.utils.data.DataLoader(test_set, batch_size=1024, shuffle=False)\n",
    "\n",
    "data_dir = os.path.abspath(\"../../\"+dir_name)\n",
    "toa_tensor_def = sio.loadmat(os.path.join(data_dir, \"all_toa_tensor\"))['toa_tensor']\n",
    "\n",
    "bool_m = toa_tensor_def == min_delay_dataset.toa_tensor_compare\n",
    "print(np.all(bool_m))  # sanity check that the data corresponding to the min_toa path is selected correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─ModuleList: 1                          []                        --\n",
      "|    └─Linear: 2-1                       [-1, 1, 48]               1,200\n",
      "|    └─Linear: 2-2                       [-1, 1, 48]               2,352\n",
      "|    └─Linear: 2-3                       [-1, 1, 48]               2,352\n",
      "|    └─Linear: 2-4                       [-1, 1, 3]                147\n",
      "==========================================================================================\n",
      "Total params: 6,051\n",
      "Trainable params: 6,051\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.01\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.02\n",
      "Estimated Total Size (MB): 0.02\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "in_dim = min_delay_dataset.input_shape\n",
    "out_dim = min_delay_dataset.output_shape\n",
    "layers_dim = [in_dim, in_dim*2, in_dim*2, in_dim*2, out_dim]\n",
    "dnn_model = DenseNet(layers_dim)\n",
    "dnn_model.to(device)\n",
    "summary(dnn_model, input_data=(1,in_dim), device=device, depth=len(layers_dim))\n",
    "\n",
    "Loss = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(dnn_model.parameters(), amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model=None, n_epochs=200):\n",
    "    # train the model\n",
    "    train_loss_history = []\n",
    "    test_loss_history = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        total_train_loss = 0\n",
    "        total_test_loss = 0\n",
    "        total_train = 0\n",
    "        total_test = 0\n",
    "\n",
    "        # training loop (iterate over the training set)\n",
    "        for x, y in train_dl:\n",
    "            x_train, y_train = x.float().to(device), y.float().to(device)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            train_output = model(x_train) # network output\n",
    "            loss = Loss(train_output, y_train) # compute loss\n",
    "            loss.backward() # compute gradient with respect to trainable parameters\n",
    "            optimizer.step() # weights update\n",
    "            total_train_loss += loss.item()\n",
    "            total_train += 1\n",
    "\n",
    "        print(f\"Training at epoch: {epoch+1} ended, Loss: {total_train_loss/total_train}\")\n",
    "        # append training loss for the epoch\n",
    "        train_loss_history.append(total_train_loss/total_train)\n",
    "\n",
    "        # testing every 10 training epochs\n",
    "        if ((epoch+1) % 10) == 0:\n",
    "            for x, y in test_dl:\n",
    "                x_test, y_test = x.float().to(device), y.float().to(device)\n",
    "                test_output = model(x_test)\n",
    "                loss = Loss(test_output, y_test)\n",
    "                total_test_loss += loss.item()\n",
    "                total_test += 1\n",
    "            print(f\"--> Testing at epoch: {epoch+1} ended, Loss: {total_test_loss/total_test}\")\n",
    "            # append test loss for the epoch\n",
    "            test_loss_history.append(total_test_loss/total_test)\n",
    "\n",
    "    print('Finished Training')\n",
    "    return train_loss_history, test_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training at epoch: 1 ended, Loss: 1787.1791803983292\n",
      "Training at epoch: 2 ended, Loss: 34.86498188057043\n",
      "Training at epoch: 3 ended, Loss: 21.221980891130077\n",
      "Training at epoch: 4 ended, Loss: 16.09716982778429\n",
      "Training at epoch: 5 ended, Loss: 13.19733549832484\n",
      "Training at epoch: 6 ended, Loss: 11.188080008831447\n",
      "Training at epoch: 7 ended, Loss: 9.768222453193452\n",
      "Training at epoch: 8 ended, Loss: 8.754940028076692\n",
      "Training at epoch: 9 ended, Loss: 8.008660907122868\n",
      "Training at epoch: 10 ended, Loss: 7.392791446717933\n",
      "--> Testing at epoch: 10 ended, Loss: 7.098536426956589\n",
      "Training at epoch: 11 ended, Loss: 6.9026432726977225\n",
      "Training at epoch: 12 ended, Loss: 6.503839444127506\n",
      "Training at epoch: 13 ended, Loss: 6.131293840070633\n",
      "Training at epoch: 14 ended, Loss: 5.822884238773239\n",
      "Training at epoch: 15 ended, Loss: 5.552465665157338\n",
      "Training at epoch: 16 ended, Loss: 5.316933146668376\n",
      "Training at epoch: 17 ended, Loss: 5.0984093011341\n",
      "Training at epoch: 18 ended, Loss: 4.910526392479195\n",
      "Training at epoch: 19 ended, Loss: 4.752360673030082\n",
      "Training at epoch: 20 ended, Loss: 4.551933230190147\n",
      "--> Testing at epoch: 20 ended, Loss: 4.518505431510307\n",
      "Training at epoch: 21 ended, Loss: 4.395655498518878\n",
      "Training at epoch: 22 ended, Loss: 4.258200594894715\n",
      "Training at epoch: 23 ended, Loss: 4.12811432328749\n",
      "Training at epoch: 24 ended, Loss: 4.012047015791664\n",
      "Training at epoch: 25 ended, Loss: 3.9023729056730203\n",
      "Training at epoch: 26 ended, Loss: 3.7903612116955654\n",
      "Training at epoch: 27 ended, Loss: 3.698763767149261\n",
      "Training at epoch: 28 ended, Loss: 3.601256020341717\n",
      "Training at epoch: 29 ended, Loss: 3.4936927299145544\n",
      "Training at epoch: 30 ended, Loss: 3.4149360182240556\n",
      "--> Testing at epoch: 30 ended, Loss: 3.5246510634551176\n",
      "Training at epoch: 31 ended, Loss: 3.35453390297019\n",
      "Training at epoch: 32 ended, Loss: 3.3003441714030077\n",
      "Training at epoch: 33 ended, Loss: 3.2261245967025642\n",
      "Training at epoch: 34 ended, Loss: 3.1742354686858305\n",
      "Training at epoch: 35 ended, Loss: 3.103023918491785\n",
      "Training at epoch: 36 ended, Loss: 3.0567550796239855\n",
      "Training at epoch: 37 ended, Loss: 2.9916962816701003\n",
      "Training at epoch: 38 ended, Loss: 2.9476564827124005\n",
      "Training at epoch: 39 ended, Loss: 2.8832846677038857\n",
      "Training at epoch: 40 ended, Loss: 2.8507892450018955\n",
      "--> Testing at epoch: 40 ended, Loss: 2.8713942315127396\n",
      "Training at epoch: 41 ended, Loss: 2.8075257874135273\n",
      "Training at epoch: 42 ended, Loss: 2.7603690904968836\n",
      "Training at epoch: 43 ended, Loss: 2.71604219035789\n",
      "Training at epoch: 44 ended, Loss: 2.674700536901837\n",
      "Training at epoch: 45 ended, Loss: 2.644351475500206\n",
      "Training at epoch: 46 ended, Loss: 2.630244233299034\n",
      "Training at epoch: 47 ended, Loss: 2.582057523150188\n",
      "Training at epoch: 48 ended, Loss: 2.5609917305791337\n",
      "Training at epoch: 49 ended, Loss: 2.5208747670969864\n",
      "Training at epoch: 50 ended, Loss: 2.48537253115466\n",
      "--> Testing at epoch: 50 ended, Loss: 2.6010262354000195\n",
      "Training at epoch: 51 ended, Loss: 2.459632110473646\n",
      "Training at epoch: 52 ended, Loss: 2.4446107492918854\n",
      "Training at epoch: 53 ended, Loss: 2.405682536801991\n",
      "Training at epoch: 54 ended, Loss: 2.3781691166117738\n",
      "Training at epoch: 55 ended, Loss: 2.3377542931828077\n",
      "Training at epoch: 56 ended, Loss: 2.310415325616407\n",
      "Training at epoch: 57 ended, Loss: 2.275847880074392\n",
      "Training at epoch: 58 ended, Loss: 2.260894217487081\n",
      "Training at epoch: 59 ended, Loss: 2.2344148321388726\n",
      "Training at epoch: 60 ended, Loss: 2.2055200825194246\n",
      "--> Testing at epoch: 60 ended, Loss: 2.2904449105262756\n",
      "Training at epoch: 61 ended, Loss: 2.1812341409280847\n",
      "Training at epoch: 62 ended, Loss: 2.1550567841646817\n",
      "Training at epoch: 63 ended, Loss: 2.1344523010680088\n",
      "Training at epoch: 64 ended, Loss: 2.113580155036962\n",
      "Training at epoch: 65 ended, Loss: 2.089448971451346\n",
      "Training at epoch: 66 ended, Loss: 2.0681460205427413\n",
      "Training at epoch: 67 ended, Loss: 2.057474452023213\n",
      "Training at epoch: 68 ended, Loss: 2.0327917919351175\n",
      "Training at epoch: 69 ended, Loss: 2.0135979804340685\n",
      "Training at epoch: 70 ended, Loss: 1.999782296654104\n",
      "--> Testing at epoch: 70 ended, Loss: 2.0520209541191927\n",
      "Training at epoch: 71 ended, Loss: 1.9737948646622714\n",
      "Training at epoch: 72 ended, Loss: 1.9523531430537586\n",
      "Training at epoch: 73 ended, Loss: 1.941739651275983\n",
      "Training at epoch: 74 ended, Loss: 1.9181329357618968\n",
      "Training at epoch: 75 ended, Loss: 1.9038648490282253\n",
      "Training at epoch: 76 ended, Loss: 1.8868863105570497\n",
      "Training at epoch: 77 ended, Loss: 1.880267131577783\n",
      "Training at epoch: 78 ended, Loss: 1.8683344106167656\n",
      "Training at epoch: 79 ended, Loss: 1.843259845310409\n",
      "Training at epoch: 80 ended, Loss: 1.8370240976074044\n",
      "--> Testing at epoch: 80 ended, Loss: 1.952223492635263\n",
      "Training at epoch: 81 ended, Loss: 1.8143569447557877\n",
      "Training at epoch: 82 ended, Loss: 1.7985567451921949\n",
      "Training at epoch: 83 ended, Loss: 1.7817965759657348\n",
      "Training at epoch: 84 ended, Loss: 1.77314592560963\n",
      "Training at epoch: 85 ended, Loss: 1.7515357856737266\n",
      "Training at epoch: 86 ended, Loss: 1.749067863061668\n",
      "Training at epoch: 87 ended, Loss: 1.7284835892834354\n",
      "Training at epoch: 88 ended, Loss: 1.7240326839273497\n",
      "Training at epoch: 89 ended, Loss: 1.7014738592373226\n",
      "Training at epoch: 90 ended, Loss: 1.685607926697556\n",
      "--> Testing at epoch: 90 ended, Loss: 1.740260217640851\n",
      "Training at epoch: 91 ended, Loss: 1.6791490055747813\n",
      "Training at epoch: 92 ended, Loss: 1.6632590516638837\n",
      "Training at epoch: 93 ended, Loss: 1.6529956147862985\n",
      "Training at epoch: 94 ended, Loss: 1.6329015271528386\n",
      "Training at epoch: 95 ended, Loss: 1.6271305652430643\n",
      "Training at epoch: 96 ended, Loss: 1.606848344422953\n",
      "Training at epoch: 97 ended, Loss: 1.6026126384073964\n",
      "Training at epoch: 98 ended, Loss: 1.5943118290969334\n",
      "Training at epoch: 99 ended, Loss: 1.579976388796808\n",
      "Training at epoch: 100 ended, Loss: 1.568321670446888\n",
      "--> Testing at epoch: 100 ended, Loss: 1.7045790536983594\n",
      "Training at epoch: 101 ended, Loss: 1.5604301764671102\n",
      "Training at epoch: 102 ended, Loss: 1.5530481575876982\n",
      "Training at epoch: 103 ended, Loss: 1.534819764145507\n",
      "Training at epoch: 104 ended, Loss: 1.526216054519594\n",
      "Training at epoch: 105 ended, Loss: 1.5195573992238924\n",
      "Training at epoch: 106 ended, Loss: 1.49933151432882\n",
      "Training at epoch: 107 ended, Loss: 1.489886866189514\n",
      "Training at epoch: 108 ended, Loss: 1.487002451837368\n",
      "Training at epoch: 109 ended, Loss: 1.4822429718112782\n",
      "Training at epoch: 110 ended, Loss: 1.4674269344288944\n",
      "--> Testing at epoch: 110 ended, Loss: 1.5054487167177975\n",
      "Training at epoch: 111 ended, Loss: 1.468395485876465\n",
      "Training at epoch: 112 ended, Loss: 1.446468632388542\n",
      "Training at epoch: 113 ended, Loss: 1.4384008691782837\n",
      "Training at epoch: 114 ended, Loss: 1.4384613092289442\n",
      "Training at epoch: 115 ended, Loss: 1.4226375325573386\n",
      "Training at epoch: 116 ended, Loss: 1.4238789469486821\n",
      "Training at epoch: 117 ended, Loss: 1.410551012439634\n",
      "Training at epoch: 118 ended, Loss: 1.4145094811178922\n",
      "Training at epoch: 119 ended, Loss: 1.397234002910173\n",
      "Training at epoch: 120 ended, Loss: 1.3939238192685228\n",
      "--> Testing at epoch: 120 ended, Loss: 1.4613070536304165\n",
      "Training at epoch: 121 ended, Loss: 1.3876194908146973\n",
      "Training at epoch: 122 ended, Loss: 1.3776278830428985\n",
      "Training at epoch: 123 ended, Loss: 1.369282832642566\n",
      "Training at epoch: 124 ended, Loss: 1.3623417340753021\n",
      "Training at epoch: 125 ended, Loss: 1.3563880540633975\n",
      "Training at epoch: 126 ended, Loss: 1.3507098407824698\n",
      "Training at epoch: 127 ended, Loss: 1.3456065455521535\n",
      "Training at epoch: 128 ended, Loss: 1.3418912801115992\n",
      "Training at epoch: 129 ended, Loss: 1.3346736874088085\n",
      "Training at epoch: 130 ended, Loss: 1.3206811107491674\n",
      "--> Testing at epoch: 130 ended, Loss: 1.4094660475447371\n",
      "Training at epoch: 131 ended, Loss: 1.3182983670269264\n",
      "Training at epoch: 132 ended, Loss: 1.3140708308272802\n",
      "Training at epoch: 133 ended, Loss: 1.3080335900746922\n",
      "Training at epoch: 134 ended, Loss: 1.3083478073516397\n",
      "Training at epoch: 135 ended, Loss: 1.2997779890055745\n",
      "Training at epoch: 136 ended, Loss: 1.2931553816785177\n",
      "Training at epoch: 137 ended, Loss: 1.2907944570959835\n",
      "Training at epoch: 138 ended, Loss: 1.2884559051575515\n",
      "Training at epoch: 139 ended, Loss: 1.2806131581016165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training at epoch: 140 ended, Loss: 1.2749996030823973\n",
      "--> Testing at epoch: 140 ended, Loss: 1.3198543142628025\n",
      "Training at epoch: 141 ended, Loss: 1.269213841255411\n",
      "Training at epoch: 142 ended, Loss: 1.2567794246808863\n",
      "Training at epoch: 143 ended, Loss: 1.2660835782390203\n",
      "Training at epoch: 144 ended, Loss: 1.2550562516922836\n",
      "Training at epoch: 145 ended, Loss: 1.249174794671682\n",
      "Training at epoch: 146 ended, Loss: 1.252248502924479\n",
      "Training at epoch: 147 ended, Loss: 1.2354972926779004\n",
      "Training at epoch: 148 ended, Loss: 1.2370680376455034\n",
      "Training at epoch: 149 ended, Loss: 1.2264933173032955\n",
      "Training at epoch: 150 ended, Loss: 1.2287035899383012\n",
      "--> Testing at epoch: 150 ended, Loss: 1.2841136624684204\n",
      "Training at epoch: 151 ended, Loss: 1.2221902371495454\n",
      "Training at epoch: 152 ended, Loss: 1.2152559268113492\n",
      "Training at epoch: 153 ended, Loss: 1.2144416476577622\n",
      "Training at epoch: 154 ended, Loss: 1.218689309032602\n",
      "Training at epoch: 155 ended, Loss: 1.198615031259662\n",
      "Training at epoch: 156 ended, Loss: 1.204228288153943\n",
      "Training at epoch: 157 ended, Loss: 1.1973045367124544\n",
      "Training at epoch: 158 ended, Loss: 1.1965434638967694\n",
      "Training at epoch: 159 ended, Loss: 1.1942648648605412\n",
      "Training at epoch: 160 ended, Loss: 1.1850771264716626\n",
      "--> Testing at epoch: 160 ended, Loss: 1.3019567695823875\n",
      "Training at epoch: 161 ended, Loss: 1.1828057463233177\n",
      "Training at epoch: 162 ended, Loss: 1.1758853812851393\n",
      "Training at epoch: 163 ended, Loss: 1.1753077226922983\n",
      "Training at epoch: 164 ended, Loss: 1.1713419287201692\n",
      "Training at epoch: 165 ended, Loss: 1.1668771554178752\n",
      "Training at epoch: 166 ended, Loss: 1.1742815671825573\n",
      "Training at epoch: 167 ended, Loss: 1.1584466041507575\n",
      "Training at epoch: 168 ended, Loss: 1.1610144312420396\n",
      "Training at epoch: 169 ended, Loss: 1.155548191721529\n",
      "Training at epoch: 170 ended, Loss: 1.1539582116433988\n",
      "--> Testing at epoch: 170 ended, Loss: 1.3260570764541626\n",
      "Training at epoch: 171 ended, Loss: 1.1447434864887402\n",
      "Training at epoch: 172 ended, Loss: 1.1466670063333808\n",
      "Training at epoch: 173 ended, Loss: 1.1372475615693034\n",
      "Training at epoch: 174 ended, Loss: 1.1383594731600009\n",
      "Training at epoch: 175 ended, Loss: 1.1388325765983236\n",
      "Training at epoch: 176 ended, Loss: 1.1265021200218706\n",
      "Training at epoch: 177 ended, Loss: 1.1250123998894017\n",
      "Training at epoch: 178 ended, Loss: 1.1183928893465842\n",
      "Training at epoch: 179 ended, Loss: 1.1176677823524426\n",
      "Training at epoch: 180 ended, Loss: 1.1147974941485368\n",
      "--> Testing at epoch: 180 ended, Loss: 1.1707460405053318\n",
      "Training at epoch: 181 ended, Loss: 1.109560538935173\n",
      "Training at epoch: 182 ended, Loss: 1.1095598702519216\n",
      "Training at epoch: 183 ended, Loss: 1.1080814579266534\n",
      "Training at epoch: 184 ended, Loss: 1.1007860402732375\n",
      "Training at epoch: 185 ended, Loss: 1.1011211385143087\n",
      "Training at epoch: 186 ended, Loss: 1.0906517881734785\n",
      "Training at epoch: 187 ended, Loss: 1.092085293052343\n",
      "Training at epoch: 188 ended, Loss: 1.0930440395422052\n",
      "Training at epoch: 189 ended, Loss: 1.0839964951747716\n",
      "Training at epoch: 190 ended, Loss: 1.0836396481930803\n",
      "--> Testing at epoch: 190 ended, Loss: 1.16506562845127\n",
      "Training at epoch: 191 ended, Loss: 1.086847766826991\n",
      "Training at epoch: 192 ended, Loss: 1.0771454805349328\n",
      "Training at epoch: 193 ended, Loss: 1.0701848411417658\n",
      "Training at epoch: 194 ended, Loss: 1.069172205447947\n",
      "Training at epoch: 195 ended, Loss: 1.0620539696957063\n",
      "Training at epoch: 196 ended, Loss: 1.0667103416200587\n",
      "Training at epoch: 197 ended, Loss: 1.0605865986858822\n",
      "Training at epoch: 198 ended, Loss: 1.0606170063234432\n",
      "Training at epoch: 199 ended, Loss: 1.0570172718996278\n",
      "Training at epoch: 200 ended, Loss: 1.0498743959457801\n",
      "--> Testing at epoch: 200 ended, Loss: 1.1502813519658268\n",
      "Training at epoch: 201 ended, Loss: 1.0537323452303433\n",
      "Training at epoch: 202 ended, Loss: 1.0500141241724377\n",
      "Training at epoch: 203 ended, Loss: 1.0502003977526577\n",
      "Training at epoch: 204 ended, Loss: 1.0419732922046054\n",
      "Training at epoch: 205 ended, Loss: 1.038273400706541\n",
      "Training at epoch: 206 ended, Loss: 1.0356740811045055\n",
      "Training at epoch: 207 ended, Loss: 1.038464622095278\n",
      "Training at epoch: 208 ended, Loss: 1.0279855171263015\n",
      "Training at epoch: 209 ended, Loss: 1.032300024315613\n",
      "Training at epoch: 210 ended, Loss: 1.0301304913472398\n",
      "--> Testing at epoch: 210 ended, Loss: 1.1050168420817401\n",
      "Training at epoch: 211 ended, Loss: 1.0212437097911338\n",
      "Training at epoch: 212 ended, Loss: 1.0191637898442485\n",
      "Training at epoch: 213 ended, Loss: 1.019785451146523\n",
      "Training at epoch: 214 ended, Loss: 1.0150997268843367\n",
      "Training at epoch: 215 ended, Loss: 1.0179551053703237\n",
      "Training at epoch: 216 ended, Loss: 1.0119630180477281\n",
      "Training at epoch: 217 ended, Loss: 1.0088333364181958\n",
      "Training at epoch: 218 ended, Loss: 1.003664961319938\n",
      "Training at epoch: 219 ended, Loss: 1.009078134792148\n",
      "Training at epoch: 220 ended, Loss: 1.0013402419819564\n",
      "--> Testing at epoch: 220 ended, Loss: 1.0675612591408394\n",
      "Training at epoch: 221 ended, Loss: 0.9975780210117635\n",
      "Training at epoch: 222 ended, Loss: 0.9970978229297103\n",
      "Training at epoch: 223 ended, Loss: 0.9895283472962652\n",
      "Training at epoch: 224 ended, Loss: 0.9928462877424605\n",
      "Training at epoch: 225 ended, Loss: 0.993309171749565\n",
      "Training at epoch: 226 ended, Loss: 0.9788610572217877\n",
      "Training at epoch: 227 ended, Loss: 0.9848885068688986\n",
      "Training at epoch: 228 ended, Loss: 0.9819139686664217\n",
      "Training at epoch: 229 ended, Loss: 0.9767632439872409\n",
      "Training at epoch: 230 ended, Loss: 0.977964232138552\n",
      "--> Testing at epoch: 230 ended, Loss: 1.0829000070288375\n",
      "Training at epoch: 231 ended, Loss: 0.9720789299890042\n",
      "Training at epoch: 232 ended, Loss: 0.970665175531301\n",
      "Training at epoch: 233 ended, Loss: 0.969846268372019\n",
      "Training at epoch: 234 ended, Loss: 0.9718867779451934\n",
      "Training at epoch: 235 ended, Loss: 0.969140615644817\n",
      "Training at epoch: 236 ended, Loss: 0.9626485858583003\n",
      "Training at epoch: 237 ended, Loss: 0.9603541555461623\n",
      "Training at epoch: 238 ended, Loss: 0.9604721934162715\n",
      "Training at epoch: 239 ended, Loss: 0.957109413100184\n",
      "Training at epoch: 240 ended, Loss: 0.9505680008845423\n",
      "--> Testing at epoch: 240 ended, Loss: 1.1823344029284812\n",
      "Training at epoch: 241 ended, Loss: 0.9468905238924173\n",
      "Training at epoch: 242 ended, Loss: 0.9535796245553693\n",
      "Training at epoch: 243 ended, Loss: 0.953761096509752\n",
      "Training at epoch: 244 ended, Loss: 0.9404069985622229\n",
      "Training at epoch: 245 ended, Loss: 0.9400896902479002\n",
      "Training at epoch: 246 ended, Loss: 0.9442364820899003\n",
      "Training at epoch: 247 ended, Loss: 0.9427206722491838\n",
      "Training at epoch: 248 ended, Loss: 0.9427414261084368\n",
      "Training at epoch: 249 ended, Loss: 0.9380255892059095\n",
      "Training at epoch: 250 ended, Loss: 0.9357425578370843\n",
      "--> Testing at epoch: 250 ended, Loss: 1.0054260089590743\n",
      "Training at epoch: 251 ended, Loss: 0.9309671173323544\n",
      "Training at epoch: 252 ended, Loss: 0.9314325874029268\n",
      "Training at epoch: 253 ended, Loss: 0.9292797346697936\n",
      "Training at epoch: 254 ended, Loss: 0.9269384601544398\n",
      "Training at epoch: 255 ended, Loss: 0.9212919216183468\n",
      "Training at epoch: 256 ended, Loss: 0.9222837955700456\n",
      "Training at epoch: 257 ended, Loss: 0.9197419814244167\n",
      "Training at epoch: 258 ended, Loss: 0.919251860294224\n",
      "Training at epoch: 259 ended, Loss: 0.9183809378664343\n",
      "Training at epoch: 260 ended, Loss: 0.90684457816209\n",
      "--> Testing at epoch: 260 ended, Loss: 1.0329746386489354\n",
      "Training at epoch: 261 ended, Loss: 0.9129763869473349\n",
      "Training at epoch: 262 ended, Loss: 0.9099031418383935\n",
      "Training at epoch: 263 ended, Loss: 0.9075081298070556\n",
      "Training at epoch: 264 ended, Loss: 0.9103916184436339\n",
      "Training at epoch: 265 ended, Loss: 0.9062809689231089\n",
      "Training at epoch: 266 ended, Loss: 0.9028036187469655\n",
      "Training at epoch: 267 ended, Loss: 0.9027561994441133\n",
      "Training at epoch: 268 ended, Loss: 0.9059141499813608\n",
      "Training at epoch: 269 ended, Loss: 0.8965520173018805\n",
      "Training at epoch: 270 ended, Loss: 0.8984296819252365\n",
      "--> Testing at epoch: 270 ended, Loss: 0.9321596090858048\n",
      "Training at epoch: 271 ended, Loss: 0.8945007952686667\n",
      "Training at epoch: 272 ended, Loss: 0.8954234046689886\n",
      "Training at epoch: 273 ended, Loss: 0.8925392875136369\n",
      "Training at epoch: 274 ended, Loss: 0.890263442382369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training at epoch: 275 ended, Loss: 0.8878039046232\n",
      "Training at epoch: 276 ended, Loss: 0.8879364197851245\n",
      "Training at epoch: 277 ended, Loss: 0.892766668095727\n",
      "Training at epoch: 278 ended, Loss: 0.8811380658807303\n",
      "Training at epoch: 279 ended, Loss: 0.8854275831425048\n",
      "Training at epoch: 280 ended, Loss: 0.8837957371839993\n",
      "--> Testing at epoch: 280 ended, Loss: 0.9560137036684397\n",
      "Training at epoch: 281 ended, Loss: 0.878468431913812\n",
      "Training at epoch: 282 ended, Loss: 0.8833854266731609\n",
      "Training at epoch: 283 ended, Loss: 0.8791979587678209\n",
      "Training at epoch: 284 ended, Loss: 0.8787488192574155\n",
      "Training at epoch: 285 ended, Loss: 0.8721127073715979\n",
      "Training at epoch: 286 ended, Loss: 0.8713648684017691\n",
      "Training at epoch: 287 ended, Loss: 0.8764940201091889\n",
      "Training at epoch: 288 ended, Loss: 0.8766509733598286\n",
      "Training at epoch: 289 ended, Loss: 0.8682832894942154\n",
      "Training at epoch: 290 ended, Loss: 0.8611103792836947\n",
      "--> Testing at epoch: 290 ended, Loss: 0.9663849803241523\n",
      "Training at epoch: 291 ended, Loss: 0.862312179764434\n",
      "Training at epoch: 292 ended, Loss: 0.8629065271047398\n",
      "Training at epoch: 293 ended, Loss: 0.8635775266793706\n",
      "Training at epoch: 294 ended, Loss: 0.8640734538842794\n",
      "Training at epoch: 295 ended, Loss: 0.8625143587385835\n",
      "Training at epoch: 296 ended, Loss: 0.8564360526171052\n",
      "Training at epoch: 297 ended, Loss: 0.8610086813759783\n",
      "Training at epoch: 298 ended, Loss: 0.8611224945316746\n",
      "Training at epoch: 299 ended, Loss: 0.8571412988553784\n",
      "Training at epoch: 300 ended, Loss: 0.855969854951566\n",
      "--> Testing at epoch: 300 ended, Loss: 0.9147140520649988\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "epochs = 300\n",
    "train_model(model=dnn_model, n_epochs=epochs)\n",
    "torch.save(dnn_model.state_dict(), \"../models/dnn_model_min_delay_v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "in_dim = min_delay_dataset.input_shape\n",
    "out_dim = min_delay_dataset.output_shape\n",
    "layers_dim = [in_dim, in_dim*2, in_dim*2, in_dim*2, out_dim]\n",
    "dnn_model = DenseNet(layers_dim)\n",
    "dnn_model.to(device)\n",
    "dnn_model.load_state_dict(torch.load(\"../models/dnn_model_min_delay_v3\"))\n",
    "\n",
    "# evaluate the model over the entire data and the test data\n",
    "train_error = []\n",
    "test_error = []\n",
    "all_error = []\n",
    "\n",
    "for xx, yy in train_dl:\n",
    "    x_to_pred, y_true = xx.float().to(device), yy.float().to(device)\n",
    "    pred = dnn_model(x_to_pred)\n",
    "    error = torch.sqrt(torch.sum((y_true - pred)**2, dim=1)).to(\"cpu\").detach().numpy()\n",
    "    all_error.extend(error)\n",
    "    train_error.extend(error)\n",
    "\n",
    "for xx, yy in test_dl:\n",
    "    x_to_pred, y_true = xx.float().to(device), yy.float().to(device)\n",
    "    pred = dnn_model(x_to_pred)\n",
    "    error = torch.sqrt(torch.sum((y_true - pred)**2, dim=1)).to(\"cpu\").detach().numpy()\n",
    "    all_error.extend(error)\n",
    "    test_error.extend(error)\n",
    "\n",
    "train_error = np.array(train_error)\n",
    "test_error = np.array(test_error)\n",
    "all_error = np.array(all_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the feature data is (150000, 4, 25, 6)\n",
      "The shape of the target data is: (150000, 3)\n",
      "Random shuffling state: 3\n",
      "The shape of the training data is: (75000, 4, 25, 6)\n",
      "The shape of the testing data is: (75000, 4, 25, 6)\n",
      "new training shape: (75000, 600)\n"
     ]
    }
   ],
   "source": [
    "all_paths_dataset = LoadDataSet(dir_name, path_mode=\"all_paths\")\n",
    "print(f\"Random shuffling state: {all_paths_dataset.rnd_state}\")\n",
    "scaler = StandardScaler()\n",
    "train_set, test_set = all_paths_dataset.get_datasets(scale=True, scaler=scaler)\n",
    "train_dl = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "test_dl = torch.utils.data.DataLoader(test_set, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─ModuleList: 1                          []                        --\n",
      "|    └─Linear: 2-1                       [-1, 1, 256]              153,856\n",
      "|    └─Linear: 2-2                       [-1, 1, 64]               16,448\n",
      "|    └─Linear: 2-3                       [-1, 1, 3]                195\n",
      "==========================================================================================\n",
      "Total params: 170,499\n",
      "Trainable params: 170,499\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.17\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.65\n",
      "Estimated Total Size (MB): 0.66\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "in_dim = all_paths_dataset.input_shape\n",
    "out_dim = all_paths_dataset.output_shape\n",
    "layers_dim = [in_dim, 256, 64, out_dim]\n",
    "dnn_model_all_paths = DenseNet(layers_dim)\n",
    "dnn_model_all_paths.to(device)\n",
    "summary(dnn_model_all_paths, input_data=(1,in_dim), device=device, depth=len(layers_dim))\n",
    "\n",
    "Loss = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(dnn_model_all_paths.parameters(), amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training at epoch: 1 ended, Loss: 0.24351692926329355\n",
      "Training at epoch: 2 ended, Loss: 0.22536540968953903\n",
      "Training at epoch: 3 ended, Loss: 0.25171498694288647\n",
      "Training at epoch: 4 ended, Loss: 0.19519713674369985\n",
      "Training at epoch: 5 ended, Loss: 0.2400540012338615\n",
      "Training at epoch: 6 ended, Loss: 0.21730976752498196\n",
      "Training at epoch: 7 ended, Loss: 0.24354467863127996\n",
      "Training at epoch: 8 ended, Loss: 0.2360339954435037\n",
      "Training at epoch: 9 ended, Loss: 0.2014892561418408\n",
      "Training at epoch: 10 ended, Loss: 0.2359370133957155\n",
      "--> Testing at epoch: 10 ended, Loss: 1.604369408375508\n",
      "Training at epoch: 11 ended, Loss: 0.19893481068745408\n",
      "Training at epoch: 12 ended, Loss: 0.2512045467793687\n",
      "Training at epoch: 13 ended, Loss: 0.2132957461464883\n",
      "Training at epoch: 14 ended, Loss: 0.23372462800141336\n",
      "Training at epoch: 15 ended, Loss: 0.2508550086773856\n",
      "Training at epoch: 16 ended, Loss: 0.21135586220771074\n",
      "Training at epoch: 17 ended, Loss: 0.21320530441319474\n",
      "Training at epoch: 18 ended, Loss: 0.22686957184594037\n",
      "Training at epoch: 19 ended, Loss: 0.26222089634661533\n",
      "Training at epoch: 20 ended, Loss: 0.24592875148433518\n",
      "--> Testing at epoch: 20 ended, Loss: 1.5053523384236\n",
      "Training at epoch: 21 ended, Loss: 0.3065016261578471\n",
      "Training at epoch: 22 ended, Loss: 0.19016222303428088\n",
      "Training at epoch: 23 ended, Loss: 0.21425207369760593\n",
      "Training at epoch: 24 ended, Loss: 0.24205366769525835\n",
      "Training at epoch: 25 ended, Loss: 0.22357162764238722\n",
      "Training at epoch: 26 ended, Loss: 0.21787301197794262\n",
      "Training at epoch: 27 ended, Loss: 0.25941075046257506\n",
      "Training at epoch: 28 ended, Loss: 0.227713981520944\n",
      "Training at epoch: 29 ended, Loss: 0.22294999429568294\n",
      "Training at epoch: 30 ended, Loss: 0.22382333920950725\n",
      "--> Testing at epoch: 30 ended, Loss: 1.4171776143280235\n",
      "Training at epoch: 31 ended, Loss: 0.22252719563435217\n",
      "Training at epoch: 32 ended, Loss: 0.20325281135197537\n",
      "Training at epoch: 33 ended, Loss: 0.24008649067457372\n",
      "Training at epoch: 34 ended, Loss: 0.20614325974943948\n",
      "Training at epoch: 35 ended, Loss: 0.23664474469504665\n",
      "Training at epoch: 36 ended, Loss: 0.20637968078783298\n",
      "Training at epoch: 37 ended, Loss: 0.23253004666125407\n",
      "Training at epoch: 38 ended, Loss: 0.19877315958510486\n",
      "Training at epoch: 39 ended, Loss: 0.18514807361619923\n",
      "Training at epoch: 40 ended, Loss: 0.21292110917924462\n",
      "--> Testing at epoch: 40 ended, Loss: 1.3786847591400146\n",
      "Training at epoch: 41 ended, Loss: 0.20783905603258582\n",
      "Training at epoch: 42 ended, Loss: 0.24338864839600824\n",
      "Training at epoch: 43 ended, Loss: 0.20311169308823224\n",
      "Training at epoch: 44 ended, Loss: 0.21364002026865456\n",
      "Training at epoch: 45 ended, Loss: 0.23308319174891193\n",
      "Training at epoch: 46 ended, Loss: 0.2251455038054862\n",
      "Training at epoch: 47 ended, Loss: 0.20459567835560838\n",
      "Training at epoch: 48 ended, Loss: 0.22654732039560638\n",
      "Training at epoch: 49 ended, Loss: 0.2498004145700955\n",
      "Training at epoch: 50 ended, Loss: 0.23871130791394224\n",
      "--> Testing at epoch: 50 ended, Loss: 1.3209929365564037\n",
      "Training at epoch: 51 ended, Loss: 0.1937028901753216\n",
      "Training at epoch: 52 ended, Loss: 0.23132521420901597\n",
      "Training at epoch: 53 ended, Loss: 0.2060115458030546\n",
      "Training at epoch: 54 ended, Loss: 0.1905924750272655\n",
      "Training at epoch: 55 ended, Loss: 0.23011154461168898\n",
      "Training at epoch: 56 ended, Loss: 0.21017491907904834\n",
      "Training at epoch: 57 ended, Loss: 0.21298727889008082\n",
      "Training at epoch: 58 ended, Loss: 0.189070907343535\n",
      "Training at epoch: 59 ended, Loss: 0.23487746265777243\n",
      "Training at epoch: 60 ended, Loss: 0.19180380912980782\n",
      "--> Testing at epoch: 60 ended, Loss: 1.4973072329083004\n",
      "Training at epoch: 61 ended, Loss: 0.2189865857124354\n",
      "Training at epoch: 62 ended, Loss: 0.2344913085900349\n",
      "Training at epoch: 63 ended, Loss: 0.2225741732042321\n",
      "Training at epoch: 64 ended, Loss: 0.21543181987492044\n",
      "Training at epoch: 65 ended, Loss: 0.24545732920881322\n",
      "Training at epoch: 66 ended, Loss: 0.22103492648630013\n",
      "Training at epoch: 67 ended, Loss: 0.20198669478475234\n",
      "Training at epoch: 68 ended, Loss: 0.2547618593677319\n",
      "Training at epoch: 69 ended, Loss: 0.2261047487293367\n",
      "Training at epoch: 70 ended, Loss: 0.2147983644338827\n",
      "--> Testing at epoch: 70 ended, Loss: 1.5132641252633687\n",
      "Training at epoch: 71 ended, Loss: 0.23609536761615058\n",
      "Training at epoch: 72 ended, Loss: 0.2171734248609012\n",
      "Training at epoch: 73 ended, Loss: 0.1909110892020522\n",
      "Training at epoch: 74 ended, Loss: 0.21757113167367648\n",
      "Training at epoch: 75 ended, Loss: 0.213302384585193\n",
      "Training at epoch: 76 ended, Loss: 0.21973926780122385\n",
      "Training at epoch: 77 ended, Loss: 0.19001561717794566\n",
      "Training at epoch: 78 ended, Loss: 0.215360523150668\n",
      "Training at epoch: 79 ended, Loss: 0.22998635585931584\n",
      "Training at epoch: 80 ended, Loss: 0.22752961846634287\n",
      "--> Testing at epoch: 80 ended, Loss: 1.3813590206004478\n",
      "Training at epoch: 81 ended, Loss: 0.19591397781611608\n",
      "Training at epoch: 82 ended, Loss: 0.24108487187245856\n",
      "Training at epoch: 83 ended, Loss: 0.20306627894036547\n",
      "Training at epoch: 84 ended, Loss: 0.2202419326465844\n",
      "Training at epoch: 85 ended, Loss: 0.18800064816645032\n",
      "Training at epoch: 86 ended, Loss: 0.20297152050309925\n",
      "Training at epoch: 87 ended, Loss: 0.20883314955642657\n",
      "Training at epoch: 88 ended, Loss: 0.23084531971428277\n",
      "Training at epoch: 89 ended, Loss: 0.19846090930116717\n",
      "Training at epoch: 90 ended, Loss: 0.21901119017573042\n",
      "--> Testing at epoch: 90 ended, Loss: 1.3859754220859424\n",
      "Training at epoch: 91 ended, Loss: 0.20256696739980995\n",
      "Training at epoch: 92 ended, Loss: 0.202906857281224\n",
      "Training at epoch: 93 ended, Loss: 0.21586862586934513\n",
      "Training at epoch: 94 ended, Loss: 0.20250938635847116\n",
      "Training at epoch: 95 ended, Loss: 0.21535541935833394\n",
      "Training at epoch: 96 ended, Loss: 0.20915133855544005\n",
      "Training at epoch: 97 ended, Loss: 0.2211211079639405\n",
      "Training at epoch: 98 ended, Loss: 0.22330812765635016\n",
      "Training at epoch: 99 ended, Loss: 0.20230570068080678\n",
      "Training at epoch: 100 ended, Loss: 0.19634330044134993\n",
      "--> Testing at epoch: 100 ended, Loss: 1.3288714732672717\n",
      "Training at epoch: 101 ended, Loss: 0.20468455441327887\n",
      "Training at epoch: 102 ended, Loss: 0.18892900237762736\n",
      "Training at epoch: 103 ended, Loss: 0.21198582792107917\n",
      "Training at epoch: 104 ended, Loss: 0.21838804411323606\n",
      "Training at epoch: 105 ended, Loss: 0.20436904029984268\n",
      "Training at epoch: 106 ended, Loss: 0.19425662056584256\n",
      "Training at epoch: 107 ended, Loss: 0.22620183282687548\n",
      "Training at epoch: 108 ended, Loss: 0.24179966632855274\n",
      "Training at epoch: 109 ended, Loss: 0.17527997602610101\n",
      "Training at epoch: 110 ended, Loss: 0.1921336288363657\n",
      "--> Testing at epoch: 110 ended, Loss: 1.4893271987502639\n",
      "Training at epoch: 111 ended, Loss: 0.2233395881286533\n",
      "Training at epoch: 112 ended, Loss: 0.2166994377166516\n",
      "Training at epoch: 113 ended, Loss: 0.20501116213290205\n",
      "Training at epoch: 114 ended, Loss: 0.2502734836019296\n",
      "Training at epoch: 115 ended, Loss: 0.19576970824285223\n",
      "Training at epoch: 116 ended, Loss: 0.24087629635951802\n",
      "Training at epoch: 117 ended, Loss: 0.24859428238091869\n",
      "Training at epoch: 118 ended, Loss: 0.23873718959396967\n",
      "Training at epoch: 119 ended, Loss: 0.23343271302033863\n",
      "Training at epoch: 120 ended, Loss: 0.21339946426769063\n",
      "--> Testing at epoch: 120 ended, Loss: 1.3357730250100832\n",
      "Training at epoch: 121 ended, Loss: 0.18277848827566812\n",
      "Training at epoch: 122 ended, Loss: 0.270197332231169\n",
      "Training at epoch: 123 ended, Loss: 0.2081596022169821\n",
      "Training at epoch: 124 ended, Loss: 0.20402549856275626\n",
      "Training at epoch: 125 ended, Loss: 0.19942811390213389\n",
      "Training at epoch: 126 ended, Loss: 0.23858317578173283\n",
      "Training at epoch: 127 ended, Loss: 0.2563722212306214\n",
      "Training at epoch: 128 ended, Loss: 0.17503203706237339\n",
      "Training at epoch: 129 ended, Loss: 0.20203787106194188\n",
      "Training at epoch: 130 ended, Loss: 0.22329246968590358\n",
      "--> Testing at epoch: 130 ended, Loss: 1.3912220113986247\n",
      "Training at epoch: 131 ended, Loss: 0.18751315759660186\n",
      "Training at epoch: 132 ended, Loss: 0.20499588703774807\n",
      "Training at epoch: 133 ended, Loss: 0.19871408479930291\n",
      "Training at epoch: 134 ended, Loss: 0.2082330128638894\n",
      "Training at epoch: 135 ended, Loss: 0.1970234241037798\n",
      "Training at epoch: 136 ended, Loss: 0.22311443940548817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training at epoch: 137 ended, Loss: 0.20778641575127332\n",
      "Training at epoch: 138 ended, Loss: 0.27488644217353936\n",
      "Training at epoch: 139 ended, Loss: 0.17776480060604755\n",
      "Training at epoch: 140 ended, Loss: 0.18710489701098246\n",
      "--> Testing at epoch: 140 ended, Loss: 1.359949393449603\n",
      "Training at epoch: 141 ended, Loss: 0.2234882511001449\n",
      "Training at epoch: 142 ended, Loss: 0.22024994396760347\n",
      "Training at epoch: 143 ended, Loss: 0.27886355876503033\n",
      "Training at epoch: 144 ended, Loss: 0.19492961243494278\n",
      "Training at epoch: 145 ended, Loss: 0.17524857402845911\n",
      "Training at epoch: 146 ended, Loss: 0.22395784188373452\n",
      "Training at epoch: 147 ended, Loss: 0.2102837358941499\n",
      "Training at epoch: 148 ended, Loss: 0.21951249553806088\n",
      "Training at epoch: 149 ended, Loss: 0.20563494480841193\n",
      "Training at epoch: 150 ended, Loss: 0.22798984794505575\n",
      "--> Testing at epoch: 150 ended, Loss: 1.3762688584424354\n",
      "Training at epoch: 151 ended, Loss: 0.19941667588618378\n",
      "Training at epoch: 152 ended, Loss: 0.2298833776675419\n",
      "Training at epoch: 153 ended, Loss: 0.18647936268299564\n",
      "Training at epoch: 154 ended, Loss: 0.25201250438390893\n",
      "Training at epoch: 155 ended, Loss: 0.19110280855434847\n",
      "Training at epoch: 156 ended, Loss: 0.22029210259953558\n",
      "Training at epoch: 157 ended, Loss: 0.2193335696733827\n",
      "Training at epoch: 158 ended, Loss: 0.20223354437124658\n",
      "Training at epoch: 159 ended, Loss: 0.18495916025299872\n",
      "Training at epoch: 160 ended, Loss: 0.19394936340737057\n",
      "--> Testing at epoch: 160 ended, Loss: 1.3734810279833305\n",
      "Training at epoch: 161 ended, Loss: 0.20386944392715095\n",
      "Training at epoch: 162 ended, Loss: 0.20467538742424837\n",
      "Training at epoch: 163 ended, Loss: 0.2205752791191607\n",
      "Training at epoch: 164 ended, Loss: 0.2031549988346295\n",
      "Training at epoch: 165 ended, Loss: 0.2508875161771736\n",
      "Training at epoch: 166 ended, Loss: 0.18221351098103938\n",
      "Training at epoch: 167 ended, Loss: 0.1897141920329539\n",
      "Training at epoch: 168 ended, Loss: 0.2091314723737436\n",
      "Training at epoch: 169 ended, Loss: 0.1791245024397003\n",
      "Training at epoch: 170 ended, Loss: 0.20541142884947028\n",
      "--> Testing at epoch: 170 ended, Loss: 1.4772611518163938\n",
      "Training at epoch: 171 ended, Loss: 0.22680451889099623\n",
      "Training at epoch: 172 ended, Loss: 0.23284999819936505\n",
      "Training at epoch: 173 ended, Loss: 0.23199957657649908\n",
      "Training at epoch: 174 ended, Loss: 0.19468744794294593\n",
      "Training at epoch: 175 ended, Loss: 0.20890470833196575\n",
      "Training at epoch: 176 ended, Loss: 0.2416143952852986\n",
      "Training at epoch: 177 ended, Loss: 0.22534146112685471\n",
      "Training at epoch: 178 ended, Loss: 0.19885891277832535\n",
      "Training at epoch: 179 ended, Loss: 0.21510852403531913\n",
      "Training at epoch: 180 ended, Loss: 0.18842278734809456\n",
      "--> Testing at epoch: 180 ended, Loss: 1.4413808592267938\n",
      "Training at epoch: 181 ended, Loss: 0.23042632076800276\n",
      "Training at epoch: 182 ended, Loss: 0.17442470537848237\n",
      "Training at epoch: 183 ended, Loss: 0.19799021013723145\n",
      "Training at epoch: 184 ended, Loss: 0.1974968204557667\n",
      "Training at epoch: 185 ended, Loss: 0.22492832272017926\n",
      "Training at epoch: 186 ended, Loss: 0.20271436738346166\n",
      "Training at epoch: 187 ended, Loss: 0.2087411944207707\n",
      "Training at epoch: 188 ended, Loss: 0.20807106236517176\n",
      "Training at epoch: 189 ended, Loss: 0.2083486536924597\n",
      "Training at epoch: 190 ended, Loss: 0.21488122993113898\n",
      "--> Testing at epoch: 190 ended, Loss: 1.5456740155413344\n",
      "Training at epoch: 191 ended, Loss: 0.18801609539894112\n",
      "Training at epoch: 192 ended, Loss: 0.18702805409551226\n",
      "Training at epoch: 193 ended, Loss: 0.22275990591330436\n",
      "Training at epoch: 194 ended, Loss: 0.18585080029951478\n",
      "Training at epoch: 195 ended, Loss: 0.19125161939634347\n",
      "Training at epoch: 196 ended, Loss: 0.21500249124853632\n",
      "Training at epoch: 197 ended, Loss: 0.18595208564881274\n",
      "Training at epoch: 198 ended, Loss: 0.21673693857394133\n",
      "Training at epoch: 199 ended, Loss: 0.20344238391991565\n",
      "Training at epoch: 200 ended, Loss: 0.187059894321747\n",
      "--> Testing at epoch: 200 ended, Loss: 1.4201211832665108\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "train_loss_history = []\n",
    "test_loss_history = []\n",
    "\n",
    "for epoch in range(200):\n",
    "    total_train_loss = 0\n",
    "    total_test_loss = 0\n",
    "    total_train = 0\n",
    "    total_test = 0\n",
    "\n",
    "    # training loop (iterate over the training set)\n",
    "    for x, y in train_dl:\n",
    "        x_train, y_train = x.float().to(device), y.float().to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        train_output = dnn_model_all_paths(x_train) # network output\n",
    "        loss = Loss(train_output, y_train) # compute loss\n",
    "        loss.backward() # compute gradient with respect to trainable parameters\n",
    "        optimizer.step() # weights update\n",
    "        total_train_loss += loss.item()\n",
    "        total_train += 1\n",
    "\n",
    "    print(f\"Training at epoch: {epoch+1} ended, Loss: {total_train_loss/total_train}\")\n",
    "    # append training loss for the epoch\n",
    "    train_loss_history.append(total_train_loss/total_train)\n",
    "\n",
    "    # testing every 10 training epochs\n",
    "    if ((epoch+1) % 10) == 0:\n",
    "        for x, y in test_dl:\n",
    "            x_test, y_test = x.float().to(device), y.float().to(device)\n",
    "            test_output = dnn_model_all_paths(x_test)\n",
    "            loss = Loss(test_output, y_test)\n",
    "            total_test_loss += loss.item()\n",
    "            total_test += 1\n",
    "        print(f\"--> Testing at epoch: {epoch+1} ended, Loss: {total_test_loss/total_test}\")\n",
    "        # append test loss for the epoch\n",
    "        test_loss_history.append(total_test_loss/total_test)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(dnn_model_all_paths.state_dict(), \"../models/dnn_model_all_paths_v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = all_paths_dataset.input_shape\n",
    "out_dim = all_paths_dataset.output_shape\n",
    "layers_dim = [in_dim, 256, 64, out_dim]\n",
    "dnn_model_all_paths = DenseNet(layers_dim)\n",
    "dnn_model_all_paths.to(device)\n",
    "dnn_model_all_paths.load_state_dict(torch.load(\"../models/dnn_model_all_paths_v3\"))\n",
    "\n",
    "# evaluate the model over the entire data and the test data\n",
    "train_error_all_paths = []\n",
    "test_error_all_paths = []\n",
    "all_error_all_paths = []\n",
    "\n",
    "for xx, yy in train_dl:\n",
    "    x_to_pred, y_true = xx.float().to(device), yy.float().to(device)\n",
    "    pred = dnn_model_all_paths(x_to_pred)\n",
    "    error = torch.sqrt(torch.sum((y_true - pred)**2, dim=1)).to(\"cpu\").detach().numpy()\n",
    "    train_error_all_paths.extend(error)\n",
    "    all_error_all_paths.extend(error)\n",
    "\n",
    "for xx, yy in test_dl:\n",
    "    x_to_pred, y_true = xx.float().to(device), yy.float().to(device)\n",
    "    pred = dnn_model_all_paths(x_to_pred)\n",
    "    error = torch.sqrt(torch.sum((y_true - pred)**2, dim=1)).to(\"cpu\").detach().numpy()\n",
    "    all_error_all_paths.extend(error)\n",
    "    test_error_all_paths.extend(error)\n",
    "\n",
    "train_error_all_paths = np.array(train_error_all_paths)\n",
    "test_error_all_paths = np.array(test_error_all_paths)\n",
    "all_error_all_paths = np.array(all_error_all_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import baseline results\n",
    "true_cord_tensor = sio.loadmat(os.path.abspath(os.path.join(\"../../\"+dir_name, \"all_true_tensor\")))['true_cord_tensor']\n",
    "tot_points = true_cord_tensor.shape[0]*true_cord_tensor.shape[1]\n",
    "true_cord_tensor_r = np.reshape(true_cord_tensor, (tot_points, true_cord_tensor.shape[2]))\n",
    "\n",
    "with open('../models/best_toa_estimated.npy', 'rb') as f:\n",
    "    best_est_3d_coords = np.load(f)\n",
    "    \n",
    "baseline_error = np.sqrt(np.sum((true_cord_tensor_r - best_est_3d_coords)**2, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150000,)\n"
     ]
    }
   ],
   "source": [
    "hashim_baseline = sio.loadmat(os.path.abspath(os.path.join(\"../../\"+dir_name, \"baselineerror\")))[\"err_data\"].reshape(-1)\n",
    "print(hashim_baseline.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tikzplotlib\n",
    "\n",
    "figure, ax = plt.subplots(1, 1, figsize=(8, 5)) \n",
    "ax.plot(np.sort(baseline_error), np.linspace(0,1,tot_points), np.sort(all_error), np.linspace(0,1,len(all_error)), \n",
    "       np.sort(all_error_all_paths), np.linspace(0,1,len(all_error_all_paths)), np.sort(hashim_baseline), np.linspace(0,1,len(hashim_baseline)))\n",
    "tikzplotlib.clean_figure(fig=figure, target_resolution=600)\n",
    "\n",
    "#plt.plot(np.sort(baseline_error), np.linspace(0,1,tot_points), linewidth=2, label=\"Baseline\")\n",
    "#plt.plot(np.sort(all_error), np.linspace(0,1,len(all_error)), linewidth=2, label=\"Min Delay\")\n",
    "#plt.plot(np.sort(all_error_all_paths), np.linspace(0,1,len(all_error_all_paths)), linewidth=2, label=\"All Paths\")\n",
    "plt.xlim([-2,20])\n",
    "plt.ylabel(\"CDF\", fontsize=12)\n",
    "plt.xlabel(\"Estimation Error [m]\", fontsize=12)\n",
    "plt.title(\"CDF of the position estimation error\")\n",
    "plt.grid()\n",
    "plt.legend([\"Baseline 1\", \"Min Delay Path\", \"All Paths\", \"Baseline 2\"], loc=\"lower right\")\n",
    "\n",
    "tikzplotlib.save(\"dnn_vs_baseline_hash_baseline.tex\")\n",
    "# plt.savefig(\"./dnn_vs_baseline.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average error for the Baseline is 18.359439931346508\n",
      "The average error for Hashim Baseline is 10.12156626704354\n",
      "The average error for the Min Delay DNN is 1.172646164894104\n",
      "The average error for the All Paths DNN is 0.8733026385307312\n"
     ]
    }
   ],
   "source": [
    "print(f\"The average error for the Baseline is {np.mean(baseline_error)}\")\n",
    "print(f\"The average error for Hashim Baseline is {np.mean(hashim_baseline)}\")\n",
    "print(f\"The average error for the Min Delay DNN is {np.mean(all_error)}\")\n",
    "print(f\"The average error for the All Paths DNN is {np.mean(all_error_all_paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
