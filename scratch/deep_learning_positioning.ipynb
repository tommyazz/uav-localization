{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from src.utils.data_loader import LoadDataSet\n",
    "from src.positioning.toa import toa_positioning\n",
    "from src.utils.models import DenseNet\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the feature data is (150000, 4, 6)\n",
      "The shape of the target data is: (150000, 3)\n",
      "Random shuffling state: 3\n",
      "The shape of the training data is: (75000, 4, 6)\n",
      "The shape of the testing data is: (75000, 4, 6)\n",
      "new training shape: (75000, 24)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "dir_name = \"Omni Spatial and Temporal Data/\"\n",
    "min_delay_dataset = LoadDataSet(dir_name)\n",
    "print(f\"Random shuffling state: {min_delay_dataset.rnd_state}\")\n",
    "scaler = StandardScaler()\n",
    "train_set, test_set = min_delay_dataset.get_datasets(scale=True, scaler=scaler)\n",
    "train_dl = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "test_dl = torch.utils.data.DataLoader(test_set, batch_size=1024, shuffle=False)\n",
    "\n",
    "data_dir = os.path.abspath(\"../../\"+dir_name)\n",
    "toa_tensor_def = sio.loadmat(os.path.join(data_dir, \"all_toa_tensor\"))['toa_tensor']\n",
    "\n",
    "bool_m = toa_tensor_def == min_delay_dataset.toa_tensor_compare\n",
    "print(np.all(bool_m))  # sanity check that the data corresponding to the min_toa path is selected correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─ModuleList: 1                          []                        --\n",
      "|    └─Linear: 2-1                       [-1, 1, 48]               1,200\n",
      "|    └─Linear: 2-2                       [-1, 1, 48]               2,352\n",
      "|    └─Linear: 2-3                       [-1, 1, 48]               2,352\n",
      "|    └─Linear: 2-4                       [-1, 1, 3]                147\n",
      "==========================================================================================\n",
      "Total params: 6,051\n",
      "Trainable params: 6,051\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.01\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.02\n",
      "Estimated Total Size (MB): 0.02\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "in_dim = min_delay_dataset.input_shape\n",
    "out_dim = min_delay_dataset.output_shape\n",
    "layers_dim = [in_dim, in_dim*2, in_dim*2, in_dim*2, out_dim]\n",
    "dnn_model = DenseNet(layers_dim)\n",
    "dnn_model.to(device)\n",
    "summary(dnn_model, input_data=(1,in_dim), device=device, depth=len(layers_dim))\n",
    "\n",
    "Loss = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(dnn_model.parameters(), amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model=None, n_epochs=200):\n",
    "    # train the model\n",
    "    train_loss_history = []\n",
    "    test_loss_history = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        total_train_loss = 0\n",
    "        total_test_loss = 0\n",
    "        total_train = 0\n",
    "        total_test = 0\n",
    "\n",
    "        # training loop (iterate over the training set)\n",
    "        for x, y in train_dl:\n",
    "            x_train, y_train = x.float().to(device), y.float().to(device)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            train_output = model(x_train) # network output\n",
    "            loss = Loss(train_output, y_train) # compute loss\n",
    "            loss.backward() # compute gradient with respect to trainable parameters\n",
    "            optimizer.step() # weights update\n",
    "            total_train_loss += loss.item()\n",
    "            total_train += 1\n",
    "\n",
    "        print(f\"Training at epoch: {epoch+1} ended, Loss: {total_train_loss/total_train}\")\n",
    "        # append training loss for the epoch\n",
    "        train_loss_history.append(total_train_loss/total_train)\n",
    "\n",
    "        # testing every 10 training epochs\n",
    "        if ((epoch+1) % 10) == 0:\n",
    "            for x, y in test_dl:\n",
    "                x_test, y_test = x.float().to(device), y.float().to(device)\n",
    "                test_output = model(x_test)\n",
    "                loss = Loss(test_output, y_test)\n",
    "                total_test_loss += loss.item()\n",
    "                total_test += 1\n",
    "            print(f\"--> Testing at epoch: {epoch+1} ended, Loss: {total_test_loss/total_test}\")\n",
    "            # append test loss for the epoch\n",
    "            test_loss_history.append(total_test_loss/total_test)\n",
    "\n",
    "    print('Finished Training')\n",
    "    return train_loss_history, test_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training at epoch: 1 ended, Loss: 1787.1791803983292\n",
      "Training at epoch: 2 ended, Loss: 34.86498188057043\n",
      "Training at epoch: 3 ended, Loss: 21.221980891130077\n",
      "Training at epoch: 4 ended, Loss: 16.09716982778429\n",
      "Training at epoch: 5 ended, Loss: 13.19733549832484\n",
      "Training at epoch: 6 ended, Loss: 11.188080008831447\n",
      "Training at epoch: 7 ended, Loss: 9.768222453193452\n",
      "Training at epoch: 8 ended, Loss: 8.754940028076692\n",
      "Training at epoch: 9 ended, Loss: 8.008660907122868\n",
      "Training at epoch: 10 ended, Loss: 7.392791446717933\n",
      "--> Testing at epoch: 10 ended, Loss: 7.098536426956589\n",
      "Training at epoch: 11 ended, Loss: 6.9026432726977225\n",
      "Training at epoch: 12 ended, Loss: 6.503839444127506\n",
      "Training at epoch: 13 ended, Loss: 6.131293840070633\n",
      "Training at epoch: 14 ended, Loss: 5.822884238773239\n",
      "Training at epoch: 15 ended, Loss: 5.552465665157338\n",
      "Training at epoch: 16 ended, Loss: 5.316933146668376\n",
      "Training at epoch: 17 ended, Loss: 5.0984093011341\n",
      "Training at epoch: 18 ended, Loss: 4.910526392479195\n",
      "Training at epoch: 19 ended, Loss: 4.752360673030082\n",
      "Training at epoch: 20 ended, Loss: 4.551933230190147\n",
      "--> Testing at epoch: 20 ended, Loss: 4.518505431510307\n",
      "Training at epoch: 21 ended, Loss: 4.395655498518878\n",
      "Training at epoch: 22 ended, Loss: 4.258200594894715\n",
      "Training at epoch: 23 ended, Loss: 4.12811432328749\n",
      "Training at epoch: 24 ended, Loss: 4.012047015791664\n",
      "Training at epoch: 25 ended, Loss: 3.9023729056730203\n",
      "Training at epoch: 26 ended, Loss: 3.7903612116955654\n",
      "Training at epoch: 27 ended, Loss: 3.698763767149261\n",
      "Training at epoch: 28 ended, Loss: 3.601256020341717\n",
      "Training at epoch: 29 ended, Loss: 3.4936927299145544\n",
      "Training at epoch: 30 ended, Loss: 3.4149360182240556\n",
      "--> Testing at epoch: 30 ended, Loss: 3.5246510634551176\n",
      "Training at epoch: 31 ended, Loss: 3.35453390297019\n",
      "Training at epoch: 32 ended, Loss: 3.3003441714030077\n",
      "Training at epoch: 33 ended, Loss: 3.2261245967025642\n",
      "Training at epoch: 34 ended, Loss: 3.1742354686858305\n",
      "Training at epoch: 35 ended, Loss: 3.103023918491785\n",
      "Training at epoch: 36 ended, Loss: 3.0567550796239855\n",
      "Training at epoch: 37 ended, Loss: 2.9916962816701003\n",
      "Training at epoch: 38 ended, Loss: 2.9476564827124005\n",
      "Training at epoch: 39 ended, Loss: 2.8832846677038857\n",
      "Training at epoch: 40 ended, Loss: 2.8507892450018955\n",
      "--> Testing at epoch: 40 ended, Loss: 2.8713942315127396\n",
      "Training at epoch: 41 ended, Loss: 2.8075257874135273\n",
      "Training at epoch: 42 ended, Loss: 2.7603690904968836\n",
      "Training at epoch: 43 ended, Loss: 2.71604219035789\n",
      "Training at epoch: 44 ended, Loss: 2.674700536901837\n",
      "Training at epoch: 45 ended, Loss: 2.644351475500206\n",
      "Training at epoch: 46 ended, Loss: 2.630244233299034\n",
      "Training at epoch: 47 ended, Loss: 2.582057523150188\n",
      "Training at epoch: 48 ended, Loss: 2.5609917305791337\n",
      "Training at epoch: 49 ended, Loss: 2.5208747670969864\n",
      "Training at epoch: 50 ended, Loss: 2.48537253115466\n",
      "--> Testing at epoch: 50 ended, Loss: 2.6010262354000195\n",
      "Training at epoch: 51 ended, Loss: 2.459632110473646\n",
      "Training at epoch: 52 ended, Loss: 2.4446107492918854\n",
      "Training at epoch: 53 ended, Loss: 2.405682536801991\n",
      "Training at epoch: 54 ended, Loss: 2.3781691166117738\n",
      "Training at epoch: 55 ended, Loss: 2.3377542931828077\n",
      "Training at epoch: 56 ended, Loss: 2.310415325616407\n",
      "Training at epoch: 57 ended, Loss: 2.275847880074392\n",
      "Training at epoch: 58 ended, Loss: 2.260894217487081\n",
      "Training at epoch: 59 ended, Loss: 2.2344148321388726\n",
      "Training at epoch: 60 ended, Loss: 2.2055200825194246\n",
      "--> Testing at epoch: 60 ended, Loss: 2.2904449105262756\n",
      "Training at epoch: 61 ended, Loss: 2.1812341409280847\n",
      "Training at epoch: 62 ended, Loss: 2.1550567841646817\n",
      "Training at epoch: 63 ended, Loss: 2.1344523010680088\n",
      "Training at epoch: 64 ended, Loss: 2.113580155036962\n",
      "Training at epoch: 65 ended, Loss: 2.089448971451346\n",
      "Training at epoch: 66 ended, Loss: 2.0681460205427413\n",
      "Training at epoch: 67 ended, Loss: 2.057474452023213\n",
      "Training at epoch: 68 ended, Loss: 2.0327917919351175\n",
      "Training at epoch: 69 ended, Loss: 2.0135979804340685\n",
      "Training at epoch: 70 ended, Loss: 1.999782296654104\n",
      "--> Testing at epoch: 70 ended, Loss: 2.0520209541191927\n",
      "Training at epoch: 71 ended, Loss: 1.9737948646622714\n",
      "Training at epoch: 72 ended, Loss: 1.9523531430537586\n",
      "Training at epoch: 73 ended, Loss: 1.941739651275983\n",
      "Training at epoch: 74 ended, Loss: 1.9181329357618968\n",
      "Training at epoch: 75 ended, Loss: 1.9038648490282253\n",
      "Training at epoch: 76 ended, Loss: 1.8868863105570497\n",
      "Training at epoch: 77 ended, Loss: 1.880267131577783\n",
      "Training at epoch: 78 ended, Loss: 1.8683344106167656\n",
      "Training at epoch: 79 ended, Loss: 1.843259845310409\n",
      "Training at epoch: 80 ended, Loss: 1.8370240976074044\n",
      "--> Testing at epoch: 80 ended, Loss: 1.952223492635263\n",
      "Training at epoch: 81 ended, Loss: 1.8143569447557877\n",
      "Training at epoch: 82 ended, Loss: 1.7985567451921949\n",
      "Training at epoch: 83 ended, Loss: 1.7817965759657348\n",
      "Training at epoch: 84 ended, Loss: 1.77314592560963\n",
      "Training at epoch: 85 ended, Loss: 1.7515357856737266\n",
      "Training at epoch: 86 ended, Loss: 1.749067863061668\n",
      "Training at epoch: 87 ended, Loss: 1.7284835892834354\n",
      "Training at epoch: 88 ended, Loss: 1.7240326839273497\n",
      "Training at epoch: 89 ended, Loss: 1.7014738592373226\n",
      "Training at epoch: 90 ended, Loss: 1.685607926697556\n",
      "--> Testing at epoch: 90 ended, Loss: 1.740260217640851\n",
      "Training at epoch: 91 ended, Loss: 1.6791490055747813\n",
      "Training at epoch: 92 ended, Loss: 1.6632590516638837\n",
      "Training at epoch: 93 ended, Loss: 1.6529956147862985\n",
      "Training at epoch: 94 ended, Loss: 1.6329015271528386\n",
      "Training at epoch: 95 ended, Loss: 1.6271305652430643\n",
      "Training at epoch: 96 ended, Loss: 1.606848344422953\n",
      "Training at epoch: 97 ended, Loss: 1.6026126384073964\n",
      "Training at epoch: 98 ended, Loss: 1.5943118290969334\n",
      "Training at epoch: 99 ended, Loss: 1.579976388796808\n",
      "Training at epoch: 100 ended, Loss: 1.568321670446888\n",
      "--> Testing at epoch: 100 ended, Loss: 1.7045790536983594\n",
      "Training at epoch: 101 ended, Loss: 1.5604301764671102\n",
      "Training at epoch: 102 ended, Loss: 1.5530481575876982\n",
      "Training at epoch: 103 ended, Loss: 1.534819764145507\n",
      "Training at epoch: 104 ended, Loss: 1.526216054519594\n",
      "Training at epoch: 105 ended, Loss: 1.5195573992238924\n",
      "Training at epoch: 106 ended, Loss: 1.49933151432882\n",
      "Training at epoch: 107 ended, Loss: 1.489886866189514\n",
      "Training at epoch: 108 ended, Loss: 1.487002451837368\n",
      "Training at epoch: 109 ended, Loss: 1.4822429718112782\n",
      "Training at epoch: 110 ended, Loss: 1.4674269344288944\n",
      "--> Testing at epoch: 110 ended, Loss: 1.5054487167177975\n",
      "Training at epoch: 111 ended, Loss: 1.468395485876465\n",
      "Training at epoch: 112 ended, Loss: 1.446468632388542\n",
      "Training at epoch: 113 ended, Loss: 1.4384008691782837\n",
      "Training at epoch: 114 ended, Loss: 1.4384613092289442\n",
      "Training at epoch: 115 ended, Loss: 1.4226375325573386\n",
      "Training at epoch: 116 ended, Loss: 1.4238789469486821\n",
      "Training at epoch: 117 ended, Loss: 1.410551012439634\n",
      "Training at epoch: 118 ended, Loss: 1.4145094811178922\n",
      "Training at epoch: 119 ended, Loss: 1.397234002910173\n",
      "Training at epoch: 120 ended, Loss: 1.3939238192685228\n",
      "--> Testing at epoch: 120 ended, Loss: 1.4613070536304165\n",
      "Training at epoch: 121 ended, Loss: 1.3876194908146973\n",
      "Training at epoch: 122 ended, Loss: 1.3776278830428985\n",
      "Training at epoch: 123 ended, Loss: 1.369282832642566\n",
      "Training at epoch: 124 ended, Loss: 1.3623417340753021\n",
      "Training at epoch: 125 ended, Loss: 1.3563880540633975\n",
      "Training at epoch: 126 ended, Loss: 1.3507098407824698\n",
      "Training at epoch: 127 ended, Loss: 1.3456065455521535\n",
      "Training at epoch: 128 ended, Loss: 1.3418912801115992\n",
      "Training at epoch: 129 ended, Loss: 1.3346736874088085\n",
      "Training at epoch: 130 ended, Loss: 1.3206811107491674\n",
      "--> Testing at epoch: 130 ended, Loss: 1.4094660475447371\n",
      "Training at epoch: 131 ended, Loss: 1.3182983670269264\n",
      "Training at epoch: 132 ended, Loss: 1.3140708308272802\n",
      "Training at epoch: 133 ended, Loss: 1.3080335900746922\n",
      "Training at epoch: 134 ended, Loss: 1.3083478073516397\n",
      "Training at epoch: 135 ended, Loss: 1.2997779890055745\n",
      "Training at epoch: 136 ended, Loss: 1.2931553816785177\n",
      "Training at epoch: 137 ended, Loss: 1.2907944570959835\n",
      "Training at epoch: 138 ended, Loss: 1.2884559051575515\n",
      "Training at epoch: 139 ended, Loss: 1.2806131581016165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training at epoch: 140 ended, Loss: 1.2749996030823973\n",
      "--> Testing at epoch: 140 ended, Loss: 1.3198543142628025\n",
      "Training at epoch: 141 ended, Loss: 1.269213841255411\n",
      "Training at epoch: 142 ended, Loss: 1.2567794246808863\n",
      "Training at epoch: 143 ended, Loss: 1.2660835782390203\n",
      "Training at epoch: 144 ended, Loss: 1.2550562516922836\n",
      "Training at epoch: 145 ended, Loss: 1.249174794671682\n",
      "Training at epoch: 146 ended, Loss: 1.252248502924479\n",
      "Training at epoch: 147 ended, Loss: 1.2354972926779004\n",
      "Training at epoch: 148 ended, Loss: 1.2370680376455034\n",
      "Training at epoch: 149 ended, Loss: 1.2264933173032955\n",
      "Training at epoch: 150 ended, Loss: 1.2287035899383012\n",
      "--> Testing at epoch: 150 ended, Loss: 1.2841136624684204\n",
      "Training at epoch: 151 ended, Loss: 1.2221902371495454\n",
      "Training at epoch: 152 ended, Loss: 1.2152559268113492\n",
      "Training at epoch: 153 ended, Loss: 1.2144416476577622\n",
      "Training at epoch: 154 ended, Loss: 1.218689309032602\n",
      "Training at epoch: 155 ended, Loss: 1.198615031259662\n",
      "Training at epoch: 156 ended, Loss: 1.204228288153943\n",
      "Training at epoch: 157 ended, Loss: 1.1973045367124544\n",
      "Training at epoch: 158 ended, Loss: 1.1965434638967694\n",
      "Training at epoch: 159 ended, Loss: 1.1942648648605412\n",
      "Training at epoch: 160 ended, Loss: 1.1850771264716626\n",
      "--> Testing at epoch: 160 ended, Loss: 1.3019567695823875\n",
      "Training at epoch: 161 ended, Loss: 1.1828057463233177\n",
      "Training at epoch: 162 ended, Loss: 1.1758853812851393\n",
      "Training at epoch: 163 ended, Loss: 1.1753077226922983\n",
      "Training at epoch: 164 ended, Loss: 1.1713419287201692\n",
      "Training at epoch: 165 ended, Loss: 1.1668771554178752\n",
      "Training at epoch: 166 ended, Loss: 1.1742815671825573\n",
      "Training at epoch: 167 ended, Loss: 1.1584466041507575\n",
      "Training at epoch: 168 ended, Loss: 1.1610144312420396\n",
      "Training at epoch: 169 ended, Loss: 1.155548191721529\n",
      "Training at epoch: 170 ended, Loss: 1.1539582116433988\n",
      "--> Testing at epoch: 170 ended, Loss: 1.3260570764541626\n",
      "Training at epoch: 171 ended, Loss: 1.1447434864887402\n",
      "Training at epoch: 172 ended, Loss: 1.1466670063333808\n",
      "Training at epoch: 173 ended, Loss: 1.1372475615693034\n",
      "Training at epoch: 174 ended, Loss: 1.1383594731600009\n",
      "Training at epoch: 175 ended, Loss: 1.1388325765983236\n",
      "Training at epoch: 176 ended, Loss: 1.1265021200218706\n",
      "Training at epoch: 177 ended, Loss: 1.1250123998894017\n",
      "Training at epoch: 178 ended, Loss: 1.1183928893465842\n",
      "Training at epoch: 179 ended, Loss: 1.1176677823524426\n",
      "Training at epoch: 180 ended, Loss: 1.1147974941485368\n",
      "--> Testing at epoch: 180 ended, Loss: 1.1707460405053318\n",
      "Training at epoch: 181 ended, Loss: 1.109560538935173\n",
      "Training at epoch: 182 ended, Loss: 1.1095598702519216\n",
      "Training at epoch: 183 ended, Loss: 1.1080814579266534\n",
      "Training at epoch: 184 ended, Loss: 1.1007860402732375\n",
      "Training at epoch: 185 ended, Loss: 1.1011211385143087\n",
      "Training at epoch: 186 ended, Loss: 1.0906517881734785\n",
      "Training at epoch: 187 ended, Loss: 1.092085293052343\n",
      "Training at epoch: 188 ended, Loss: 1.0930440395422052\n",
      "Training at epoch: 189 ended, Loss: 1.0839964951747716\n",
      "Training at epoch: 190 ended, Loss: 1.0836396481930803\n",
      "--> Testing at epoch: 190 ended, Loss: 1.16506562845127\n",
      "Training at epoch: 191 ended, Loss: 1.086847766826991\n",
      "Training at epoch: 192 ended, Loss: 1.0771454805349328\n",
      "Training at epoch: 193 ended, Loss: 1.0701848411417658\n",
      "Training at epoch: 194 ended, Loss: 1.069172205447947\n",
      "Training at epoch: 195 ended, Loss: 1.0620539696957063\n",
      "Training at epoch: 196 ended, Loss: 1.0667103416200587\n",
      "Training at epoch: 197 ended, Loss: 1.0605865986858822\n",
      "Training at epoch: 198 ended, Loss: 1.0606170063234432\n",
      "Training at epoch: 199 ended, Loss: 1.0570172718996278\n",
      "Training at epoch: 200 ended, Loss: 1.0498743959457801\n",
      "--> Testing at epoch: 200 ended, Loss: 1.1502813519658268\n",
      "Training at epoch: 201 ended, Loss: 1.0537323452303433\n",
      "Training at epoch: 202 ended, Loss: 1.0500141241724377\n",
      "Training at epoch: 203 ended, Loss: 1.0502003977526577\n",
      "Training at epoch: 204 ended, Loss: 1.0419732922046054\n",
      "Training at epoch: 205 ended, Loss: 1.038273400706541\n",
      "Training at epoch: 206 ended, Loss: 1.0356740811045055\n",
      "Training at epoch: 207 ended, Loss: 1.038464622095278\n",
      "Training at epoch: 208 ended, Loss: 1.0279855171263015\n",
      "Training at epoch: 209 ended, Loss: 1.032300024315613\n",
      "Training at epoch: 210 ended, Loss: 1.0301304913472398\n",
      "--> Testing at epoch: 210 ended, Loss: 1.1050168420817401\n",
      "Training at epoch: 211 ended, Loss: 1.0212437097911338\n",
      "Training at epoch: 212 ended, Loss: 1.0191637898442485\n",
      "Training at epoch: 213 ended, Loss: 1.019785451146523\n",
      "Training at epoch: 214 ended, Loss: 1.0150997268843367\n",
      "Training at epoch: 215 ended, Loss: 1.0179551053703237\n",
      "Training at epoch: 216 ended, Loss: 1.0119630180477281\n",
      "Training at epoch: 217 ended, Loss: 1.0088333364181958\n",
      "Training at epoch: 218 ended, Loss: 1.003664961319938\n",
      "Training at epoch: 219 ended, Loss: 1.009078134792148\n",
      "Training at epoch: 220 ended, Loss: 1.0013402419819564\n",
      "--> Testing at epoch: 220 ended, Loss: 1.0675612591408394\n",
      "Training at epoch: 221 ended, Loss: 0.9975780210117635\n",
      "Training at epoch: 222 ended, Loss: 0.9970978229297103\n",
      "Training at epoch: 223 ended, Loss: 0.9895283472962652\n",
      "Training at epoch: 224 ended, Loss: 0.9928462877424605\n",
      "Training at epoch: 225 ended, Loss: 0.993309171749565\n",
      "Training at epoch: 226 ended, Loss: 0.9788610572217877\n",
      "Training at epoch: 227 ended, Loss: 0.9848885068688986\n",
      "Training at epoch: 228 ended, Loss: 0.9819139686664217\n",
      "Training at epoch: 229 ended, Loss: 0.9767632439872409\n",
      "Training at epoch: 230 ended, Loss: 0.977964232138552\n",
      "--> Testing at epoch: 230 ended, Loss: 1.0829000070288375\n",
      "Training at epoch: 231 ended, Loss: 0.9720789299890042\n",
      "Training at epoch: 232 ended, Loss: 0.970665175531301\n",
      "Training at epoch: 233 ended, Loss: 0.969846268372019\n",
      "Training at epoch: 234 ended, Loss: 0.9718867779451934\n",
      "Training at epoch: 235 ended, Loss: 0.969140615644817\n",
      "Training at epoch: 236 ended, Loss: 0.9626485858583003\n",
      "Training at epoch: 237 ended, Loss: 0.9603541555461623\n",
      "Training at epoch: 238 ended, Loss: 0.9604721934162715\n",
      "Training at epoch: 239 ended, Loss: 0.957109413100184\n",
      "Training at epoch: 240 ended, Loss: 0.9505680008845423\n",
      "--> Testing at epoch: 240 ended, Loss: 1.1823344029284812\n",
      "Training at epoch: 241 ended, Loss: 0.9468905238924173\n",
      "Training at epoch: 242 ended, Loss: 0.9535796245553693\n",
      "Training at epoch: 243 ended, Loss: 0.953761096509752\n",
      "Training at epoch: 244 ended, Loss: 0.9404069985622229\n",
      "Training at epoch: 245 ended, Loss: 0.9400896902479002\n",
      "Training at epoch: 246 ended, Loss: 0.9442364820899003\n",
      "Training at epoch: 247 ended, Loss: 0.9427206722491838\n",
      "Training at epoch: 248 ended, Loss: 0.9427414261084368\n",
      "Training at epoch: 249 ended, Loss: 0.9380255892059095\n",
      "Training at epoch: 250 ended, Loss: 0.9357425578370843\n",
      "--> Testing at epoch: 250 ended, Loss: 1.0054260089590743\n",
      "Training at epoch: 251 ended, Loss: 0.9309671173323544\n",
      "Training at epoch: 252 ended, Loss: 0.9314325874029268\n",
      "Training at epoch: 253 ended, Loss: 0.9292797346697936\n",
      "Training at epoch: 254 ended, Loss: 0.9269384601544398\n",
      "Training at epoch: 255 ended, Loss: 0.9212919216183468\n",
      "Training at epoch: 256 ended, Loss: 0.9222837955700456\n",
      "Training at epoch: 257 ended, Loss: 0.9197419814244167\n",
      "Training at epoch: 258 ended, Loss: 0.919251860294224\n",
      "Training at epoch: 259 ended, Loss: 0.9183809378664343\n",
      "Training at epoch: 260 ended, Loss: 0.90684457816209\n",
      "--> Testing at epoch: 260 ended, Loss: 1.0329746386489354\n",
      "Training at epoch: 261 ended, Loss: 0.9129763869473349\n",
      "Training at epoch: 262 ended, Loss: 0.9099031418383935\n",
      "Training at epoch: 263 ended, Loss: 0.9075081298070556\n",
      "Training at epoch: 264 ended, Loss: 0.9103916184436339\n",
      "Training at epoch: 265 ended, Loss: 0.9062809689231089\n",
      "Training at epoch: 266 ended, Loss: 0.9028036187469655\n",
      "Training at epoch: 267 ended, Loss: 0.9027561994441133\n",
      "Training at epoch: 268 ended, Loss: 0.9059141499813608\n",
      "Training at epoch: 269 ended, Loss: 0.8965520173018805\n",
      "Training at epoch: 270 ended, Loss: 0.8984296819252365\n",
      "--> Testing at epoch: 270 ended, Loss: 0.9321596090858048\n",
      "Training at epoch: 271 ended, Loss: 0.8945007952686667\n",
      "Training at epoch: 272 ended, Loss: 0.8954234046689886\n",
      "Training at epoch: 273 ended, Loss: 0.8925392875136369\n",
      "Training at epoch: 274 ended, Loss: 0.890263442382369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training at epoch: 275 ended, Loss: 0.8878039046232\n",
      "Training at epoch: 276 ended, Loss: 0.8879364197851245\n",
      "Training at epoch: 277 ended, Loss: 0.892766668095727\n",
      "Training at epoch: 278 ended, Loss: 0.8811380658807303\n",
      "Training at epoch: 279 ended, Loss: 0.8854275831425048\n",
      "Training at epoch: 280 ended, Loss: 0.8837957371839993\n",
      "--> Testing at epoch: 280 ended, Loss: 0.9560137036684397\n",
      "Training at epoch: 281 ended, Loss: 0.878468431913812\n",
      "Training at epoch: 282 ended, Loss: 0.8833854266731609\n",
      "Training at epoch: 283 ended, Loss: 0.8791979587678209\n",
      "Training at epoch: 284 ended, Loss: 0.8787488192574155\n",
      "Training at epoch: 285 ended, Loss: 0.8721127073715979\n",
      "Training at epoch: 286 ended, Loss: 0.8713648684017691\n",
      "Training at epoch: 287 ended, Loss: 0.8764940201091889\n",
      "Training at epoch: 288 ended, Loss: 0.8766509733598286\n",
      "Training at epoch: 289 ended, Loss: 0.8682832894942154\n",
      "Training at epoch: 290 ended, Loss: 0.8611103792836947\n",
      "--> Testing at epoch: 290 ended, Loss: 0.9663849803241523\n",
      "Training at epoch: 291 ended, Loss: 0.862312179764434\n",
      "Training at epoch: 292 ended, Loss: 0.8629065271047398\n",
      "Training at epoch: 293 ended, Loss: 0.8635775266793706\n",
      "Training at epoch: 294 ended, Loss: 0.8640734538842794\n",
      "Training at epoch: 295 ended, Loss: 0.8625143587385835\n",
      "Training at epoch: 296 ended, Loss: 0.8564360526171052\n",
      "Training at epoch: 297 ended, Loss: 0.8610086813759783\n",
      "Training at epoch: 298 ended, Loss: 0.8611224945316746\n",
      "Training at epoch: 299 ended, Loss: 0.8571412988553784\n",
      "Training at epoch: 300 ended, Loss: 0.855969854951566\n",
      "--> Testing at epoch: 300 ended, Loss: 0.9147140520649988\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "epochs = 300\n",
    "train_model(model=dnn_model, n_epochs=epochs)\n",
    "torch.save(dnn_model.state_dict(), \"../models/dnn_model_min_delay_v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#in_dim = min_delay_dataset.input_shape\n",
    "#out_dim = min_delay_dataset.output_shape\n",
    "#layers_dim = [in_dim, in_dim*2, in_dim*2, in_dim*2, out_dim]\n",
    "#dnn_model = DenseNet(layers_dim)\n",
    "#dnn_model.to(device)\n",
    "#dnn_model.load_state_dict(torch.load(\"../models/\"))\n",
    "\n",
    "# evaluate the model over the entire data and the test data\n",
    "train_error = []\n",
    "test_error = []\n",
    "all_error = []\n",
    "\n",
    "for xx, yy in train_dl:\n",
    "    x_to_pred, y_true = xx.float().to(device), yy.float().to(device)\n",
    "    pred = dnn_model(x_to_pred)\n",
    "    error = torch.sqrt(torch.sum((y_true - pred)**2, dim=1)).to(\"cpu\").detach().numpy()\n",
    "    all_error.extend(error)\n",
    "    train_error.extend(error)\n",
    "\n",
    "for xx, yy in test_dl:\n",
    "    x_to_pred, y_true = xx.float().to(device), yy.float().to(device)\n",
    "    pred = dnn_model(x_to_pred)\n",
    "    error = torch.sqrt(torch.sum((y_true - pred)**2, dim=1)).to(\"cpu\").detach().numpy()\n",
    "    all_error.extend(error)\n",
    "    test_error.extend(error)\n",
    "\n",
    "train_error = np.array(train_error)\n",
    "test_error = np.array(test_error)\n",
    "all_error = np.array(all_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the feature data is (150000, 4, 25, 6)\n",
      "The shape of the target data is: (150000, 3)\n",
      "Random shuffling state: 3\n",
      "The shape of the training data is: (75000, 4, 25, 6)\n",
      "The shape of the testing data is: (75000, 4, 25, 6)\n",
      "new training shape: (75000, 600)\n"
     ]
    }
   ],
   "source": [
    "all_paths_dataset = LoadDataSet(dir_name, path_mode=\"all_paths\")\n",
    "print(f\"Random shuffling state: {all_paths_dataset.rnd_state}\")\n",
    "scaler = StandardScaler()\n",
    "train_set, test_set = all_paths_dataset.get_datasets(scale=True, scaler=scaler)\n",
    "train_dl = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "test_dl = torch.utils.data.DataLoader(test_set, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─ModuleList: 1                          []                        --\n",
      "|    └─Linear: 2-1                       [-1, 1, 256]              153,856\n",
      "|    └─Linear: 2-2                       [-1, 1, 64]               16,448\n",
      "|    └─Linear: 2-3                       [-1, 1, 3]                195\n",
      "==========================================================================================\n",
      "Total params: 170,499\n",
      "Trainable params: 170,499\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.17\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.65\n",
      "Estimated Total Size (MB): 0.66\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "in_dim = all_paths_dataset.input_shape\n",
    "out_dim = all_paths_dataset.output_shape\n",
    "layers_dim = [in_dim, 256, 64, out_dim]\n",
    "dnn_model_all_paths = DenseNet(layers_dim)\n",
    "dnn_model_all_paths.to(device)\n",
    "summary(dnn_model_all_paths, input_data=(1,in_dim), device=device, depth=len(layers_dim))\n",
    "\n",
    "Loss = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(dnn_model_all_paths.parameters(), amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training at epoch: 1 ended, Loss: 0.24351692926329355\n",
      "Training at epoch: 2 ended, Loss: 0.22536540968953903\n",
      "Training at epoch: 3 ended, Loss: 0.25171498694288647\n",
      "Training at epoch: 4 ended, Loss: 0.19519713674369985\n",
      "Training at epoch: 5 ended, Loss: 0.2400540012338615\n",
      "Training at epoch: 6 ended, Loss: 0.21730976752498196\n",
      "Training at epoch: 7 ended, Loss: 0.24354467863127996\n",
      "Training at epoch: 8 ended, Loss: 0.2360339954435037\n",
      "Training at epoch: 9 ended, Loss: 0.2014892561418408\n",
      "Training at epoch: 10 ended, Loss: 0.2359370133957155\n",
      "--> Testing at epoch: 10 ended, Loss: 1.604369408375508\n",
      "Training at epoch: 11 ended, Loss: 0.19893481068745408\n",
      "Training at epoch: 12 ended, Loss: 0.2512045467793687\n",
      "Training at epoch: 13 ended, Loss: 0.2132957461464883\n",
      "Training at epoch: 14 ended, Loss: 0.23372462800141336\n",
      "Training at epoch: 15 ended, Loss: 0.2508550086773856\n",
      "Training at epoch: 16 ended, Loss: 0.21135586220771074\n",
      "Training at epoch: 17 ended, Loss: 0.21320530441319474\n",
      "Training at epoch: 18 ended, Loss: 0.22686957184594037\n",
      "Training at epoch: 19 ended, Loss: 0.26222089634661533\n",
      "Training at epoch: 20 ended, Loss: 0.24592875148433518\n",
      "--> Testing at epoch: 20 ended, Loss: 1.5053523384236\n",
      "Training at epoch: 21 ended, Loss: 0.3065016261578471\n",
      "Training at epoch: 22 ended, Loss: 0.19016222303428088\n",
      "Training at epoch: 23 ended, Loss: 0.21425207369760593\n",
      "Training at epoch: 24 ended, Loss: 0.24205366769525835\n",
      "Training at epoch: 25 ended, Loss: 0.22357162764238722\n",
      "Training at epoch: 26 ended, Loss: 0.21787301197794262\n",
      "Training at epoch: 27 ended, Loss: 0.25941075046257506\n",
      "Training at epoch: 28 ended, Loss: 0.227713981520944\n",
      "Training at epoch: 29 ended, Loss: 0.22294999429568294\n",
      "Training at epoch: 30 ended, Loss: 0.22382333920950725\n",
      "--> Testing at epoch: 30 ended, Loss: 1.4171776143280235\n",
      "Training at epoch: 31 ended, Loss: 0.22252719563435217\n",
      "Training at epoch: 32 ended, Loss: 0.20325281135197537\n",
      "Training at epoch: 33 ended, Loss: 0.24008649067457372\n",
      "Training at epoch: 34 ended, Loss: 0.20614325974943948\n",
      "Training at epoch: 35 ended, Loss: 0.23664474469504665\n",
      "Training at epoch: 36 ended, Loss: 0.20637968078783298\n",
      "Training at epoch: 37 ended, Loss: 0.23253004666125407\n",
      "Training at epoch: 38 ended, Loss: 0.19877315958510486\n",
      "Training at epoch: 39 ended, Loss: 0.18514807361619923\n",
      "Training at epoch: 40 ended, Loss: 0.21292110917924462\n",
      "--> Testing at epoch: 40 ended, Loss: 1.3786847591400146\n",
      "Training at epoch: 41 ended, Loss: 0.20783905603258582\n",
      "Training at epoch: 42 ended, Loss: 0.24338864839600824\n",
      "Training at epoch: 43 ended, Loss: 0.20311169308823224\n",
      "Training at epoch: 44 ended, Loss: 0.21364002026865456\n",
      "Training at epoch: 45 ended, Loss: 0.23308319174891193\n",
      "Training at epoch: 46 ended, Loss: 0.2251455038054862\n",
      "Training at epoch: 47 ended, Loss: 0.20459567835560838\n",
      "Training at epoch: 48 ended, Loss: 0.22654732039560638\n",
      "Training at epoch: 49 ended, Loss: 0.2498004145700955\n",
      "Training at epoch: 50 ended, Loss: 0.23871130791394224\n",
      "--> Testing at epoch: 50 ended, Loss: 1.3209929365564037\n",
      "Training at epoch: 51 ended, Loss: 0.1937028901753216\n",
      "Training at epoch: 52 ended, Loss: 0.23132521420901597\n",
      "Training at epoch: 53 ended, Loss: 0.2060115458030546\n",
      "Training at epoch: 54 ended, Loss: 0.1905924750272655\n",
      "Training at epoch: 55 ended, Loss: 0.23011154461168898\n",
      "Training at epoch: 56 ended, Loss: 0.21017491907904834\n",
      "Training at epoch: 57 ended, Loss: 0.21298727889008082\n",
      "Training at epoch: 58 ended, Loss: 0.189070907343535\n",
      "Training at epoch: 59 ended, Loss: 0.23487746265777243\n",
      "Training at epoch: 60 ended, Loss: 0.19180380912980782\n",
      "--> Testing at epoch: 60 ended, Loss: 1.4973072329083004\n",
      "Training at epoch: 61 ended, Loss: 0.2189865857124354\n",
      "Training at epoch: 62 ended, Loss: 0.2344913085900349\n",
      "Training at epoch: 63 ended, Loss: 0.2225741732042321\n",
      "Training at epoch: 64 ended, Loss: 0.21543181987492044\n",
      "Training at epoch: 65 ended, Loss: 0.24545732920881322\n",
      "Training at epoch: 66 ended, Loss: 0.22103492648630013\n",
      "Training at epoch: 67 ended, Loss: 0.20198669478475234\n",
      "Training at epoch: 68 ended, Loss: 0.2547618593677319\n",
      "Training at epoch: 69 ended, Loss: 0.2261047487293367\n",
      "Training at epoch: 70 ended, Loss: 0.2147983644338827\n",
      "--> Testing at epoch: 70 ended, Loss: 1.5132641252633687\n",
      "Training at epoch: 71 ended, Loss: 0.23609536761615058\n",
      "Training at epoch: 72 ended, Loss: 0.2171734248609012\n",
      "Training at epoch: 73 ended, Loss: 0.1909110892020522\n",
      "Training at epoch: 74 ended, Loss: 0.21757113167367648\n",
      "Training at epoch: 75 ended, Loss: 0.213302384585193\n",
      "Training at epoch: 76 ended, Loss: 0.21973926780122385\n",
      "Training at epoch: 77 ended, Loss: 0.19001561717794566\n",
      "Training at epoch: 78 ended, Loss: 0.215360523150668\n",
      "Training at epoch: 79 ended, Loss: 0.22998635585931584\n",
      "Training at epoch: 80 ended, Loss: 0.22752961846634287\n",
      "--> Testing at epoch: 80 ended, Loss: 1.3813590206004478\n",
      "Training at epoch: 81 ended, Loss: 0.19591397781611608\n",
      "Training at epoch: 82 ended, Loss: 0.24108487187245856\n",
      "Training at epoch: 83 ended, Loss: 0.20306627894036547\n",
      "Training at epoch: 84 ended, Loss: 0.2202419326465844\n",
      "Training at epoch: 85 ended, Loss: 0.18800064816645032\n",
      "Training at epoch: 86 ended, Loss: 0.20297152050309925\n",
      "Training at epoch: 87 ended, Loss: 0.20883314955642657\n",
      "Training at epoch: 88 ended, Loss: 0.23084531971428277\n",
      "Training at epoch: 89 ended, Loss: 0.19846090930116717\n",
      "Training at epoch: 90 ended, Loss: 0.21901119017573042\n",
      "--> Testing at epoch: 90 ended, Loss: 1.3859754220859424\n",
      "Training at epoch: 91 ended, Loss: 0.20256696739980995\n",
      "Training at epoch: 92 ended, Loss: 0.202906857281224\n",
      "Training at epoch: 93 ended, Loss: 0.21586862586934513\n",
      "Training at epoch: 94 ended, Loss: 0.20250938635847116\n",
      "Training at epoch: 95 ended, Loss: 0.21535541935833394\n",
      "Training at epoch: 96 ended, Loss: 0.20915133855544005\n",
      "Training at epoch: 97 ended, Loss: 0.2211211079639405\n",
      "Training at epoch: 98 ended, Loss: 0.22330812765635016\n",
      "Training at epoch: 99 ended, Loss: 0.20230570068080678\n",
      "Training at epoch: 100 ended, Loss: 0.19634330044134993\n",
      "--> Testing at epoch: 100 ended, Loss: 1.3288714732672717\n",
      "Training at epoch: 101 ended, Loss: 0.20468455441327887\n",
      "Training at epoch: 102 ended, Loss: 0.18892900237762736\n",
      "Training at epoch: 103 ended, Loss: 0.21198582792107917\n",
      "Training at epoch: 104 ended, Loss: 0.21838804411323606\n",
      "Training at epoch: 105 ended, Loss: 0.20436904029984268\n",
      "Training at epoch: 106 ended, Loss: 0.19425662056584256\n",
      "Training at epoch: 107 ended, Loss: 0.22620183282687548\n",
      "Training at epoch: 108 ended, Loss: 0.24179966632855274\n",
      "Training at epoch: 109 ended, Loss: 0.17527997602610101\n",
      "Training at epoch: 110 ended, Loss: 0.1921336288363657\n",
      "--> Testing at epoch: 110 ended, Loss: 1.4893271987502639\n",
      "Training at epoch: 111 ended, Loss: 0.2233395881286533\n",
      "Training at epoch: 112 ended, Loss: 0.2166994377166516\n",
      "Training at epoch: 113 ended, Loss: 0.20501116213290205\n",
      "Training at epoch: 114 ended, Loss: 0.2502734836019296\n",
      "Training at epoch: 115 ended, Loss: 0.19576970824285223\n",
      "Training at epoch: 116 ended, Loss: 0.24087629635951802\n",
      "Training at epoch: 117 ended, Loss: 0.24859428238091869\n",
      "Training at epoch: 118 ended, Loss: 0.23873718959396967\n",
      "Training at epoch: 119 ended, Loss: 0.23343271302033863\n",
      "Training at epoch: 120 ended, Loss: 0.21339946426769063\n",
      "--> Testing at epoch: 120 ended, Loss: 1.3357730250100832\n",
      "Training at epoch: 121 ended, Loss: 0.18277848827566812\n",
      "Training at epoch: 122 ended, Loss: 0.270197332231169\n",
      "Training at epoch: 123 ended, Loss: 0.2081596022169821\n",
      "Training at epoch: 124 ended, Loss: 0.20402549856275626\n",
      "Training at epoch: 125 ended, Loss: 0.19942811390213389\n",
      "Training at epoch: 126 ended, Loss: 0.23858317578173283\n",
      "Training at epoch: 127 ended, Loss: 0.2563722212306214\n",
      "Training at epoch: 128 ended, Loss: 0.17503203706237339\n",
      "Training at epoch: 129 ended, Loss: 0.20203787106194188\n",
      "Training at epoch: 130 ended, Loss: 0.22329246968590358\n",
      "--> Testing at epoch: 130 ended, Loss: 1.3912220113986247\n",
      "Training at epoch: 131 ended, Loss: 0.18751315759660186\n",
      "Training at epoch: 132 ended, Loss: 0.20499588703774807\n",
      "Training at epoch: 133 ended, Loss: 0.19871408479930291\n",
      "Training at epoch: 134 ended, Loss: 0.2082330128638894\n",
      "Training at epoch: 135 ended, Loss: 0.1970234241037798\n",
      "Training at epoch: 136 ended, Loss: 0.22311443940548817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training at epoch: 137 ended, Loss: 0.20778641575127332\n",
      "Training at epoch: 138 ended, Loss: 0.27488644217353936\n",
      "Training at epoch: 139 ended, Loss: 0.17776480060604755\n",
      "Training at epoch: 140 ended, Loss: 0.18710489701098246\n",
      "--> Testing at epoch: 140 ended, Loss: 1.359949393449603\n",
      "Training at epoch: 141 ended, Loss: 0.2234882511001449\n",
      "Training at epoch: 142 ended, Loss: 0.22024994396760347\n",
      "Training at epoch: 143 ended, Loss: 0.27886355876503033\n",
      "Training at epoch: 144 ended, Loss: 0.19492961243494278\n",
      "Training at epoch: 145 ended, Loss: 0.17524857402845911\n",
      "Training at epoch: 146 ended, Loss: 0.22395784188373452\n",
      "Training at epoch: 147 ended, Loss: 0.2102837358941499\n",
      "Training at epoch: 148 ended, Loss: 0.21951249553806088\n",
      "Training at epoch: 149 ended, Loss: 0.20563494480841193\n",
      "Training at epoch: 150 ended, Loss: 0.22798984794505575\n",
      "--> Testing at epoch: 150 ended, Loss: 1.3762688584424354\n",
      "Training at epoch: 151 ended, Loss: 0.19941667588618378\n",
      "Training at epoch: 152 ended, Loss: 0.2298833776675419\n",
      "Training at epoch: 153 ended, Loss: 0.18647936268299564\n",
      "Training at epoch: 154 ended, Loss: 0.25201250438390893\n",
      "Training at epoch: 155 ended, Loss: 0.19110280855434847\n",
      "Training at epoch: 156 ended, Loss: 0.22029210259953558\n",
      "Training at epoch: 157 ended, Loss: 0.2193335696733827\n",
      "Training at epoch: 158 ended, Loss: 0.20223354437124658\n",
      "Training at epoch: 159 ended, Loss: 0.18495916025299872\n",
      "Training at epoch: 160 ended, Loss: 0.19394936340737057\n",
      "--> Testing at epoch: 160 ended, Loss: 1.3734810279833305\n",
      "Training at epoch: 161 ended, Loss: 0.20386944392715095\n",
      "Training at epoch: 162 ended, Loss: 0.20467538742424837\n",
      "Training at epoch: 163 ended, Loss: 0.2205752791191607\n",
      "Training at epoch: 164 ended, Loss: 0.2031549988346295\n",
      "Training at epoch: 165 ended, Loss: 0.2508875161771736\n",
      "Training at epoch: 166 ended, Loss: 0.18221351098103938\n",
      "Training at epoch: 167 ended, Loss: 0.1897141920329539\n",
      "Training at epoch: 168 ended, Loss: 0.2091314723737436\n",
      "Training at epoch: 169 ended, Loss: 0.1791245024397003\n",
      "Training at epoch: 170 ended, Loss: 0.20541142884947028\n",
      "--> Testing at epoch: 170 ended, Loss: 1.4772611518163938\n",
      "Training at epoch: 171 ended, Loss: 0.22680451889099623\n",
      "Training at epoch: 172 ended, Loss: 0.23284999819936505\n",
      "Training at epoch: 173 ended, Loss: 0.23199957657649908\n",
      "Training at epoch: 174 ended, Loss: 0.19468744794294593\n",
      "Training at epoch: 175 ended, Loss: 0.20890470833196575\n",
      "Training at epoch: 176 ended, Loss: 0.2416143952852986\n",
      "Training at epoch: 177 ended, Loss: 0.22534146112685471\n",
      "Training at epoch: 178 ended, Loss: 0.19885891277832535\n",
      "Training at epoch: 179 ended, Loss: 0.21510852403531913\n",
      "Training at epoch: 180 ended, Loss: 0.18842278734809456\n",
      "--> Testing at epoch: 180 ended, Loss: 1.4413808592267938\n",
      "Training at epoch: 181 ended, Loss: 0.23042632076800276\n",
      "Training at epoch: 182 ended, Loss: 0.17442470537848237\n",
      "Training at epoch: 183 ended, Loss: 0.19799021013723145\n",
      "Training at epoch: 184 ended, Loss: 0.1974968204557667\n",
      "Training at epoch: 185 ended, Loss: 0.22492832272017926\n",
      "Training at epoch: 186 ended, Loss: 0.20271436738346166\n",
      "Training at epoch: 187 ended, Loss: 0.2087411944207707\n",
      "Training at epoch: 188 ended, Loss: 0.20807106236517176\n",
      "Training at epoch: 189 ended, Loss: 0.2083486536924597\n",
      "Training at epoch: 190 ended, Loss: 0.21488122993113898\n",
      "--> Testing at epoch: 190 ended, Loss: 1.5456740155413344\n",
      "Training at epoch: 191 ended, Loss: 0.18801609539894112\n",
      "Training at epoch: 192 ended, Loss: 0.18702805409551226\n",
      "Training at epoch: 193 ended, Loss: 0.22275990591330436\n",
      "Training at epoch: 194 ended, Loss: 0.18585080029951478\n",
      "Training at epoch: 195 ended, Loss: 0.19125161939634347\n",
      "Training at epoch: 196 ended, Loss: 0.21500249124853632\n",
      "Training at epoch: 197 ended, Loss: 0.18595208564881274\n",
      "Training at epoch: 198 ended, Loss: 0.21673693857394133\n",
      "Training at epoch: 199 ended, Loss: 0.20344238391991565\n",
      "Training at epoch: 200 ended, Loss: 0.187059894321747\n",
      "--> Testing at epoch: 200 ended, Loss: 1.4201211832665108\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "train_loss_history = []\n",
    "test_loss_history = []\n",
    "\n",
    "for epoch in range(200):\n",
    "    total_train_loss = 0\n",
    "    total_test_loss = 0\n",
    "    total_train = 0\n",
    "    total_test = 0\n",
    "\n",
    "    # training loop (iterate over the training set)\n",
    "    for x, y in train_dl:\n",
    "        x_train, y_train = x.float().to(device), y.float().to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        train_output = dnn_model_all_paths(x_train) # network output\n",
    "        loss = Loss(train_output, y_train) # compute loss\n",
    "        loss.backward() # compute gradient with respect to trainable parameters\n",
    "        optimizer.step() # weights update\n",
    "        total_train_loss += loss.item()\n",
    "        total_train += 1\n",
    "\n",
    "    print(f\"Training at epoch: {epoch+1} ended, Loss: {total_train_loss/total_train}\")\n",
    "    # append training loss for the epoch\n",
    "    train_loss_history.append(total_train_loss/total_train)\n",
    "\n",
    "    # testing every 10 training epochs\n",
    "    if ((epoch+1) % 10) == 0:\n",
    "        for x, y in test_dl:\n",
    "            x_test, y_test = x.float().to(device), y.float().to(device)\n",
    "            test_output = dnn_model_all_paths(x_test)\n",
    "            loss = Loss(test_output, y_test)\n",
    "            total_test_loss += loss.item()\n",
    "            total_test += 1\n",
    "        print(f\"--> Testing at epoch: {epoch+1} ended, Loss: {total_test_loss/total_test}\")\n",
    "        # append test loss for the epoch\n",
    "        test_loss_history.append(total_test_loss/total_test)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(dnn_model_all_paths.state_dict(), \"../models/dnn_model_all_paths_v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in_dim = all_paths_dataset.input_shape\n",
    "#out_dim = all_paths_dataset.output_shape\n",
    "#layers_dim = [in_dim, 256, 64, out_dim]\n",
    "#dnn_model_all_paths = DenseNet(layers_dim)\n",
    "#dnn_model_all_paths.to(device)\n",
    "#dnn_model_all_paths.load_state_dict(torch.load(\"../models/dnn_model_all_paths_v2\"))\n",
    "\n",
    "# evaluate the model over the entire data and the test data\n",
    "train_error_all_paths = []\n",
    "test_error_all_paths = []\n",
    "all_error_all_paths = []\n",
    "\n",
    "for xx, yy in train_dl:\n",
    "    x_to_pred, y_true = xx.float().to(device), yy.float().to(device)\n",
    "    pred = dnn_model_all_paths(x_to_pred)\n",
    "    error = torch.sqrt(torch.sum((y_true - pred)**2, dim=1)).to(\"cpu\").detach().numpy()\n",
    "    train_error_all_paths.extend(error)\n",
    "    all_error_all_paths.extend(error)\n",
    "\n",
    "for xx, yy in test_dl:\n",
    "    x_to_pred, y_true = xx.float().to(device), yy.float().to(device)\n",
    "    pred = dnn_model_all_paths(x_to_pred)\n",
    "    error = torch.sqrt(torch.sum((y_true - pred)**2, dim=1)).to(\"cpu\").detach().numpy()\n",
    "    all_error_all_paths.extend(error)\n",
    "    test_error_all_paths.extend(error)\n",
    "\n",
    "train_error_all_paths = np.array(train_error_all_paths)\n",
    "test_error_all_paths = np.array(test_error_all_paths)\n",
    "all_error_all_paths = np.array(all_error_all_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import baseline results\n",
    "true_cord_tensor = sio.loadmat(os.path.abspath(os.path.join(\"../../\"+dir_name, \"all_true_tensor\")))['true_cord_tensor']\n",
    "tot_points = true_cord_tensor.shape[0]*true_cord_tensor.shape[1]\n",
    "true_cord_tensor_r = np.reshape(true_cord_tensor, (tot_points, true_cord_tensor.shape[2]))\n",
    "\n",
    "with open('../models/best_toa_estimated.npy', 'rb') as f:\n",
    "    best_est_3d_coords = np.load(f)\n",
    "    \n",
    "baseline_error = np.sqrt(np.sum((true_cord_tensor_r - best_est_3d_coords)**2, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tikzplotlib\n",
    "\n",
    "figure, ax = plt.subplots(1, 1, figsize=(8, 5)) \n",
    "ax.plot(np.sort(baseline_error), np.linspace(0,1,tot_points), np.sort(all_error), np.linspace(0,1,len(all_error)), \n",
    "       np.sort(all_error_all_paths), np.linspace(0,1,len(all_error_all_paths)))\n",
    "tikzplotlib.clean_figure(fig=figure, target_resolution=600)\n",
    "\n",
    "#plt.plot(np.sort(baseline_error), np.linspace(0,1,tot_points), linewidth=2, label=\"Baseline\")\n",
    "#plt.plot(np.sort(all_error), np.linspace(0,1,len(all_error)), linewidth=2, label=\"Min Delay\")\n",
    "#plt.plot(np.sort(all_error_all_paths), np.linspace(0,1,len(all_error_all_paths)), linewidth=2, label=\"All Paths\")\n",
    "plt.xlim([-2,20])\n",
    "plt.ylabel(\"CDF\", fontsize=12)\n",
    "plt.xlabel(\"Estimation Error [m]\", fontsize=12)\n",
    "plt.title(\"CDF of the position estimation error\")\n",
    "plt.grid()\n",
    "plt.legend([\"Baseline\", \"Min Delay Path\", \"All Paths\"], loc=\"lower right\")\n",
    "\n",
    "tikzplotlib.save(\"dnn_vs_baseline.tex\")\n",
    "# plt.savefig(\"./dnn_vs_baseline.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
