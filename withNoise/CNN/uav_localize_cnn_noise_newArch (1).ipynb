{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "name": "uav_localize_cnn_noise_newArch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e4o1K6WKcev"
      },
      "source": [
        "Downloading data from Google *drive*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuJ4fzmuKc-U"
      },
      "source": [
        "import requests\n",
        "\n",
        "def download_file_from_google_drive(id, destination):\n",
        "    URL = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "\n",
        "    save_response_content(response, destination)    \n",
        "\n",
        "def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "\n",
        "    return None\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "                \n",
        "                \n",
        "file_id = '1GMnPjRUju5YzZEWL5bk9qCrf90vXWEqD'\n",
        "destination = 'data_uav.zip'\n",
        "download_file_from_google_drive(file_id, destination)\n",
        "\n",
        "from zipfile import ZipFile\n",
        "\n",
        "with ZipFile('data_uav.zip', 'r') as zipObj:\n",
        "   # Extract all the contents of zip file in current directory\n",
        "   zipObj.extractall()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efcYrEcSKbIj"
      },
      "source": [
        "Reading the rx power tensor so that we can calculate SNR."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBMrzgNqpvkQ",
        "outputId": "b903f338-0437-439a-9bf7-7c2e16b51af6"
      },
      "source": [
        "\n",
        "import scipy.io\n",
        "import numpy as np\n",
        "\n",
        "num_trajs = 50\n",
        "n_bs = 4\n",
        "n_paths = 25\n",
        "n_time = 3e3\n",
        "n_features = 6\n",
        "\n",
        "\n",
        "mat = scipy.io.loadmat('all_rxpower_tensor_paths.mat')\n",
        "rx_power_tensor = mat['rx_power_tensor'] # dimensions are n_traj x n_time x n_bs x n_paths\n",
        "rx_power_tensor = np.reshape(rx_power_tensor,(int(num_trajs*n_time),n_bs,n_paths))\n",
        "rx_power_tensor[rx_power_tensor==0] = -np.infty # in log scale. Will translate to 0 in watts\n",
        "rx_power_tensor = 10**(0.1*rx_power_tensor) # in Watts\n",
        "\n",
        "\n",
        "#print(np.sum((rx_power_tensor==0.0)))\n",
        "#print(10**(-0.1*np.infty))\n",
        "\n",
        "print(np.min(rx_power_tensor))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhor8fc7Kfaz"
      },
      "source": [
        "Calculating SNR. Non-existent paths have been assigned -250 dB SNR which is very low."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIhvffG1pwcn",
        "outputId": "ba0e72b8-e5a3-401e-8076-3031e8df7120"
      },
      "source": [
        "BW = 400e6 #bandwidth\n",
        "k = 1.38e-23 #boltzmann's constant\n",
        "NF = 10**(0.9) #noise figue \n",
        "T = 298 #temperature\n",
        "\n",
        "NoisePower = k*BW*NF*T\n",
        "\n",
        "\n",
        "SNRs = rx_power_tensor/NoisePower\n",
        "\n",
        "SNRs[SNRs==0.0] = 1e-25 # no paths to -250 dB SNR\n",
        "print(np.sum((SNRs==1e-25))) # total number of non exisitent paths\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "54582\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnkgmbsVKpGd"
      },
      "source": [
        "Reading all other characteristics and adding noise\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MH2DlZeHpy38"
      },
      "source": [
        "\n",
        "import scipy.io\n",
        "import numpy as np\n",
        "\n",
        "num_trajs = 50\n",
        "n_bs = 4\n",
        "n_paths = 25\n",
        "n_time = 3e3\n",
        "n_features = 6\n",
        "\n",
        "mat = scipy.io.loadmat('all_azimuth_aoa_tensor_paths.mat')\n",
        "azimuth_aoa_tensor = mat['azimuth_aoa_tensor'] # dimensions are n_traj x n_time x n_bs x n_paths\n",
        "azimuth_aoa_tensor = np.reshape(azimuth_aoa_tensor,(int(num_trajs*n_time),n_bs,n_paths))\n",
        "azimuth_aoa_tensor = azimuth_aoa_tensor + azimuth_aoa_tensor/np.sqrt(SNRs)*np.random.normal(0, 1, size=(int(num_trajs*n_time),n_bs,n_paths)) #adding noise\n",
        "\n",
        "\n",
        "\n",
        "mat = scipy.io.loadmat('all_azimuth_aod_tensor_paths.mat')\n",
        "azimuth_aod_tensor = mat['azimuth_aod_tensor'] # dimensions are n_traj x n_time x n_bs x n_paths\n",
        "azimuth_aod_tensor = np.reshape(azimuth_aod_tensor,(int(num_trajs*n_time),n_bs,n_paths))\n",
        "azimuth_aod_tensor = azimuth_aod_tensor + azimuth_aod_tensor/np.sqrt(SNRs)*np.random.normal(0, 1, size=(int(num_trajs*n_time),n_bs,n_paths))#adding noise\n",
        "\n",
        "mat = scipy.io.loadmat('all_zenith_aod_tensor_paths.mat')\n",
        "zenith_aod_tensor = mat['zenith_aod_tensor'] # dimensions are n_traj x n_time x n_bs x n_paths\n",
        "zenith_aod_tensor = np.reshape(zenith_aod_tensor,(int(num_trajs*n_time),n_bs,n_paths))\n",
        "zenith_aod_tensor = zenith_aod_tensor + zenith_aod_tensor/np.sqrt(SNRs)*np.random.normal(0, 1, size=(int(num_trajs*n_time),n_bs,n_paths))#adding noise\n",
        "\n",
        "\n",
        "mat = scipy.io.loadmat('all_zenith_aoa_tensor_paths.mat')\n",
        "zenith_aoa_tensor = mat['zenith_aoa_tensor'] # dimensions are n_traj x n_time x n_bs x n_paths\n",
        "zenith_aoa_tensor = np.reshape(zenith_aoa_tensor,(int(num_trajs*n_time),n_bs,n_paths))\n",
        "zenith_aoa_tensor = zenith_aoa_tensor + zenith_aoa_tensor/np.sqrt(SNRs)*np.random.normal(0, 1, size=(int(num_trajs*n_time),n_bs,n_paths))#adding noise\n",
        "\n",
        "\n",
        "mat = scipy.io.loadmat('all_toa_tensor_paths.mat')\n",
        "toa_tensor = mat['toa_tensor'] # dimensions are n_traj x n_time x n_bs x n_paths\n",
        "toa_tensor = np.reshape(toa_tensor,(int(num_trajs*n_time),n_bs,n_paths))*1e3 #in ms\n",
        "toa_tensor = toa_tensor + toa_tensor/np.sqrt(SNRs)*np.random.normal(0, 1, size=(int(num_trajs*n_time),n_bs,n_paths))#adding noise\n",
        "\n",
        "mat = scipy.io.loadmat('all_true_tensor.mat')\n",
        "true_cord_tensor = mat['true_cord_tensor'] # dimensions are n_traj x n_time x 3\n",
        "true_cord_tensor = np.reshape(true_cord_tensor,(int(num_trajs*n_time),3))#adding noise\n",
        "del mat\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luOlJmYXKamz"
      },
      "source": [
        "**Constructing the input tensor with dimensions n_samples x n_bs x n_paths x n_characteristics**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbZpm1r9Kamz",
        "outputId": "5ddba797-e064-470d-9243-ccf8fe69906d"
      },
      "source": [
        "import math\n",
        "input_tensor = np.zeros((int(num_trajs*n_time),n_bs,n_paths,n_features))*math.nan\n",
        "\n",
        "n_samples = int(num_trajs*n_time)\n",
        "\n",
        "\n",
        "input_tensor[:,:,:,0] = azimuth_aoa_tensor\n",
        "del azimuth_aoa_tensor\n",
        "\n",
        "input_tensor[:,:,:,1] = azimuth_aod_tensor\n",
        "del azimuth_aod_tensor\n",
        "\n",
        "input_tensor[:,:,:,2] = zenith_aoa_tensor\n",
        "del zenith_aoa_tensor\n",
        "\n",
        "input_tensor[:,:,:,3] = zenith_aod_tensor\n",
        "del zenith_aod_tensor\n",
        "\n",
        "input_tensor[:,:,:,4] = rx_power_tensor\n",
        "del rx_power_tensor\n",
        "\n",
        "input_tensor[:,:,:,5] = toa_tensor\n",
        "del toa_tensor\n",
        "\n",
        "print(np.sum(np.isnan(input_tensor))) ## making sure that there are no values left unassigned"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tz24FeiNK0fH"
      },
      "source": [
        "Each input sample has a shape 4x25x6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjQN2SDQKam0",
        "outputId": "9f9a5d26-359b-4335-ebe5-933ce645a36d"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "inputShape = input_tensor.shape[1:]\n",
        "print(inputShape)\n",
        "#input_tensor = np.reshape(input_tensor,(n_samples,int(n_bs*n_paths*n_features)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4, 25, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "119h0zRkKam0"
      },
      "source": [
        "**CNN  approach**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxKDn5BPK4DI"
      },
      "source": [
        "Constructing CNN and declaring hyper-parameters. We tried batchnorm and dropout, it resulted in worse performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OU4jRtBfKam1",
        "outputId": "af650ab6-241f-4c5c-c2d5-2aa73a7df920"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "\n",
        "input_tensor = input_tensor \n",
        "true_cord_tensor = true_cord_tensor - np.min(true_cord_tensor,axis = 0) ## assuming we know the minimum coordinates, done for stability of relu \n",
        "true_cord_tensor = true_cord_tensor\n",
        "\n",
        "\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(64, (7,7), padding=\"same\", input_shape=inputShape,activation=\"relu\",strides=1))\n",
        "model.add(layers.Conv2D(64, (7,7), padding=\"same\",activation=\"relu\",strides=1))\n",
        "model.add(layers.Conv2D(32, (5,5), padding=\"same\",activation=\"relu\",strides=1))\n",
        "model.add(layers.Conv2D(16, (5,5), padding=\"same\",activation=\"relu\",strides=1))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(3,activation = 'relu'))\n",
        "print(model.summary())\n",
        "opt = tf.keras.optimizers.Adam()\n",
        "model.compile(loss='mse', optimizer=opt)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 4, 25, 64)         18880     \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 4, 25, 64)         200768    \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 4, 25, 32)         51232     \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 4, 25, 16)         12816     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 3)                 4803      \n",
            "=================================================================\n",
            "Total params: 288,499\n",
            "Trainable params: 288,499\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bi-6Bi8K_uY"
      },
      "source": [
        "Training split. Preprocessing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gl4cmjGLKam1",
        "outputId": "8ec4ad71-5301-48e0-b863-ac422a213e20"
      },
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(np.reshape(input_tensor,(n_samples,int(n_bs*n_paths*n_features))), true_cord_tensor, test_size=0.5)\n",
        "\n",
        "\n",
        "scaler = preprocessing.StandardScaler() # preprocessing data\n",
        "scaled_df = scaler.fit(X_train)\n",
        "scaled_df = scaler.transform(X_train)\n",
        "\n",
        "\n",
        "scaled_df = np.reshape(scaled_df,(scaled_df.shape[0],n_bs,n_paths,n_features))\n",
        "print(scaled_df.shape)\n",
        "# print(np.min(y_train_scaled))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(75000, 4, 25, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jg6oKNMRLGj_"
      },
      "source": [
        "Running training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2DbjK2HKam2",
        "outputId": "9c82eb4e-ce47-4860-a9c6-18511ca9224f"
      },
      "source": [
        "model.fit(scaled_df,y_train, epochs=500, batch_size=32, verbose = 2)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "2344/2344 - 40s - loss: 2556.7651\n",
            "Epoch 2/500\n",
            "2344/2344 - 7s - loss: 483.9175\n",
            "Epoch 3/500\n",
            "2344/2344 - 7s - loss: 316.0674\n",
            "Epoch 4/500\n",
            "2344/2344 - 7s - loss: 235.5771\n",
            "Epoch 5/500\n",
            "2344/2344 - 7s - loss: 183.8432\n",
            "Epoch 6/500\n",
            "2344/2344 - 7s - loss: 136.6416\n",
            "Epoch 7/500\n",
            "2344/2344 - 7s - loss: 110.3335\n",
            "Epoch 8/500\n",
            "2344/2344 - 7s - loss: 96.6700\n",
            "Epoch 9/500\n",
            "2344/2344 - 7s - loss: 76.6817\n",
            "Epoch 10/500\n",
            "2344/2344 - 7s - loss: 68.7926\n",
            "Epoch 11/500\n",
            "2344/2344 - 7s - loss: 62.7374\n",
            "Epoch 12/500\n",
            "2344/2344 - 7s - loss: 57.0542\n",
            "Epoch 13/500\n",
            "2344/2344 - 7s - loss: 55.1825\n",
            "Epoch 14/500\n",
            "2344/2344 - 7s - loss: 46.0012\n",
            "Epoch 15/500\n",
            "2344/2344 - 7s - loss: 42.2077\n",
            "Epoch 16/500\n",
            "2344/2344 - 7s - loss: 38.9065\n",
            "Epoch 17/500\n",
            "2344/2344 - 7s - loss: 35.2855\n",
            "Epoch 18/500\n",
            "2344/2344 - 7s - loss: 33.1095\n",
            "Epoch 19/500\n",
            "2344/2344 - 7s - loss: 33.1723\n",
            "Epoch 20/500\n",
            "2344/2344 - 7s - loss: 28.0629\n",
            "Epoch 21/500\n",
            "2344/2344 - 7s - loss: 27.9113\n",
            "Epoch 22/500\n",
            "2344/2344 - 7s - loss: 28.0423\n",
            "Epoch 23/500\n",
            "2344/2344 - 7s - loss: 23.7482\n",
            "Epoch 24/500\n",
            "2344/2344 - 7s - loss: 26.7767\n",
            "Epoch 25/500\n",
            "2344/2344 - 7s - loss: 22.4493\n",
            "Epoch 26/500\n",
            "2344/2344 - 7s - loss: 21.1379\n",
            "Epoch 27/500\n",
            "2344/2344 - 7s - loss: 21.0572\n",
            "Epoch 28/500\n",
            "2344/2344 - 7s - loss: 22.9663\n",
            "Epoch 29/500\n",
            "2344/2344 - 7s - loss: 23.6457\n",
            "Epoch 30/500\n",
            "2344/2344 - 7s - loss: 17.5107\n",
            "Epoch 31/500\n",
            "2344/2344 - 7s - loss: 17.4336\n",
            "Epoch 32/500\n",
            "2344/2344 - 7s - loss: 21.0936\n",
            "Epoch 33/500\n",
            "2344/2344 - 7s - loss: 15.7849\n",
            "Epoch 34/500\n",
            "2344/2344 - 7s - loss: 16.7363\n",
            "Epoch 35/500\n",
            "2344/2344 - 7s - loss: 14.0025\n",
            "Epoch 36/500\n",
            "2344/2344 - 7s - loss: 20.8451\n",
            "Epoch 37/500\n",
            "2344/2344 - 7s - loss: 14.3610\n",
            "Epoch 38/500\n",
            "2344/2344 - 7s - loss: 15.6099\n",
            "Epoch 39/500\n",
            "2344/2344 - 7s - loss: 17.4696\n",
            "Epoch 40/500\n",
            "2344/2344 - 7s - loss: 11.9201\n",
            "Epoch 41/500\n",
            "2344/2344 - 7s - loss: 14.2968\n",
            "Epoch 42/500\n",
            "2344/2344 - 7s - loss: 24.5385\n",
            "Epoch 43/500\n",
            "2344/2344 - 7s - loss: 11.6820\n",
            "Epoch 44/500\n",
            "2344/2344 - 7s - loss: 12.0927\n",
            "Epoch 45/500\n",
            "2344/2344 - 7s - loss: 12.9107\n",
            "Epoch 46/500\n",
            "2344/2344 - 7s - loss: 12.1912\n",
            "Epoch 47/500\n",
            "2344/2344 - 7s - loss: 12.5985\n",
            "Epoch 48/500\n",
            "2344/2344 - 7s - loss: 13.1083\n",
            "Epoch 49/500\n",
            "2344/2344 - 7s - loss: 10.6197\n",
            "Epoch 50/500\n",
            "2344/2344 - 7s - loss: 11.9455\n",
            "Epoch 51/500\n",
            "2344/2344 - 7s - loss: 12.6437\n",
            "Epoch 52/500\n",
            "2344/2344 - 7s - loss: 10.3081\n",
            "Epoch 53/500\n",
            "2344/2344 - 7s - loss: 12.1016\n",
            "Epoch 54/500\n",
            "2344/2344 - 7s - loss: 10.1885\n",
            "Epoch 55/500\n",
            "2344/2344 - 7s - loss: 9.8502\n",
            "Epoch 56/500\n",
            "2344/2344 - 7s - loss: 12.3269\n",
            "Epoch 57/500\n",
            "2344/2344 - 7s - loss: 9.1281\n",
            "Epoch 58/500\n",
            "2344/2344 - 7s - loss: 8.5571\n",
            "Epoch 59/500\n",
            "2344/2344 - 7s - loss: 10.5236\n",
            "Epoch 60/500\n",
            "2344/2344 - 7s - loss: 10.5810\n",
            "Epoch 61/500\n",
            "2344/2344 - 7s - loss: 12.5938\n",
            "Epoch 62/500\n",
            "2344/2344 - 7s - loss: 8.9238\n",
            "Epoch 63/500\n",
            "2344/2344 - 7s - loss: 9.9941\n",
            "Epoch 64/500\n",
            "2344/2344 - 7s - loss: 8.3565\n",
            "Epoch 65/500\n",
            "2344/2344 - 7s - loss: 8.5659\n",
            "Epoch 66/500\n",
            "2344/2344 - 7s - loss: 10.0726\n",
            "Epoch 67/500\n",
            "2344/2344 - 7s - loss: 7.7281\n",
            "Epoch 68/500\n",
            "2344/2344 - 7s - loss: 10.9388\n",
            "Epoch 69/500\n",
            "2344/2344 - 7s - loss: 6.7720\n",
            "Epoch 70/500\n",
            "2344/2344 - 7s - loss: 9.6913\n",
            "Epoch 71/500\n",
            "2344/2344 - 7s - loss: 8.1980\n",
            "Epoch 72/500\n",
            "2344/2344 - 7s - loss: 7.3373\n",
            "Epoch 73/500\n",
            "2344/2344 - 7s - loss: 9.3671\n",
            "Epoch 74/500\n",
            "2344/2344 - 7s - loss: 6.6629\n",
            "Epoch 75/500\n",
            "2344/2344 - 7s - loss: 6.1861\n",
            "Epoch 76/500\n",
            "2344/2344 - 7s - loss: 7.8822\n",
            "Epoch 77/500\n",
            "2344/2344 - 7s - loss: 8.8638\n",
            "Epoch 78/500\n",
            "2344/2344 - 7s - loss: 6.3958\n",
            "Epoch 79/500\n",
            "2344/2344 - 7s - loss: 7.1006\n",
            "Epoch 80/500\n",
            "2344/2344 - 7s - loss: 8.6615\n",
            "Epoch 81/500\n",
            "2344/2344 - 7s - loss: 9.0872\n",
            "Epoch 82/500\n",
            "2344/2344 - 7s - loss: 5.5931\n",
            "Epoch 83/500\n",
            "2344/2344 - 7s - loss: 6.5153\n",
            "Epoch 84/500\n",
            "2344/2344 - 7s - loss: 8.5656\n",
            "Epoch 85/500\n",
            "2344/2344 - 7s - loss: 6.4139\n",
            "Epoch 86/500\n",
            "2344/2344 - 7s - loss: 7.0982\n",
            "Epoch 87/500\n",
            "2344/2344 - 7s - loss: 5.7605\n",
            "Epoch 88/500\n",
            "2344/2344 - 7s - loss: 6.3967\n",
            "Epoch 89/500\n",
            "2344/2344 - 7s - loss: 8.6148\n",
            "Epoch 90/500\n",
            "2344/2344 - 7s - loss: 6.5125\n",
            "Epoch 91/500\n",
            "2344/2344 - 7s - loss: 5.2743\n",
            "Epoch 92/500\n",
            "2344/2344 - 7s - loss: 8.5265\n",
            "Epoch 93/500\n",
            "2344/2344 - 7s - loss: 5.7845\n",
            "Epoch 94/500\n",
            "2344/2344 - 7s - loss: 6.6499\n",
            "Epoch 95/500\n",
            "2344/2344 - 7s - loss: 5.1254\n",
            "Epoch 96/500\n",
            "2344/2344 - 7s - loss: 6.0603\n",
            "Epoch 97/500\n",
            "2344/2344 - 7s - loss: 6.1400\n",
            "Epoch 98/500\n",
            "2344/2344 - 7s - loss: 5.7668\n",
            "Epoch 99/500\n",
            "2344/2344 - 7s - loss: 7.0562\n",
            "Epoch 100/500\n",
            "2344/2344 - 7s - loss: 7.2739\n",
            "Epoch 101/500\n",
            "2344/2344 - 7s - loss: 4.2017\n",
            "Epoch 102/500\n",
            "2344/2344 - 7s - loss: 5.1084\n",
            "Epoch 103/500\n",
            "2344/2344 - 7s - loss: 5.8270\n",
            "Epoch 104/500\n",
            "2344/2344 - 7s - loss: 5.5664\n",
            "Epoch 105/500\n",
            "2344/2344 - 7s - loss: 4.8361\n",
            "Epoch 106/500\n",
            "2344/2344 - 7s - loss: 5.8725\n",
            "Epoch 107/500\n",
            "2344/2344 - 7s - loss: 7.2989\n",
            "Epoch 108/500\n",
            "2344/2344 - 7s - loss: 5.8626\n",
            "Epoch 109/500\n",
            "2344/2344 - 7s - loss: 4.2071\n",
            "Epoch 110/500\n",
            "2344/2344 - 7s - loss: 5.8314\n",
            "Epoch 111/500\n",
            "2344/2344 - 7s - loss: 4.6121\n",
            "Epoch 112/500\n",
            "2344/2344 - 7s - loss: 4.4401\n",
            "Epoch 113/500\n",
            "2344/2344 - 7s - loss: 4.8438\n",
            "Epoch 114/500\n",
            "2344/2344 - 7s - loss: 6.3346\n",
            "Epoch 115/500\n",
            "2344/2344 - 7s - loss: 4.1019\n",
            "Epoch 116/500\n",
            "2344/2344 - 7s - loss: 4.1072\n",
            "Epoch 117/500\n",
            "2344/2344 - 7s - loss: 5.1673\n",
            "Epoch 118/500\n",
            "2344/2344 - 7s - loss: 4.8078\n",
            "Epoch 119/500\n",
            "2344/2344 - 7s - loss: 4.8281\n",
            "Epoch 120/500\n",
            "2344/2344 - 7s - loss: 4.6570\n",
            "Epoch 121/500\n",
            "2344/2344 - 7s - loss: 5.0175\n",
            "Epoch 122/500\n",
            "2344/2344 - 7s - loss: 4.9292\n",
            "Epoch 123/500\n",
            "2344/2344 - 7s - loss: 4.3056\n",
            "Epoch 124/500\n",
            "2344/2344 - 7s - loss: 5.4064\n",
            "Epoch 125/500\n",
            "2344/2344 - 7s - loss: 4.8992\n",
            "Epoch 126/500\n",
            "2344/2344 - 7s - loss: 4.1224\n",
            "Epoch 127/500\n",
            "2344/2344 - 7s - loss: 6.1745\n",
            "Epoch 128/500\n",
            "2344/2344 - 7s - loss: 7.9266\n",
            "Epoch 129/500\n",
            "2344/2344 - 7s - loss: 4.3516\n",
            "Epoch 130/500\n",
            "2344/2344 - 7s - loss: 3.9350\n",
            "Epoch 131/500\n",
            "2344/2344 - 7s - loss: 7.5501\n",
            "Epoch 132/500\n",
            "2344/2344 - 7s - loss: 4.4134\n",
            "Epoch 133/500\n",
            "2344/2344 - 7s - loss: 3.5765\n",
            "Epoch 134/500\n",
            "2344/2344 - 7s - loss: 3.9290\n",
            "Epoch 135/500\n",
            "2344/2344 - 7s - loss: 5.1725\n",
            "Epoch 136/500\n",
            "2344/2344 - 7s - loss: 4.9981\n",
            "Epoch 137/500\n",
            "2344/2344 - 7s - loss: 3.3021\n",
            "Epoch 138/500\n",
            "2344/2344 - 7s - loss: 4.0260\n",
            "Epoch 139/500\n",
            "2344/2344 - 7s - loss: 4.8287\n",
            "Epoch 140/500\n",
            "2344/2344 - 7s - loss: 3.8868\n",
            "Epoch 141/500\n",
            "2344/2344 - 7s - loss: 3.6950\n",
            "Epoch 142/500\n",
            "2344/2344 - 7s - loss: 4.2619\n",
            "Epoch 143/500\n",
            "2344/2344 - 7s - loss: 3.9228\n",
            "Epoch 144/500\n",
            "2344/2344 - 7s - loss: 3.6328\n",
            "Epoch 145/500\n",
            "2344/2344 - 7s - loss: 4.9249\n",
            "Epoch 146/500\n",
            "2344/2344 - 7s - loss: 3.3046\n",
            "Epoch 147/500\n",
            "2344/2344 - 7s - loss: 3.7323\n",
            "Epoch 148/500\n",
            "2344/2344 - 7s - loss: 5.3459\n",
            "Epoch 149/500\n",
            "2344/2344 - 7s - loss: 3.2608\n",
            "Epoch 150/500\n",
            "2344/2344 - 7s - loss: 3.7882\n",
            "Epoch 151/500\n",
            "2344/2344 - 7s - loss: 4.0742\n",
            "Epoch 152/500\n",
            "2344/2344 - 7s - loss: 3.5719\n",
            "Epoch 153/500\n",
            "2344/2344 - 7s - loss: 3.4071\n",
            "Epoch 154/500\n",
            "2344/2344 - 7s - loss: 4.0863\n",
            "Epoch 155/500\n",
            "2344/2344 - 7s - loss: 4.7276\n",
            "Epoch 156/500\n",
            "2344/2344 - 7s - loss: 3.4084\n",
            "Epoch 157/500\n",
            "2344/2344 - 7s - loss: 3.4221\n",
            "Epoch 158/500\n",
            "2344/2344 - 7s - loss: 3.8232\n",
            "Epoch 159/500\n",
            "2344/2344 - 7s - loss: 4.3612\n",
            "Epoch 160/500\n",
            "2344/2344 - 7s - loss: 3.4049\n",
            "Epoch 161/500\n",
            "2344/2344 - 7s - loss: 4.0630\n",
            "Epoch 162/500\n",
            "2344/2344 - 7s - loss: 3.7006\n",
            "Epoch 163/500\n",
            "2344/2344 - 7s - loss: 3.3134\n",
            "Epoch 164/500\n",
            "2344/2344 - 7s - loss: 4.1642\n",
            "Epoch 165/500\n",
            "2344/2344 - 7s - loss: 3.2856\n",
            "Epoch 166/500\n",
            "2344/2344 - 7s - loss: 3.1796\n",
            "Epoch 167/500\n",
            "2344/2344 - 7s - loss: 4.4004\n",
            "Epoch 168/500\n",
            "2344/2344 - 7s - loss: 3.5361\n",
            "Epoch 169/500\n",
            "2344/2344 - 7s - loss: 3.0061\n",
            "Epoch 170/500\n",
            "2344/2344 - 7s - loss: 4.5641\n",
            "Epoch 171/500\n",
            "2344/2344 - 7s - loss: 3.0461\n",
            "Epoch 172/500\n",
            "2344/2344 - 7s - loss: 4.5393\n",
            "Epoch 173/500\n",
            "2344/2344 - 7s - loss: 2.8530\n",
            "Epoch 174/500\n",
            "2344/2344 - 7s - loss: 3.5525\n",
            "Epoch 175/500\n",
            "2344/2344 - 7s - loss: 3.3690\n",
            "Epoch 176/500\n",
            "2344/2344 - 7s - loss: 3.4659\n",
            "Epoch 177/500\n",
            "2344/2344 - 7s - loss: 3.1916\n",
            "Epoch 178/500\n",
            "2344/2344 - 7s - loss: 3.4383\n",
            "Epoch 179/500\n",
            "2344/2344 - 7s - loss: 3.2072\n",
            "Epoch 180/500\n",
            "2344/2344 - 7s - loss: 3.5961\n",
            "Epoch 181/500\n",
            "2344/2344 - 7s - loss: 3.6021\n",
            "Epoch 182/500\n",
            "2344/2344 - 7s - loss: 3.9506\n",
            "Epoch 183/500\n",
            "2344/2344 - 7s - loss: 3.6946\n",
            "Epoch 184/500\n",
            "2344/2344 - 7s - loss: 3.7397\n",
            "Epoch 185/500\n",
            "2344/2344 - 7s - loss: 2.6804\n",
            "Epoch 186/500\n",
            "2344/2344 - 7s - loss: 2.9183\n",
            "Epoch 187/500\n",
            "2344/2344 - 7s - loss: 3.0331\n",
            "Epoch 188/500\n",
            "2344/2344 - 7s - loss: 3.5798\n",
            "Epoch 189/500\n",
            "2344/2344 - 7s - loss: 3.7711\n",
            "Epoch 190/500\n",
            "2344/2344 - 7s - loss: 3.5175\n",
            "Epoch 191/500\n",
            "2344/2344 - 7s - loss: 3.8668\n",
            "Epoch 192/500\n",
            "2344/2344 - 7s - loss: 3.1829\n",
            "Epoch 193/500\n",
            "2344/2344 - 7s - loss: 4.8281\n",
            "Epoch 194/500\n",
            "2344/2344 - 7s - loss: 4.1717\n",
            "Epoch 195/500\n",
            "2344/2344 - 7s - loss: 3.0272\n",
            "Epoch 196/500\n",
            "2344/2344 - 7s - loss: 3.1365\n",
            "Epoch 197/500\n",
            "2344/2344 - 7s - loss: 3.1096\n",
            "Epoch 198/500\n",
            "2344/2344 - 7s - loss: 3.7728\n",
            "Epoch 199/500\n",
            "2344/2344 - 7s - loss: 3.0901\n",
            "Epoch 200/500\n",
            "2344/2344 - 7s - loss: 2.9390\n",
            "Epoch 201/500\n",
            "2344/2344 - 7s - loss: 3.2725\n",
            "Epoch 202/500\n",
            "2344/2344 - 7s - loss: 3.2521\n",
            "Epoch 203/500\n",
            "2344/2344 - 7s - loss: 5.5368\n",
            "Epoch 204/500\n",
            "2344/2344 - 7s - loss: 2.5444\n",
            "Epoch 205/500\n",
            "2344/2344 - 7s - loss: 2.6390\n",
            "Epoch 206/500\n",
            "2344/2344 - 7s - loss: 3.1375\n",
            "Epoch 207/500\n",
            "2344/2344 - 7s - loss: 3.6217\n",
            "Epoch 208/500\n",
            "2344/2344 - 7s - loss: 3.8624\n",
            "Epoch 209/500\n",
            "2344/2344 - 7s - loss: 2.9275\n",
            "Epoch 210/500\n",
            "2344/2344 - 7s - loss: 2.9335\n",
            "Epoch 211/500\n",
            "2344/2344 - 7s - loss: 3.6697\n",
            "Epoch 212/500\n",
            "2344/2344 - 7s - loss: 3.3395\n",
            "Epoch 213/500\n",
            "2344/2344 - 7s - loss: 2.8023\n",
            "Epoch 214/500\n",
            "2344/2344 - 7s - loss: 3.0464\n",
            "Epoch 215/500\n",
            "2344/2344 - 7s - loss: 4.6774\n",
            "Epoch 216/500\n",
            "2344/2344 - 7s - loss: 2.8834\n",
            "Epoch 217/500\n",
            "2344/2344 - 7s - loss: 2.3334\n",
            "Epoch 218/500\n",
            "2344/2344 - 7s - loss: 4.0062\n",
            "Epoch 219/500\n",
            "2344/2344 - 7s - loss: 2.3051\n",
            "Epoch 220/500\n",
            "2344/2344 - 7s - loss: 3.2426\n",
            "Epoch 221/500\n",
            "2344/2344 - 7s - loss: 2.6148\n",
            "Epoch 222/500\n",
            "2344/2344 - 7s - loss: 3.8615\n",
            "Epoch 223/500\n",
            "2344/2344 - 7s - loss: 4.3719\n",
            "Epoch 224/500\n",
            "2344/2344 - 7s - loss: 2.4612\n",
            "Epoch 225/500\n",
            "2344/2344 - 7s - loss: 2.7978\n",
            "Epoch 226/500\n",
            "2344/2344 - 7s - loss: 2.7996\n",
            "Epoch 227/500\n",
            "2344/2344 - 7s - loss: 3.0437\n",
            "Epoch 228/500\n",
            "2344/2344 - 7s - loss: 2.9635\n",
            "Epoch 229/500\n",
            "2344/2344 - 7s - loss: 3.6916\n",
            "Epoch 230/500\n",
            "2344/2344 - 7s - loss: 2.6128\n",
            "Epoch 231/500\n",
            "2344/2344 - 7s - loss: 2.3995\n",
            "Epoch 232/500\n",
            "2344/2344 - 7s - loss: 3.4529\n",
            "Epoch 233/500\n",
            "2344/2344 - 7s - loss: 2.8579\n",
            "Epoch 234/500\n",
            "2344/2344 - 7s - loss: 3.2382\n",
            "Epoch 235/500\n",
            "2344/2344 - 7s - loss: 2.1547\n",
            "Epoch 236/500\n",
            "2344/2344 - 7s - loss: 2.9822\n",
            "Epoch 237/500\n",
            "2344/2344 - 7s - loss: 3.4526\n",
            "Epoch 238/500\n",
            "2344/2344 - 7s - loss: 3.3567\n",
            "Epoch 239/500\n",
            "2344/2344 - 7s - loss: 2.3015\n",
            "Epoch 240/500\n",
            "2344/2344 - 7s - loss: 2.6809\n",
            "Epoch 241/500\n",
            "2344/2344 - 7s - loss: 2.5896\n",
            "Epoch 242/500\n",
            "2344/2344 - 7s - loss: 2.8054\n",
            "Epoch 243/500\n",
            "2344/2344 - 7s - loss: 38.3773\n",
            "Epoch 244/500\n",
            "2344/2344 - 7s - loss: 1.9450\n",
            "Epoch 245/500\n",
            "2344/2344 - 7s - loss: 2.2328\n",
            "Epoch 246/500\n",
            "2344/2344 - 7s - loss: 2.5635\n",
            "Epoch 247/500\n",
            "2344/2344 - 7s - loss: 2.8283\n",
            "Epoch 248/500\n",
            "2344/2344 - 7s - loss: 3.3004\n",
            "Epoch 249/500\n",
            "2344/2344 - 7s - loss: 2.4557\n",
            "Epoch 250/500\n",
            "2344/2344 - 7s - loss: 2.5976\n",
            "Epoch 251/500\n",
            "2344/2344 - 7s - loss: 2.2385\n",
            "Epoch 252/500\n",
            "2344/2344 - 7s - loss: 4.1549\n",
            "Epoch 253/500\n",
            "2344/2344 - 7s - loss: 2.5733\n",
            "Epoch 254/500\n",
            "2344/2344 - 7s - loss: 2.5353\n",
            "Epoch 255/500\n",
            "2344/2344 - 7s - loss: 2.9204\n",
            "Epoch 256/500\n",
            "2344/2344 - 7s - loss: 3.8799\n",
            "Epoch 257/500\n",
            "2344/2344 - 7s - loss: 2.2544\n",
            "Epoch 258/500\n",
            "2344/2344 - 7s - loss: 2.7630\n",
            "Epoch 259/500\n",
            "2344/2344 - 7s - loss: 2.8330\n",
            "Epoch 260/500\n",
            "2344/2344 - 7s - loss: 2.3400\n",
            "Epoch 261/500\n",
            "2344/2344 - 7s - loss: 2.6888\n",
            "Epoch 262/500\n",
            "2344/2344 - 7s - loss: 2.7083\n",
            "Epoch 263/500\n",
            "2344/2344 - 7s - loss: 3.1804\n",
            "Epoch 264/500\n",
            "2344/2344 - 7s - loss: 2.4486\n",
            "Epoch 265/500\n",
            "2344/2344 - 7s - loss: 2.5297\n",
            "Epoch 266/500\n",
            "2344/2344 - 7s - loss: 2.9740\n",
            "Epoch 267/500\n",
            "2344/2344 - 7s - loss: 2.0823\n",
            "Epoch 268/500\n",
            "2344/2344 - 7s - loss: 3.2885\n",
            "Epoch 269/500\n",
            "2344/2344 - 7s - loss: 2.3331\n",
            "Epoch 270/500\n",
            "2344/2344 - 7s - loss: 2.7532\n",
            "Epoch 271/500\n",
            "2344/2344 - 7s - loss: 2.7678\n",
            "Epoch 272/500\n",
            "2344/2344 - 7s - loss: 2.4197\n",
            "Epoch 273/500\n",
            "2344/2344 - 7s - loss: 3.5116\n",
            "Epoch 274/500\n",
            "2344/2344 - 7s - loss: 2.2172\n",
            "Epoch 275/500\n",
            "2344/2344 - 7s - loss: 2.9856\n",
            "Epoch 276/500\n",
            "2344/2344 - 7s - loss: 2.7755\n",
            "Epoch 277/500\n",
            "2344/2344 - 7s - loss: 2.5223\n",
            "Epoch 278/500\n",
            "2344/2344 - 7s - loss: 2.5700\n",
            "Epoch 279/500\n",
            "2344/2344 - 7s - loss: 2.4651\n",
            "Epoch 280/500\n",
            "2344/2344 - 7s - loss: 2.7838\n",
            "Epoch 281/500\n",
            "2344/2344 - 7s - loss: 2.3554\n",
            "Epoch 282/500\n",
            "2344/2344 - 7s - loss: 2.6963\n",
            "Epoch 283/500\n",
            "2344/2344 - 7s - loss: 2.7317\n",
            "Epoch 284/500\n",
            "2344/2344 - 7s - loss: 3.0345\n",
            "Epoch 285/500\n",
            "2344/2344 - 7s - loss: 2.3861\n",
            "Epoch 286/500\n",
            "2344/2344 - 7s - loss: 2.0973\n",
            "Epoch 287/500\n",
            "2344/2344 - 7s - loss: 3.4488\n",
            "Epoch 288/500\n",
            "2344/2344 - 7s - loss: 2.0277\n",
            "Epoch 289/500\n",
            "2344/2344 - 7s - loss: 2.7542\n",
            "Epoch 290/500\n",
            "2344/2344 - 7s - loss: 7.3684\n",
            "Epoch 291/500\n",
            "2344/2344 - 7s - loss: 2.4860\n",
            "Epoch 292/500\n",
            "2344/2344 - 7s - loss: 2.9432\n",
            "Epoch 293/500\n",
            "2344/2344 - 7s - loss: 2.4833\n",
            "Epoch 294/500\n",
            "2344/2344 - 7s - loss: 2.4718\n",
            "Epoch 295/500\n",
            "2344/2344 - 7s - loss: 3.0531\n",
            "Epoch 296/500\n",
            "2344/2344 - 7s - loss: 3.5606\n",
            "Epoch 297/500\n",
            "2344/2344 - 7s - loss: 2.1587\n",
            "Epoch 298/500\n",
            "2344/2344 - 7s - loss: 3.3270\n",
            "Epoch 299/500\n",
            "2344/2344 - 7s - loss: 2.5325\n",
            "Epoch 300/500\n",
            "2344/2344 - 7s - loss: 3.2281\n",
            "Epoch 301/500\n",
            "2344/2344 - 7s - loss: 2.3722\n",
            "Epoch 302/500\n",
            "2344/2344 - 7s - loss: 2.2713\n",
            "Epoch 303/500\n",
            "2344/2344 - 7s - loss: 2.9440\n",
            "Epoch 304/500\n",
            "2344/2344 - 7s - loss: 3.0670\n",
            "Epoch 305/500\n",
            "2344/2344 - 7s - loss: 3.9632\n",
            "Epoch 306/500\n",
            "2344/2344 - 7s - loss: 15.0007\n",
            "Epoch 307/500\n",
            "2344/2344 - 7s - loss: 2.1989\n",
            "Epoch 308/500\n",
            "2344/2344 - 7s - loss: 2.2543\n",
            "Epoch 309/500\n",
            "2344/2344 - 7s - loss: 2.5360\n",
            "Epoch 310/500\n",
            "2344/2344 - 7s - loss: 2.5338\n",
            "Epoch 311/500\n",
            "2344/2344 - 7s - loss: 4.6522\n",
            "Epoch 312/500\n",
            "2344/2344 - 7s - loss: 2.2672\n",
            "Epoch 313/500\n",
            "2344/2344 - 7s - loss: 5.1944\n",
            "Epoch 314/500\n",
            "2344/2344 - 7s - loss: 1.9820\n",
            "Epoch 315/500\n",
            "2344/2344 - 7s - loss: 3.2355\n",
            "Epoch 316/500\n",
            "2344/2344 - 7s - loss: 2.0525\n",
            "Epoch 317/500\n",
            "2344/2344 - 7s - loss: 2.3322\n",
            "Epoch 318/500\n",
            "2344/2344 - 7s - loss: 2.7416\n",
            "Epoch 319/500\n",
            "2344/2344 - 7s - loss: 3.0915\n",
            "Epoch 320/500\n",
            "2344/2344 - 7s - loss: 2.2359\n",
            "Epoch 321/500\n",
            "2344/2344 - 7s - loss: 2.1762\n",
            "Epoch 322/500\n",
            "2344/2344 - 7s - loss: 3.2171\n",
            "Epoch 323/500\n",
            "2344/2344 - 7s - loss: 1.8965\n",
            "Epoch 324/500\n",
            "2344/2344 - 7s - loss: 2.5081\n",
            "Epoch 325/500\n",
            "2344/2344 - 7s - loss: 3.0836\n",
            "Epoch 326/500\n",
            "2344/2344 - 7s - loss: 2.5766\n",
            "Epoch 327/500\n",
            "2344/2344 - 7s - loss: 2.0713\n",
            "Epoch 328/500\n",
            "2344/2344 - 7s - loss: 2.3613\n",
            "Epoch 329/500\n",
            "2344/2344 - 7s - loss: 2.2071\n",
            "Epoch 330/500\n",
            "2344/2344 - 7s - loss: 2.3956\n",
            "Epoch 331/500\n",
            "2344/2344 - 7s - loss: 3.0660\n",
            "Epoch 332/500\n",
            "2344/2344 - 7s - loss: 3.3153\n",
            "Epoch 333/500\n",
            "2344/2344 - 7s - loss: 1.7979\n",
            "Epoch 334/500\n",
            "2344/2344 - 7s - loss: 2.4948\n",
            "Epoch 335/500\n",
            "2344/2344 - 7s - loss: 2.9539\n",
            "Epoch 336/500\n",
            "2344/2344 - 7s - loss: 2.1821\n",
            "Epoch 337/500\n",
            "2344/2344 - 7s - loss: 3.8037\n",
            "Epoch 338/500\n",
            "2344/2344 - 7s - loss: 1.9980\n",
            "Epoch 339/500\n",
            "2344/2344 - 7s - loss: 3.1597\n",
            "Epoch 340/500\n",
            "2344/2344 - 7s - loss: 2.4337\n",
            "Epoch 341/500\n",
            "2344/2344 - 7s - loss: 2.8537\n",
            "Epoch 342/500\n",
            "2344/2344 - 7s - loss: 2.4608\n",
            "Epoch 343/500\n",
            "2344/2344 - 7s - loss: 2.5407\n",
            "Epoch 344/500\n",
            "2344/2344 - 7s - loss: 3.3365\n",
            "Epoch 345/500\n",
            "2344/2344 - 7s - loss: 2.2071\n",
            "Epoch 346/500\n",
            "2344/2344 - 7s - loss: 2.5941\n",
            "Epoch 347/500\n",
            "2344/2344 - 7s - loss: 2.6743\n",
            "Epoch 348/500\n",
            "2344/2344 - 7s - loss: 1.7266\n",
            "Epoch 349/500\n",
            "2344/2344 - 7s - loss: 3.1334\n",
            "Epoch 350/500\n",
            "2344/2344 - 7s - loss: 2.7246\n",
            "Epoch 351/500\n",
            "2344/2344 - 7s - loss: 2.0818\n",
            "Epoch 352/500\n",
            "2344/2344 - 7s - loss: 2.1274\n",
            "Epoch 353/500\n",
            "2344/2344 - 7s - loss: 2.4157\n",
            "Epoch 354/500\n",
            "2344/2344 - 7s - loss: 2.0727\n",
            "Epoch 355/500\n",
            "2344/2344 - 7s - loss: 3.8808\n",
            "Epoch 356/500\n",
            "2344/2344 - 7s - loss: 2.4526\n",
            "Epoch 357/500\n",
            "2344/2344 - 7s - loss: 2.4705\n",
            "Epoch 358/500\n",
            "2344/2344 - 7s - loss: 2.6104\n",
            "Epoch 359/500\n",
            "2344/2344 - 7s - loss: 2.1331\n",
            "Epoch 360/500\n",
            "2344/2344 - 7s - loss: 4.1623\n",
            "Epoch 361/500\n",
            "2344/2344 - 7s - loss: 3.4321\n",
            "Epoch 362/500\n",
            "2344/2344 - 7s - loss: 2.2218\n",
            "Epoch 363/500\n",
            "2344/2344 - 7s - loss: 2.4176\n",
            "Epoch 364/500\n",
            "2344/2344 - 7s - loss: 2.7645\n",
            "Epoch 365/500\n",
            "2344/2344 - 7s - loss: 2.4853\n",
            "Epoch 366/500\n",
            "2344/2344 - 7s - loss: 2.2136\n",
            "Epoch 367/500\n",
            "2344/2344 - 7s - loss: 2.8264\n",
            "Epoch 368/500\n",
            "2344/2344 - 7s - loss: 1.9149\n",
            "Epoch 369/500\n",
            "2344/2344 - 7s - loss: 2.8760\n",
            "Epoch 370/500\n",
            "2344/2344 - 7s - loss: 2.3437\n",
            "Epoch 371/500\n",
            "2344/2344 - 7s - loss: 2.2193\n",
            "Epoch 372/500\n",
            "2344/2344 - 7s - loss: 2.2820\n",
            "Epoch 373/500\n",
            "2344/2344 - 7s - loss: 2.0590\n",
            "Epoch 374/500\n",
            "2344/2344 - 7s - loss: 2.9075\n",
            "Epoch 375/500\n",
            "2344/2344 - 7s - loss: 3.3004\n",
            "Epoch 376/500\n",
            "2344/2344 - 7s - loss: 2.0011\n",
            "Epoch 377/500\n",
            "2344/2344 - 7s - loss: 2.2010\n",
            "Epoch 378/500\n",
            "2344/2344 - 7s - loss: 2.3369\n",
            "Epoch 379/500\n",
            "2344/2344 - 7s - loss: 2.5200\n",
            "Epoch 380/500\n",
            "2344/2344 - 7s - loss: 2.2567\n",
            "Epoch 381/500\n",
            "2344/2344 - 7s - loss: 3.0221\n",
            "Epoch 382/500\n",
            "2344/2344 - 7s - loss: 1.9303\n",
            "Epoch 383/500\n",
            "2344/2344 - 7s - loss: 2.1606\n",
            "Epoch 384/500\n",
            "2344/2344 - 7s - loss: 4.4183\n",
            "Epoch 385/500\n",
            "2344/2344 - 7s - loss: 1.7219\n",
            "Epoch 386/500\n",
            "2344/2344 - 7s - loss: 2.1749\n",
            "Epoch 387/500\n",
            "2344/2344 - 7s - loss: 1.8935\n",
            "Epoch 388/500\n",
            "2344/2344 - 7s - loss: 3.3182\n",
            "Epoch 389/500\n",
            "2344/2344 - 7s - loss: 1.6019\n",
            "Epoch 390/500\n",
            "2344/2344 - 7s - loss: 2.4179\n",
            "Epoch 391/500\n",
            "2344/2344 - 7s - loss: 1.8517\n",
            "Epoch 392/500\n",
            "2344/2344 - 7s - loss: 2.2387\n",
            "Epoch 393/500\n",
            "2344/2344 - 7s - loss: 2.1875\n",
            "Epoch 394/500\n",
            "2344/2344 - 7s - loss: 2.0798\n",
            "Epoch 395/500\n",
            "2344/2344 - 7s - loss: 3.0545\n",
            "Epoch 396/500\n",
            "2344/2344 - 7s - loss: 2.2535\n",
            "Epoch 397/500\n",
            "2344/2344 - 7s - loss: 1.8037\n",
            "Epoch 398/500\n",
            "2344/2344 - 7s - loss: 2.8498\n",
            "Epoch 399/500\n",
            "2344/2344 - 7s - loss: 2.1685\n",
            "Epoch 400/500\n",
            "2344/2344 - 7s - loss: 2.1955\n",
            "Epoch 401/500\n",
            "2344/2344 - 7s - loss: 1.8339\n",
            "Epoch 402/500\n",
            "2344/2344 - 7s - loss: 1.9837\n",
            "Epoch 403/500\n",
            "2344/2344 - 7s - loss: 3.6703\n",
            "Epoch 404/500\n",
            "2344/2344 - 7s - loss: 1.9570\n",
            "Epoch 405/500\n",
            "2344/2344 - 7s - loss: 2.7476\n",
            "Epoch 406/500\n",
            "2344/2344 - 7s - loss: 1.5144\n",
            "Epoch 407/500\n",
            "2344/2344 - 7s - loss: 1.8568\n",
            "Epoch 408/500\n",
            "2344/2344 - 7s - loss: 2.3182\n",
            "Epoch 409/500\n",
            "2344/2344 - 7s - loss: 1.9760\n",
            "Epoch 410/500\n",
            "2344/2344 - 7s - loss: 2.2937\n",
            "Epoch 411/500\n",
            "2344/2344 - 7s - loss: 1.8423\n",
            "Epoch 412/500\n",
            "2344/2344 - 7s - loss: 2.7295\n",
            "Epoch 413/500\n",
            "2344/2344 - 7s - loss: 3.3370\n",
            "Epoch 414/500\n",
            "2344/2344 - 7s - loss: 1.9012\n",
            "Epoch 415/500\n",
            "2344/2344 - 7s - loss: 2.2020\n",
            "Epoch 416/500\n",
            "2344/2344 - 7s - loss: 2.1909\n",
            "Epoch 417/500\n",
            "2344/2344 - 7s - loss: 2.0531\n",
            "Epoch 418/500\n",
            "2344/2344 - 7s - loss: 1.9904\n",
            "Epoch 419/500\n",
            "2344/2344 - 7s - loss: 2.7974\n",
            "Epoch 420/500\n",
            "2344/2344 - 7s - loss: 2.1197\n",
            "Epoch 421/500\n",
            "2344/2344 - 7s - loss: 1.6052\n",
            "Epoch 422/500\n",
            "2344/2344 - 7s - loss: 1.9714\n",
            "Epoch 423/500\n",
            "2344/2344 - 7s - loss: 2.6069\n",
            "Epoch 424/500\n",
            "2344/2344 - 7s - loss: 2.9072\n",
            "Epoch 425/500\n",
            "2344/2344 - 7s - loss: 1.4987\n",
            "Epoch 426/500\n",
            "2344/2344 - 7s - loss: 2.5778\n",
            "Epoch 427/500\n",
            "2344/2344 - 7s - loss: 1.5811\n",
            "Epoch 428/500\n",
            "2344/2344 - 7s - loss: 2.0010\n",
            "Epoch 429/500\n",
            "2344/2344 - 7s - loss: 2.6222\n",
            "Epoch 430/500\n",
            "2344/2344 - 7s - loss: 1.5014\n",
            "Epoch 431/500\n",
            "2344/2344 - 7s - loss: 1.6452\n",
            "Epoch 432/500\n",
            "2344/2344 - 7s - loss: 2.2763\n",
            "Epoch 433/500\n",
            "2344/2344 - 7s - loss: 2.0415\n",
            "Epoch 434/500\n",
            "2344/2344 - 7s - loss: 1.7630\n",
            "Epoch 435/500\n",
            "2344/2344 - 7s - loss: 2.8488\n",
            "Epoch 436/500\n",
            "2344/2344 - 7s - loss: 1.6133\n",
            "Epoch 437/500\n",
            "2344/2344 - 7s - loss: 2.7360\n",
            "Epoch 438/500\n",
            "2344/2344 - 7s - loss: 1.8426\n",
            "Epoch 439/500\n",
            "2344/2344 - 7s - loss: 2.2233\n",
            "Epoch 440/500\n",
            "2344/2344 - 7s - loss: 1.8324\n",
            "Epoch 441/500\n",
            "2344/2344 - 7s - loss: 1.9345\n",
            "Epoch 442/500\n",
            "2344/2344 - 7s - loss: 1.6264\n",
            "Epoch 443/500\n",
            "2344/2344 - 7s - loss: 2.4299\n",
            "Epoch 444/500\n",
            "2344/2344 - 7s - loss: 1.6877\n",
            "Epoch 445/500\n",
            "2344/2344 - 7s - loss: 2.3219\n",
            "Epoch 446/500\n",
            "2344/2344 - 7s - loss: 1.7522\n",
            "Epoch 447/500\n",
            "2344/2344 - 7s - loss: 1.5694\n",
            "Epoch 448/500\n",
            "2344/2344 - 7s - loss: 2.0996\n",
            "Epoch 449/500\n",
            "2344/2344 - 7s - loss: 2.1989\n",
            "Epoch 450/500\n",
            "2344/2344 - 7s - loss: 2.4944\n",
            "Epoch 451/500\n",
            "2344/2344 - 7s - loss: 1.8819\n",
            "Epoch 452/500\n",
            "2344/2344 - 7s - loss: 2.0488\n",
            "Epoch 453/500\n",
            "2344/2344 - 7s - loss: 1.8467\n",
            "Epoch 454/500\n",
            "2344/2344 - 7s - loss: 1.9257\n",
            "Epoch 455/500\n",
            "2344/2344 - 7s - loss: 1.6770\n",
            "Epoch 456/500\n",
            "2344/2344 - 7s - loss: 2.1266\n",
            "Epoch 457/500\n",
            "2344/2344 - 7s - loss: 2.0627\n",
            "Epoch 458/500\n",
            "2344/2344 - 7s - loss: 2.2913\n",
            "Epoch 459/500\n",
            "2344/2344 - 7s - loss: 1.6794\n",
            "Epoch 460/500\n",
            "2344/2344 - 7s - loss: 2.4787\n",
            "Epoch 461/500\n",
            "2344/2344 - 7s - loss: 1.5330\n",
            "Epoch 462/500\n",
            "2344/2344 - 7s - loss: 1.6413\n",
            "Epoch 463/500\n",
            "2344/2344 - 7s - loss: 2.1743\n",
            "Epoch 464/500\n",
            "2344/2344 - 7s - loss: 1.5241\n",
            "Epoch 465/500\n",
            "2344/2344 - 7s - loss: 2.0042\n",
            "Epoch 466/500\n",
            "2344/2344 - 7s - loss: 3.2791\n",
            "Epoch 467/500\n",
            "2344/2344 - 7s - loss: 1.8550\n",
            "Epoch 468/500\n",
            "2344/2344 - 7s - loss: 1.7506\n",
            "Epoch 469/500\n",
            "2344/2344 - 7s - loss: 4.1313\n",
            "Epoch 470/500\n",
            "2344/2344 - 7s - loss: 2.2978\n",
            "Epoch 471/500\n",
            "2344/2344 - 7s - loss: 2.2761\n",
            "Epoch 472/500\n",
            "2344/2344 - 7s - loss: 1.6457\n",
            "Epoch 473/500\n",
            "2344/2344 - 7s - loss: 2.2195\n",
            "Epoch 474/500\n",
            "2344/2344 - 7s - loss: 3.0085\n",
            "Epoch 475/500\n",
            "2344/2344 - 7s - loss: 1.7848\n",
            "Epoch 476/500\n",
            "2344/2344 - 7s - loss: 1.7833\n",
            "Epoch 477/500\n",
            "2344/2344 - 7s - loss: 1.9902\n",
            "Epoch 478/500\n",
            "2344/2344 - 7s - loss: 1.8991\n",
            "Epoch 479/500\n",
            "2344/2344 - 7s - loss: 2.5754\n",
            "Epoch 480/500\n",
            "2344/2344 - 7s - loss: 1.5059\n",
            "Epoch 481/500\n",
            "2344/2344 - 7s - loss: 2.0011\n",
            "Epoch 482/500\n",
            "2344/2344 - 7s - loss: 1.4764\n",
            "Epoch 483/500\n",
            "2344/2344 - 7s - loss: 1.9992\n",
            "Epoch 484/500\n",
            "2344/2344 - 7s - loss: 2.3470\n",
            "Epoch 485/500\n",
            "2344/2344 - 7s - loss: 1.6350\n",
            "Epoch 486/500\n",
            "2344/2344 - 7s - loss: 1.5919\n",
            "Epoch 487/500\n",
            "2344/2344 - 7s - loss: 2.2982\n",
            "Epoch 488/500\n",
            "2344/2344 - 7s - loss: 2.2320\n",
            "Epoch 489/500\n",
            "2344/2344 - 7s - loss: 2.0971\n",
            "Epoch 490/500\n",
            "2344/2344 - 7s - loss: 1.7594\n",
            "Epoch 491/500\n",
            "2344/2344 - 7s - loss: 2.2657\n",
            "Epoch 492/500\n",
            "2344/2344 - 7s - loss: 1.3905\n",
            "Epoch 493/500\n",
            "2344/2344 - 7s - loss: 1.8023\n",
            "Epoch 494/500\n",
            "2344/2344 - 7s - loss: 1.6757\n",
            "Epoch 495/500\n",
            "2344/2344 - 7s - loss: 1.9641\n",
            "Epoch 496/500\n",
            "2344/2344 - 7s - loss: 2.6033\n",
            "Epoch 497/500\n",
            "2344/2344 - 7s - loss: 1.5113\n",
            "Epoch 498/500\n",
            "2344/2344 - 7s - loss: 2.0248\n",
            "Epoch 499/500\n",
            "2344/2344 - 7s - loss: 1.6713\n",
            "Epoch 500/500\n",
            "2344/2344 - 7s - loss: 2.6193\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f06800eb990>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gl-9eqYrLO-y"
      },
      "source": [
        "Predicting on all data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58PIikBcKam2"
      },
      "source": [
        "scaled_input = np.reshape(input_tensor,(n_samples,int(n_bs*n_paths*n_features))) \n",
        "scaled_input = scaler.transform(scaled_input) # transforming all data\n",
        "scaled_input = np.reshape(scaled_input,(scaled_input.shape[0],n_bs,n_paths,n_features)) # reshaping for 2DCNN\n",
        "pred_vals = model.predict(scaled_input) # predicting UAV position\n",
        "\n",
        "# pred_train = model.predict(scaled_df)\n",
        "# scaled_test =scaler.transform(X_test)\n",
        "# scaled_test = np.reshape(scaled_test,(scaled_test.shape[0],n_bs,n_paths,n_features))\n",
        "# pred_test = model.predict(scaled_test)\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GIhMUgrLWV0"
      },
      "source": [
        "In case we need to calculate errors in training, testing, overall. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_t2mWBSvKam3"
      },
      "source": [
        "#print(X_test.shape)\n",
        "#pred_vals = scaler.inverse_transform(pred_vals)\n",
        "#pred_train = scaler.inverse_transform(pred_train)\n",
        "#pred_test = scaler.inverse_transform(pred_test)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2V0bnAvLcFK"
      },
      "source": [
        "Calculating norm of the errors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pS1C-nsIKam3"
      },
      "source": [
        "#pred_vals = outScaler.inverse_transform(pred_vals)\n",
        "norm_error = np.linalg.norm(pred_vals - true_cord_tensor,axis = 1)\n",
        "# norm_error_train = np.linalg.norm(pred_train - y_train,axis = 1)\n",
        "# norm_error_test = np.linalg.norm(pred_test - y_test,axis = 1)\n",
        "# print(norm_error.shape)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8I4C_hhLgUB"
      },
      "source": [
        "Constructing CDFs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "xqaxOOBFKam4",
        "outputId": "6af42f83-b878-4b68-8c2f-eea18f351686"
      },
      "source": [
        "from statsmodels.distributions.empirical_distribution import ECDF\n",
        "\n",
        "cdf = ECDF(norm_error/1)\n",
        "plt.plot(cdf.x,cdf.y)\n",
        "plt.grid()\n",
        "plt.xlim([0,20])\n",
        "\n",
        "# cdf = ECDF(norm_error_train/1)\n",
        "# plt.plot(cdf.x,cdf.y)\n",
        "# plt.grid()\n",
        "# plt.xlim([0,20])\n",
        "\n",
        "\n",
        "# cdf = ECDF(norm_error_test/1)\n",
        "# plt.plot(cdf.x,cdf.y)\n",
        "# plt.grid()\n",
        "# plt.xlim([0,6])\n",
        "\n",
        "\n",
        "# mat = scipy.io.loadmat('overall_DNN.mat')\n",
        "# err_data = mat['norm_error']\n",
        "# err_data = (err_data).flatten()\n",
        "# cdf = ECDF(err_data*1)\n",
        "# plt.plot(cdf.x,cdf.y)\n",
        "\n",
        "\n",
        "# mat = scipy.io.loadmat('baselineerror.mat')\n",
        "# err_data = mat['err_data']\n",
        "# err_data = (err_data).flatten()\n",
        "# cdf = ECDF(err_data*1)\n",
        "# plt.plot(cdf.x,cdf.y)\n",
        "\n",
        "\n",
        "\n",
        "plt.xlabel('Error in meters')\n",
        "plt.ylabel('CDF')\n",
        "plt.legend(['CNN','NN','baseline'])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f0637fba610>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiZJOBolLi90"
      },
      "source": [
        "Mean error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lePWwt2IKam4",
        "outputId": "e01c1245-3c49-43f8-a65f-a7d0022670fe"
      },
      "source": [
        "print(np.mean(norm_error))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.4008620861180132\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQS2snFRLkOx"
      },
      "source": [
        "80% CDF intersection at the following point."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlaB2iBjKam5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "232ddcf4-1186-4962-bd0c-3f0e7fa98fa5"
      },
      "source": [
        "cdf = ECDF(norm_error/1)\n",
        "y = cdf.y\n",
        "indx = cdf.x[y>0.8]\n",
        "#print(\" {0}\":)\n",
        "print(\"The 80% percentile error in meters is \", indx[0], end='')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The 80% percentile error in meters is  1.5470220975172526"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kK10B761Lpum"
      },
      "source": [
        "Saving error array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqQ5o6COIc8e"
      },
      "source": [
        "scipy.io.savemat('2DCNN_with_noise_newarch_error.mat', {'norm_error':norm_error})"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0zhhUO8Ousa"
      },
      "source": [
        "Checking error array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ssReTeAIyTZ",
        "outputId": "bca25e8a-6afe-407a-f7a1-6cbcf035498e"
      },
      "source": [
        "err = scipy.io.loadmat('2DCNN_with_noise_newarch_error.mat')\n",
        "err = err['norm_error']\n",
        "print(np.mean(err))\n",
        "cdf = ECDF(err.flatten())\n",
        "y = cdf.y\n",
        "indx = cdf.x[y>0.8]\n",
        "# #print(\" {0}\":)\n",
        "print(\"The 80% percentile error in meters is \", indx[0], end='')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.4008620861180132\n",
            "The 80% percentile error in meters is  1.5470220975172526"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UO_1g5UUHO2"
      },
      "source": [
        ""
      ],
      "execution_count": 18,
      "outputs": []
    }
  ]
}