{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "name": "uav_localize_cnn_noise.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLiQNKQcKamv"
      },
      "source": [
        "**Importing Data from .mat files**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e4o1K6WKcev"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuJ4fzmuKc-U"
      },
      "source": [
        "import requests\n",
        "\n",
        "def download_file_from_google_drive(id, destination):\n",
        "    URL = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "\n",
        "    save_response_content(response, destination)    \n",
        "\n",
        "def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "\n",
        "    return None\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "                \n",
        "                \n",
        "file_id = '1GMnPjRUju5YzZEWL5bk9qCrf90vXWEqD'\n",
        "destination = 'data_uav.zip'\n",
        "download_file_from_google_drive(file_id, destination)\n",
        "\n",
        "from zipfile import ZipFile\n",
        "\n",
        "with ZipFile('data_uav.zip', 'r') as zipObj:\n",
        "   # Extract all the contents of zip file in current directory\n",
        "   zipObj.extractall()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBMrzgNqpvkQ",
        "outputId": "57fd1113-57b9-497e-b50c-84bc599194d2"
      },
      "source": [
        "\n",
        "import scipy.io\n",
        "import numpy as np\n",
        "\n",
        "num_trajs = 50\n",
        "n_bs = 4\n",
        "n_paths = 25\n",
        "n_time = 3e3\n",
        "n_features = 6\n",
        "\n",
        "\n",
        "mat = scipy.io.loadmat('all_rxpower_tensor_paths.mat')\n",
        "rx_power_tensor = mat['rx_power_tensor'] # dimensions are n_traj x n_time x n_bs x n_paths\n",
        "rx_power_tensor = np.reshape(rx_power_tensor,(int(num_trajs*n_time),n_bs,n_paths))\n",
        "rx_power_tensor[rx_power_tensor==0] = -np.infty\n",
        "rx_power_tensor = 10**(0.1*rx_power_tensor) # in Watts\n",
        "\n",
        "\n",
        "#print(np.sum((rx_power_tensor==0.0)))\n",
        "#print(10**(-0.1*np.infty))\n",
        "\n",
        "print(np.min(rx_power_tensor))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIhvffG1pwcn",
        "outputId": "b6f64bd3-47c4-4d86-f3f6-026f6dfe02f0"
      },
      "source": [
        "BW = 400e6\n",
        "k = 1.38e-23\n",
        "NF = 10**(0.9)\n",
        "T = 298\n",
        "\n",
        "NoisePower = k*BW*NF*T\n",
        "\n",
        "\n",
        "SNRs = rx_power_tensor/NoisePower\n",
        "\n",
        "SNRs[SNRs==0.0] = 1e-25 # no paths to -250 dB SNR\n",
        "print(np.sum((SNRs==1e-25)))\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "54582\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MH2DlZeHpy38"
      },
      "source": [
        "\n",
        "import scipy.io\n",
        "import numpy as np\n",
        "\n",
        "num_trajs = 50\n",
        "n_bs = 4\n",
        "n_paths = 25\n",
        "n_time = 3e3\n",
        "n_features = 6\n",
        "\n",
        "mat = scipy.io.loadmat('all_azimuth_aoa_tensor_paths.mat')\n",
        "azimuth_aoa_tensor = mat['azimuth_aoa_tensor'] # dimensions are n_traj x n_time x n_bs x n_paths\n",
        "azimuth_aoa_tensor = np.reshape(azimuth_aoa_tensor,(int(num_trajs*n_time),n_bs,n_paths))\n",
        "azimuth_aoa_tensor = azimuth_aoa_tensor + azimuth_aoa_tensor/np.sqrt(SNRs)*np.random.normal(0, 1, size=(int(num_trajs*n_time),n_bs,n_paths))\n",
        "\n",
        "\n",
        "\n",
        "mat = scipy.io.loadmat('all_azimuth_aod_tensor_paths.mat')\n",
        "azimuth_aod_tensor = mat['azimuth_aod_tensor'] # dimensions are n_traj x n_time x n_bs x n_paths\n",
        "azimuth_aod_tensor = np.reshape(azimuth_aod_tensor,(int(num_trajs*n_time),n_bs,n_paths))\n",
        "azimuth_aod_tensor = azimuth_aod_tensor + azimuth_aod_tensor/np.sqrt(SNRs)*np.random.normal(0, 1, size=(int(num_trajs*n_time),n_bs,n_paths))\n",
        "\n",
        "mat = scipy.io.loadmat('all_zenith_aod_tensor_paths.mat')\n",
        "zenith_aod_tensor = mat['zenith_aod_tensor'] # dimensions are n_traj x n_time x n_bs x n_paths\n",
        "zenith_aod_tensor = np.reshape(zenith_aod_tensor,(int(num_trajs*n_time),n_bs,n_paths))\n",
        "zenith_aod_tensor = zenith_aod_tensor + zenith_aod_tensor/np.sqrt(SNRs)*np.random.normal(0, 1, size=(int(num_trajs*n_time),n_bs,n_paths))\n",
        "\n",
        "\n",
        "mat = scipy.io.loadmat('all_zenith_aoa_tensor_paths.mat')\n",
        "zenith_aoa_tensor = mat['zenith_aoa_tensor'] # dimensions are n_traj x n_time x n_bs x n_paths\n",
        "zenith_aoa_tensor = np.reshape(zenith_aoa_tensor,(int(num_trajs*n_time),n_bs,n_paths))\n",
        "zenith_aoa_tensor = zenith_aoa_tensor + zenith_aoa_tensor/np.sqrt(SNRs)*np.random.normal(0, 1, size=(int(num_trajs*n_time),n_bs,n_paths))\n",
        "\n",
        "# mat = scipy.io.loadmat('all_rxpower_tensor_paths.mat')\n",
        "# rx_power_tensor = mat['rx_power_tensor'] # dimensions are n_traj x n_time x n_bs x n_paths\n",
        "# rx_power_tensor = np.reshape(rx_power_tensor,(int(num_trajs*n_time),n_bs,n_paths))\n",
        "# rx_power_tensor = 10**(0.1*rx_power_tensor) # in Watts\n",
        "\n",
        "mat = scipy.io.loadmat('all_toa_tensor_paths.mat')\n",
        "toa_tensor = mat['toa_tensor'] # dimensions are n_traj x n_time x n_bs x n_paths\n",
        "toa_tensor = np.reshape(toa_tensor,(int(num_trajs*n_time),n_bs,n_paths))*1e3 #in ms\n",
        "toa_tensor = toa_tensor + toa_tensor/np.sqrt(SNRs)*np.random.normal(0, 1, size=(int(num_trajs*n_time),n_bs,n_paths))\n",
        "\n",
        "mat = scipy.io.loadmat('all_true_tensor.mat')\n",
        "true_cord_tensor = mat['true_cord_tensor'] # dimensions are n_traj x n_time x 3\n",
        "true_cord_tensor = np.reshape(true_cord_tensor,(int(num_trajs*n_time),3))\n",
        "del mat\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luOlJmYXKamz"
      },
      "source": [
        "**Constructing the input tensor with dimensions n_samples x n_paths x n_bs x n_features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbZpm1r9Kamz",
        "outputId": "50b1c16b-ed96-485b-bbc2-91c6a0cdc570"
      },
      "source": [
        "import math\n",
        "input_tensor = np.zeros((int(num_trajs*n_time),n_bs,n_paths,n_features))*math.nan\n",
        "\n",
        "n_samples = int(num_trajs*n_time)\n",
        "\n",
        "\n",
        "input_tensor[:,:,:,0] = azimuth_aoa_tensor\n",
        "del azimuth_aoa_tensor\n",
        "\n",
        "input_tensor[:,:,:,1] = azimuth_aod_tensor\n",
        "del azimuth_aod_tensor\n",
        "\n",
        "input_tensor[:,:,:,2] = zenith_aoa_tensor\n",
        "del zenith_aoa_tensor\n",
        "\n",
        "input_tensor[:,:,:,3] = zenith_aod_tensor\n",
        "del zenith_aod_tensor\n",
        "\n",
        "input_tensor[:,:,:,4] = rx_power_tensor\n",
        "del rx_power_tensor\n",
        "\n",
        "input_tensor[:,:,:,5] = toa_tensor\n",
        "del toa_tensor\n",
        "\n",
        "print(np.sum(np.isnan(input_tensor))) ## making sure that there are no values left unassigned"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjQN2SDQKam0",
        "outputId": "3d45733d-046c-4ba6-ccae-376ed947f682"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "inputShape = input_tensor.shape[1:]\n",
        "print(inputShape)\n",
        "#input_tensor = np.reshape(input_tensor,(n_samples,int(n_bs*n_paths*n_features)))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4, 25, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "119h0zRkKam0"
      },
      "source": [
        "**CNN  approach**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OU4jRtBfKam1",
        "outputId": "3adfa1fa-1d79-4ee1-ff04-b4fff3641b3b"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "\n",
        "input_tensor = input_tensor \n",
        "true_cord_tensor = true_cord_tensor - np.min(true_cord_tensor,axis = 0) ## assuming we know the minimum coordinates\n",
        "#of UE \n",
        "true_cord_tensor = true_cord_tensor*1\n",
        "\n",
        "\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(64, (7,7), padding=\"same\", input_shape=inputShape,activation=\"relu\",strides=1))\n",
        "#model.add(layers.Dropout(0.2))\n",
        "model.add(layers.Conv2D(32, (5,5), padding=\"same\",activation=\"relu\",strides=1))\n",
        "#model.add(layers.Dropout(0.2))\n",
        "model.add(layers.Conv2D(16, (5,5), padding=\"same\",activation=\"relu\",strides=1))\n",
        "#model.add(layers.Conv2D(16, (5,5), padding=\"same\",activation=\"relu\",strides=1))\n",
        "#model.add(layers.Dropout(0.2))\n",
        "# model.add(layers.Conv2D(32, (3,3), padding=\"same\",activation=\"relu\"))\n",
        "# model.add(layers.Dropout(0.2))\n",
        "# model.add(layers.Conv2D(32, (3,3), padding=\"same\",activation=\"relu\"))\n",
        "# model.add(layers.Dropout(0.2))\n",
        "# model.add(layers.Conv2D(32, (5,5), padding=\"same\",activation=\"relu\"))\n",
        "# model.add(layers.Dropout(0.2))\n",
        "# model.add(layers.Conv2D(32, (5,5), padding=\"same\",activation=\"relu\"))\n",
        "# model.add(layers.Dropout(0.2))\n",
        "# model.add(layers.Conv2D(8, (3, 3), padding=\"same\",activation=\"relu\"))\n",
        "# model.add(layers.Dropout(0.2))\n",
        "#model.add(layers.Dense(64,activation = 'relu'))\n",
        "model.add(layers.Flatten())\n",
        "#model.add(layers.Dropout(0.2))\n",
        "# model.add(layers.Dense(16,activation = 'relu'))\n",
        "#model.add(layers.Dense(16,activation = 'relu'))\n",
        "model.add(layers.Dense(3,activation = 'relu'))\n",
        "print(model.summary())\n",
        "opt = tf.keras.optimizers.Adam()\n",
        "model.compile(loss='mse', optimizer=opt)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 4, 25, 64)         18880     \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 4, 25, 32)         51232     \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 4, 25, 16)         12816     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 3)                 4803      \n",
            "=================================================================\n",
            "Total params: 87,731\n",
            "Trainable params: 87,731\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gl4cmjGLKam1",
        "outputId": "4ecc8cb5-7f85-4467-f966-b57440e4f598"
      },
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(np.reshape(input_tensor,(n_samples,int(n_bs*n_paths*n_features))), true_cord_tensor, test_size=0.5)\n",
        "\n",
        "\n",
        "scaler = preprocessing.StandardScaler()\n",
        "scaled_df = scaler.fit(X_train)\n",
        "scaled_df = scaler.transform(X_train)\n",
        "\n",
        "# outScaler = preprocessing.StandardScaler()\n",
        "# y_train_scaled = outScaler.fit(y_train)\n",
        "# y_train_scaled = outScaler.transform(y_train)\n",
        "\n",
        "scaled_df = np.reshape(scaled_df,(scaled_df.shape[0],n_bs,n_paths,n_features))\n",
        "print(scaled_df.shape)\n",
        "# print(np.min(y_train_scaled))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(75000, 4, 25, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2DbjK2HKam2",
        "outputId": "e80fb8a1-5265-4ad6-f286-860e36b0cace"
      },
      "source": [
        "model.fit(scaled_df,y_train, epochs=300, batch_size=32, verbose = 2)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "2344/2344 - 36s - loss: 3624.7053\n",
            "Epoch 2/300\n",
            "2344/2344 - 5s - loss: 799.2570\n",
            "Epoch 3/300\n",
            "2344/2344 - 5s - loss: 572.3821\n",
            "Epoch 4/300\n",
            "2344/2344 - 6s - loss: 388.3453\n",
            "Epoch 5/300\n",
            "2344/2344 - 6s - loss: 293.5453\n",
            "Epoch 6/300\n",
            "2344/2344 - 6s - loss: 240.7262\n",
            "Epoch 7/300\n",
            "2344/2344 - 6s - loss: 195.6544\n",
            "Epoch 8/300\n",
            "2344/2344 - 6s - loss: 170.1589\n",
            "Epoch 9/300\n",
            "2344/2344 - 6s - loss: 137.7402\n",
            "Epoch 10/300\n",
            "2344/2344 - 6s - loss: 117.0712\n",
            "Epoch 11/300\n",
            "2344/2344 - 6s - loss: 99.3119\n",
            "Epoch 12/300\n",
            "2344/2344 - 6s - loss: 91.4878\n",
            "Epoch 13/300\n",
            "2344/2344 - 5s - loss: 77.6610\n",
            "Epoch 14/300\n",
            "2344/2344 - 5s - loss: 69.2875\n",
            "Epoch 15/300\n",
            "2344/2344 - 5s - loss: 66.4863\n",
            "Epoch 16/300\n",
            "2344/2344 - 5s - loss: 57.9600\n",
            "Epoch 17/300\n",
            "2344/2344 - 5s - loss: 53.7480\n",
            "Epoch 18/300\n",
            "2344/2344 - 6s - loss: 50.5825\n",
            "Epoch 19/300\n",
            "2344/2344 - 6s - loss: 47.2119\n",
            "Epoch 20/300\n",
            "2344/2344 - 5s - loss: 46.4486\n",
            "Epoch 21/300\n",
            "2344/2344 - 5s - loss: 42.5335\n",
            "Epoch 22/300\n",
            "2344/2344 - 5s - loss: 39.0984\n",
            "Epoch 23/300\n",
            "2344/2344 - 5s - loss: 40.4515\n",
            "Epoch 24/300\n",
            "2344/2344 - 6s - loss: 38.2272\n",
            "Epoch 25/300\n",
            "2344/2344 - 6s - loss: 35.8307\n",
            "Epoch 26/300\n",
            "2344/2344 - 5s - loss: 32.2240\n",
            "Epoch 27/300\n",
            "2344/2344 - 6s - loss: 32.7572\n",
            "Epoch 28/300\n",
            "2344/2344 - 5s - loss: 32.7302\n",
            "Epoch 29/300\n",
            "2344/2344 - 5s - loss: 28.6014\n",
            "Epoch 30/300\n",
            "2344/2344 - 5s - loss: 28.5653\n",
            "Epoch 31/300\n",
            "2344/2344 - 5s - loss: 28.8012\n",
            "Epoch 32/300\n",
            "2344/2344 - 6s - loss: 28.4294\n",
            "Epoch 33/300\n",
            "2344/2344 - 5s - loss: 26.2746\n",
            "Epoch 34/300\n",
            "2344/2344 - 5s - loss: 24.7287\n",
            "Epoch 35/300\n",
            "2344/2344 - 5s - loss: 25.7723\n",
            "Epoch 36/300\n",
            "2344/2344 - 5s - loss: 27.9958\n",
            "Epoch 37/300\n",
            "2344/2344 - 5s - loss: 22.4326\n",
            "Epoch 38/300\n",
            "2344/2344 - 5s - loss: 21.9475\n",
            "Epoch 39/300\n",
            "2344/2344 - 5s - loss: 25.7524\n",
            "Epoch 40/300\n",
            "2344/2344 - 5s - loss: 23.0646\n",
            "Epoch 41/300\n",
            "2344/2344 - 5s - loss: 22.2865\n",
            "Epoch 42/300\n",
            "2344/2344 - 5s - loss: 20.8035\n",
            "Epoch 43/300\n",
            "2344/2344 - 6s - loss: 21.0315\n",
            "Epoch 44/300\n",
            "2344/2344 - 6s - loss: 20.2210\n",
            "Epoch 45/300\n",
            "2344/2344 - 6s - loss: 20.3388\n",
            "Epoch 46/300\n",
            "2344/2344 - 5s - loss: 19.5022\n",
            "Epoch 47/300\n",
            "2344/2344 - 5s - loss: 40.0886\n",
            "Epoch 48/300\n",
            "2344/2344 - 5s - loss: 26.2611\n",
            "Epoch 49/300\n",
            "2344/2344 - 6s - loss: 17.0222\n",
            "Epoch 50/300\n",
            "2344/2344 - 5s - loss: 16.6566\n",
            "Epoch 51/300\n",
            "2344/2344 - 6s - loss: 16.7119\n",
            "Epoch 52/300\n",
            "2344/2344 - 5s - loss: 18.9383\n",
            "Epoch 53/300\n",
            "2344/2344 - 5s - loss: 16.8687\n",
            "Epoch 54/300\n",
            "2344/2344 - 6s - loss: 18.2727\n",
            "Epoch 55/300\n",
            "2344/2344 - 5s - loss: 16.7185\n",
            "Epoch 56/300\n",
            "2344/2344 - 6s - loss: 17.3717\n",
            "Epoch 57/300\n",
            "2344/2344 - 6s - loss: 15.7368\n",
            "Epoch 58/300\n",
            "2344/2344 - 6s - loss: 17.2815\n",
            "Epoch 59/300\n",
            "2344/2344 - 6s - loss: 15.6414\n",
            "Epoch 60/300\n",
            "2344/2344 - 6s - loss: 15.4124\n",
            "Epoch 61/300\n",
            "2344/2344 - 5s - loss: 14.7175\n",
            "Epoch 62/300\n",
            "2344/2344 - 5s - loss: 13.8520\n",
            "Epoch 63/300\n",
            "2344/2344 - 5s - loss: 15.1331\n",
            "Epoch 64/300\n",
            "2344/2344 - 5s - loss: 15.8017\n",
            "Epoch 65/300\n",
            "2344/2344 - 6s - loss: 16.0992\n",
            "Epoch 66/300\n",
            "2344/2344 - 6s - loss: 13.7395\n",
            "Epoch 67/300\n",
            "2344/2344 - 6s - loss: 13.3600\n",
            "Epoch 68/300\n",
            "2344/2344 - 6s - loss: 14.4233\n",
            "Epoch 69/300\n",
            "2344/2344 - 5s - loss: 13.5161\n",
            "Epoch 70/300\n",
            "2344/2344 - 5s - loss: 14.1793\n",
            "Epoch 71/300\n",
            "2344/2344 - 5s - loss: 13.8038\n",
            "Epoch 72/300\n",
            "2344/2344 - 6s - loss: 12.3943\n",
            "Epoch 73/300\n",
            "2344/2344 - 6s - loss: 12.2149\n",
            "Epoch 74/300\n",
            "2344/2344 - 5s - loss: 12.6862\n",
            "Epoch 75/300\n",
            "2344/2344 - 6s - loss: 12.8064\n",
            "Epoch 76/300\n",
            "2344/2344 - 6s - loss: 12.6256\n",
            "Epoch 77/300\n",
            "2344/2344 - 6s - loss: 12.1011\n",
            "Epoch 78/300\n",
            "2344/2344 - 6s - loss: 11.8949\n",
            "Epoch 79/300\n",
            "2344/2344 - 6s - loss: 12.5009\n",
            "Epoch 80/300\n",
            "2344/2344 - 6s - loss: 11.2695\n",
            "Epoch 81/300\n",
            "2344/2344 - 5s - loss: 12.0692\n",
            "Epoch 82/300\n",
            "2344/2344 - 5s - loss: 12.0410\n",
            "Epoch 83/300\n",
            "2344/2344 - 5s - loss: 10.3669\n",
            "Epoch 84/300\n",
            "2344/2344 - 5s - loss: 11.6077\n",
            "Epoch 85/300\n",
            "2344/2344 - 5s - loss: 10.9963\n",
            "Epoch 86/300\n",
            "2344/2344 - 5s - loss: 12.2614\n",
            "Epoch 87/300\n",
            "2344/2344 - 6s - loss: 10.4503\n",
            "Epoch 88/300\n",
            "2344/2344 - 5s - loss: 9.5034\n",
            "Epoch 89/300\n",
            "2344/2344 - 6s - loss: 10.8950\n",
            "Epoch 90/300\n",
            "2344/2344 - 6s - loss: 11.3659\n",
            "Epoch 91/300\n",
            "2344/2344 - 6s - loss: 9.6944\n",
            "Epoch 92/300\n",
            "2344/2344 - 5s - loss: 9.7743\n",
            "Epoch 93/300\n",
            "2344/2344 - 5s - loss: 10.5217\n",
            "Epoch 94/300\n",
            "2344/2344 - 6s - loss: 10.1381\n",
            "Epoch 95/300\n",
            "2344/2344 - 5s - loss: 9.6200\n",
            "Epoch 96/300\n",
            "2344/2344 - 5s - loss: 11.5010\n",
            "Epoch 97/300\n",
            "2344/2344 - 5s - loss: 9.8749\n",
            "Epoch 98/300\n",
            "2344/2344 - 6s - loss: 10.6982\n",
            "Epoch 99/300\n",
            "2344/2344 - 5s - loss: 8.8283\n",
            "Epoch 100/300\n",
            "2344/2344 - 5s - loss: 9.6826\n",
            "Epoch 101/300\n",
            "2344/2344 - 6s - loss: 9.4230\n",
            "Epoch 102/300\n",
            "2344/2344 - 6s - loss: 9.3489\n",
            "Epoch 103/300\n",
            "2344/2344 - 6s - loss: 9.3511\n",
            "Epoch 104/300\n",
            "2344/2344 - 5s - loss: 10.1691\n",
            "Epoch 105/300\n",
            "2344/2344 - 5s - loss: 9.2964\n",
            "Epoch 106/300\n",
            "2344/2344 - 5s - loss: 9.0785\n",
            "Epoch 107/300\n",
            "2344/2344 - 6s - loss: 12.7176\n",
            "Epoch 108/300\n",
            "2344/2344 - 5s - loss: 7.2337\n",
            "Epoch 109/300\n",
            "2344/2344 - 6s - loss: 8.7021\n",
            "Epoch 110/300\n",
            "2344/2344 - 5s - loss: 8.1936\n",
            "Epoch 111/300\n",
            "2344/2344 - 5s - loss: 9.9395\n",
            "Epoch 112/300\n",
            "2344/2344 - 5s - loss: 8.0432\n",
            "Epoch 113/300\n",
            "2344/2344 - 5s - loss: 8.1768\n",
            "Epoch 114/300\n",
            "2344/2344 - 5s - loss: 8.8889\n",
            "Epoch 115/300\n",
            "2344/2344 - 5s - loss: 9.5262\n",
            "Epoch 116/300\n",
            "2344/2344 - 5s - loss: 8.8137\n",
            "Epoch 117/300\n",
            "2344/2344 - 5s - loss: 9.2983\n",
            "Epoch 118/300\n",
            "2344/2344 - 5s - loss: 9.1071\n",
            "Epoch 119/300\n",
            "2344/2344 - 5s - loss: 7.7097\n",
            "Epoch 120/300\n",
            "2344/2344 - 5s - loss: 8.6118\n",
            "Epoch 121/300\n",
            "2344/2344 - 5s - loss: 9.3240\n",
            "Epoch 122/300\n",
            "2344/2344 - 5s - loss: 9.0999\n",
            "Epoch 123/300\n",
            "2344/2344 - 5s - loss: 8.8582\n",
            "Epoch 124/300\n",
            "2344/2344 - 5s - loss: 8.1549\n",
            "Epoch 125/300\n",
            "2344/2344 - 5s - loss: 7.6560\n",
            "Epoch 126/300\n",
            "2344/2344 - 5s - loss: 9.5389\n",
            "Epoch 127/300\n",
            "2344/2344 - 5s - loss: 7.6711\n",
            "Epoch 128/300\n",
            "2344/2344 - 5s - loss: 7.5451\n",
            "Epoch 129/300\n",
            "2344/2344 - 5s - loss: 7.2772\n",
            "Epoch 130/300\n",
            "2344/2344 - 5s - loss: 7.4979\n",
            "Epoch 131/300\n",
            "2344/2344 - 5s - loss: 9.6277\n",
            "Epoch 132/300\n",
            "2344/2344 - 5s - loss: 6.4484\n",
            "Epoch 133/300\n",
            "2344/2344 - 5s - loss: 7.8270\n",
            "Epoch 134/300\n",
            "2344/2344 - 5s - loss: 7.1517\n",
            "Epoch 135/300\n",
            "2344/2344 - 5s - loss: 7.6241\n",
            "Epoch 136/300\n",
            "2344/2344 - 5s - loss: 7.8193\n",
            "Epoch 137/300\n",
            "2344/2344 - 5s - loss: 7.1396\n",
            "Epoch 138/300\n",
            "2344/2344 - 5s - loss: 6.7130\n",
            "Epoch 139/300\n",
            "2344/2344 - 5s - loss: 7.8517\n",
            "Epoch 140/300\n",
            "2344/2344 - 5s - loss: 7.3233\n",
            "Epoch 141/300\n",
            "2344/2344 - 5s - loss: 6.4237\n",
            "Epoch 142/300\n",
            "2344/2344 - 5s - loss: 8.8294\n",
            "Epoch 143/300\n",
            "2344/2344 - 5s - loss: 6.6588\n",
            "Epoch 144/300\n",
            "2344/2344 - 5s - loss: 6.8999\n",
            "Epoch 145/300\n",
            "2344/2344 - 5s - loss: 7.2973\n",
            "Epoch 146/300\n",
            "2344/2344 - 5s - loss: 6.7906\n",
            "Epoch 147/300\n",
            "2344/2344 - 5s - loss: 6.3377\n",
            "Epoch 148/300\n",
            "2344/2344 - 5s - loss: 6.2108\n",
            "Epoch 149/300\n",
            "2344/2344 - 5s - loss: 6.4308\n",
            "Epoch 150/300\n",
            "2344/2344 - 5s - loss: 6.9071\n",
            "Epoch 151/300\n",
            "2344/2344 - 5s - loss: 7.0500\n",
            "Epoch 152/300\n",
            "2344/2344 - 5s - loss: 6.5175\n",
            "Epoch 153/300\n",
            "2344/2344 - 5s - loss: 7.5560\n",
            "Epoch 154/300\n",
            "2344/2344 - 5s - loss: 6.2139\n",
            "Epoch 155/300\n",
            "2344/2344 - 5s - loss: 6.2263\n",
            "Epoch 156/300\n",
            "2344/2344 - 5s - loss: 6.6888\n",
            "Epoch 157/300\n",
            "2344/2344 - 5s - loss: 6.4186\n",
            "Epoch 158/300\n",
            "2344/2344 - 6s - loss: 6.8550\n",
            "Epoch 159/300\n",
            "2344/2344 - 5s - loss: 6.8116\n",
            "Epoch 160/300\n",
            "2344/2344 - 5s - loss: 6.5585\n",
            "Epoch 161/300\n",
            "2344/2344 - 5s - loss: 5.8931\n",
            "Epoch 162/300\n",
            "2344/2344 - 5s - loss: 6.7514\n",
            "Epoch 163/300\n",
            "2344/2344 - 5s - loss: 6.5050\n",
            "Epoch 164/300\n",
            "2344/2344 - 5s - loss: 5.5017\n",
            "Epoch 165/300\n",
            "2344/2344 - 5s - loss: 6.5091\n",
            "Epoch 166/300\n",
            "2344/2344 - 5s - loss: 6.0439\n",
            "Epoch 167/300\n",
            "2344/2344 - 5s - loss: 6.1536\n",
            "Epoch 168/300\n",
            "2344/2344 - 5s - loss: 5.9128\n",
            "Epoch 169/300\n",
            "2344/2344 - 5s - loss: 6.6678\n",
            "Epoch 170/300\n",
            "2344/2344 - 5s - loss: 6.2163\n",
            "Epoch 171/300\n",
            "2344/2344 - 5s - loss: 6.3089\n",
            "Epoch 172/300\n",
            "2344/2344 - 5s - loss: 6.1670\n",
            "Epoch 173/300\n",
            "2344/2344 - 5s - loss: 6.1433\n",
            "Epoch 174/300\n",
            "2344/2344 - 5s - loss: 7.2862\n",
            "Epoch 175/300\n",
            "2344/2344 - 5s - loss: 6.4037\n",
            "Epoch 176/300\n",
            "2344/2344 - 5s - loss: 7.6564\n",
            "Epoch 177/300\n",
            "2344/2344 - 5s - loss: 5.3129\n",
            "Epoch 178/300\n",
            "2344/2344 - 5s - loss: 5.6099\n",
            "Epoch 179/300\n",
            "2344/2344 - 5s - loss: 7.6142\n",
            "Epoch 180/300\n",
            "2344/2344 - 5s - loss: 6.0435\n",
            "Epoch 181/300\n",
            "2344/2344 - 5s - loss: 5.7727\n",
            "Epoch 182/300\n",
            "2344/2344 - 5s - loss: 6.2269\n",
            "Epoch 183/300\n",
            "2344/2344 - 5s - loss: 5.2943\n",
            "Epoch 184/300\n",
            "2344/2344 - 5s - loss: 5.9279\n",
            "Epoch 185/300\n",
            "2344/2344 - 5s - loss: 5.8722\n",
            "Epoch 186/300\n",
            "2344/2344 - 5s - loss: 5.8377\n",
            "Epoch 187/300\n",
            "2344/2344 - 5s - loss: 6.9195\n",
            "Epoch 188/300\n",
            "2344/2344 - 5s - loss: 5.9997\n",
            "Epoch 189/300\n",
            "2344/2344 - 5s - loss: 5.1403\n",
            "Epoch 190/300\n",
            "2344/2344 - 5s - loss: 5.3425\n",
            "Epoch 191/300\n",
            "2344/2344 - 5s - loss: 5.4085\n",
            "Epoch 192/300\n",
            "2344/2344 - 5s - loss: 5.6946\n",
            "Epoch 193/300\n",
            "2344/2344 - 5s - loss: 5.4686\n",
            "Epoch 194/300\n",
            "2344/2344 - 5s - loss: 5.2995\n",
            "Epoch 195/300\n",
            "2344/2344 - 5s - loss: 5.2540\n",
            "Epoch 196/300\n",
            "2344/2344 - 5s - loss: 6.1260\n",
            "Epoch 197/300\n",
            "2344/2344 - 5s - loss: 5.5552\n",
            "Epoch 198/300\n",
            "2344/2344 - 5s - loss: 6.4003\n",
            "Epoch 199/300\n",
            "2344/2344 - 5s - loss: 5.1542\n",
            "Epoch 200/300\n",
            "2344/2344 - 5s - loss: 5.3894\n",
            "Epoch 201/300\n",
            "2344/2344 - 5s - loss: 5.7453\n",
            "Epoch 202/300\n",
            "2344/2344 - 5s - loss: 4.8851\n",
            "Epoch 203/300\n",
            "2344/2344 - 5s - loss: 6.1710\n",
            "Epoch 204/300\n",
            "2344/2344 - 5s - loss: 10.4724\n",
            "Epoch 205/300\n",
            "2344/2344 - 5s - loss: 5.5557\n",
            "Epoch 206/300\n",
            "2344/2344 - 5s - loss: 4.3105\n",
            "Epoch 207/300\n",
            "2344/2344 - 5s - loss: 4.8172\n",
            "Epoch 208/300\n",
            "2344/2344 - 5s - loss: 6.0428\n",
            "Epoch 209/300\n",
            "2344/2344 - 5s - loss: 5.2613\n",
            "Epoch 210/300\n",
            "2344/2344 - 5s - loss: 5.1709\n",
            "Epoch 211/300\n",
            "2344/2344 - 5s - loss: 5.4238\n",
            "Epoch 212/300\n",
            "2344/2344 - 5s - loss: 5.9938\n",
            "Epoch 213/300\n",
            "2344/2344 - 5s - loss: 5.7854\n",
            "Epoch 214/300\n",
            "2344/2344 - 5s - loss: 4.9577\n",
            "Epoch 215/300\n",
            "2344/2344 - 5s - loss: 6.5752\n",
            "Epoch 216/300\n",
            "2344/2344 - 5s - loss: 4.7670\n",
            "Epoch 217/300\n",
            "2344/2344 - 5s - loss: 5.1144\n",
            "Epoch 218/300\n",
            "2344/2344 - 5s - loss: 5.0939\n",
            "Epoch 219/300\n",
            "2344/2344 - 5s - loss: 4.9231\n",
            "Epoch 220/300\n",
            "2344/2344 - 5s - loss: 5.3093\n",
            "Epoch 221/300\n",
            "2344/2344 - 5s - loss: 4.8152\n",
            "Epoch 222/300\n",
            "2344/2344 - 5s - loss: 4.9660\n",
            "Epoch 223/300\n",
            "2344/2344 - 5s - loss: 5.7322\n",
            "Epoch 224/300\n",
            "2344/2344 - 5s - loss: 4.6574\n",
            "Epoch 225/300\n",
            "2344/2344 - 5s - loss: 4.9952\n",
            "Epoch 226/300\n",
            "2344/2344 - 5s - loss: 12.8083\n",
            "Epoch 227/300\n",
            "2344/2344 - 5s - loss: 4.5808\n",
            "Epoch 228/300\n",
            "2344/2344 - 5s - loss: 9.3996\n",
            "Epoch 229/300\n",
            "2344/2344 - 5s - loss: 3.9222\n",
            "Epoch 230/300\n",
            "2344/2344 - 5s - loss: 4.4177\n",
            "Epoch 231/300\n",
            "2344/2344 - 5s - loss: 4.8670\n",
            "Epoch 232/300\n",
            "2344/2344 - 5s - loss: 5.0515\n",
            "Epoch 233/300\n",
            "2344/2344 - 5s - loss: 6.9023\n",
            "Epoch 234/300\n",
            "2344/2344 - 5s - loss: 4.5402\n",
            "Epoch 235/300\n",
            "2344/2344 - 5s - loss: 4.4707\n",
            "Epoch 236/300\n",
            "2344/2344 - 5s - loss: 4.8844\n",
            "Epoch 237/300\n",
            "2344/2344 - 5s - loss: 5.5157\n",
            "Epoch 238/300\n",
            "2344/2344 - 5s - loss: 5.0730\n",
            "Epoch 239/300\n",
            "2344/2344 - 5s - loss: 4.8376\n",
            "Epoch 240/300\n",
            "2344/2344 - 5s - loss: 4.6089\n",
            "Epoch 241/300\n",
            "2344/2344 - 5s - loss: 5.3891\n",
            "Epoch 242/300\n",
            "2344/2344 - 5s - loss: 6.0242\n",
            "Epoch 243/300\n",
            "2344/2344 - 5s - loss: 5.8980\n",
            "Epoch 244/300\n",
            "2344/2344 - 5s - loss: 4.4369\n",
            "Epoch 245/300\n",
            "2344/2344 - 5s - loss: 3.8467\n",
            "Epoch 246/300\n",
            "2344/2344 - 5s - loss: 4.5900\n",
            "Epoch 247/300\n",
            "2344/2344 - 5s - loss: 4.9610\n",
            "Epoch 248/300\n",
            "2344/2344 - 5s - loss: 4.8359\n",
            "Epoch 249/300\n",
            "2344/2344 - 5s - loss: 5.1255\n",
            "Epoch 250/300\n",
            "2344/2344 - 5s - loss: 4.8767\n",
            "Epoch 251/300\n",
            "2344/2344 - 5s - loss: 4.7490\n",
            "Epoch 252/300\n",
            "2344/2344 - 5s - loss: 4.1735\n",
            "Epoch 253/300\n",
            "2344/2344 - 5s - loss: 4.7861\n",
            "Epoch 254/300\n",
            "2344/2344 - 5s - loss: 4.4810\n",
            "Epoch 255/300\n",
            "2344/2344 - 5s - loss: 5.3176\n",
            "Epoch 256/300\n",
            "2344/2344 - 5s - loss: 3.8636\n",
            "Epoch 257/300\n",
            "2344/2344 - 5s - loss: 4.6244\n",
            "Epoch 258/300\n",
            "2344/2344 - 5s - loss: 4.4662\n",
            "Epoch 259/300\n",
            "2344/2344 - 5s - loss: 4.6265\n",
            "Epoch 260/300\n",
            "2344/2344 - 5s - loss: 5.2285\n",
            "Epoch 261/300\n",
            "2344/2344 - 6s - loss: 4.7516\n",
            "Epoch 262/300\n",
            "2344/2344 - 6s - loss: 5.4541\n",
            "Epoch 263/300\n",
            "2344/2344 - 6s - loss: 4.1316\n",
            "Epoch 264/300\n",
            "2344/2344 - 6s - loss: 4.2427\n",
            "Epoch 265/300\n",
            "2344/2344 - 5s - loss: 4.6364\n",
            "Epoch 266/300\n",
            "2344/2344 - 5s - loss: 4.6031\n",
            "Epoch 267/300\n",
            "2344/2344 - 5s - loss: 4.9516\n",
            "Epoch 268/300\n",
            "2344/2344 - 5s - loss: 4.4412\n",
            "Epoch 269/300\n",
            "2344/2344 - 5s - loss: 6.6300\n",
            "Epoch 270/300\n",
            "2344/2344 - 5s - loss: 4.5573\n",
            "Epoch 271/300\n",
            "2344/2344 - 5s - loss: 4.1852\n",
            "Epoch 272/300\n",
            "2344/2344 - 5s - loss: 4.5474\n",
            "Epoch 273/300\n",
            "2344/2344 - 6s - loss: 4.5632\n",
            "Epoch 274/300\n",
            "2344/2344 - 5s - loss: 4.7502\n",
            "Epoch 275/300\n",
            "2344/2344 - 5s - loss: 6.1160\n",
            "Epoch 276/300\n",
            "2344/2344 - 5s - loss: 3.9650\n",
            "Epoch 277/300\n",
            "2344/2344 - 5s - loss: 4.2535\n",
            "Epoch 278/300\n",
            "2344/2344 - 5s - loss: 5.1241\n",
            "Epoch 279/300\n",
            "2344/2344 - 5s - loss: 6.0421\n",
            "Epoch 280/300\n",
            "2344/2344 - 5s - loss: 4.0104\n",
            "Epoch 281/300\n",
            "2344/2344 - 5s - loss: 4.1408\n",
            "Epoch 282/300\n",
            "2344/2344 - 5s - loss: 4.8930\n",
            "Epoch 283/300\n",
            "2344/2344 - 5s - loss: 4.8811\n",
            "Epoch 284/300\n",
            "2344/2344 - 5s - loss: 5.2748\n",
            "Epoch 285/300\n",
            "2344/2344 - 5s - loss: 4.0549\n",
            "Epoch 286/300\n",
            "2344/2344 - 5s - loss: 4.8379\n",
            "Epoch 287/300\n",
            "2344/2344 - 5s - loss: 4.3719\n",
            "Epoch 288/300\n",
            "2344/2344 - 5s - loss: 4.9014\n",
            "Epoch 289/300\n",
            "2344/2344 - 5s - loss: 4.7825\n",
            "Epoch 290/300\n",
            "2344/2344 - 5s - loss: 3.8786\n",
            "Epoch 291/300\n",
            "2344/2344 - 5s - loss: 4.2020\n",
            "Epoch 292/300\n",
            "2344/2344 - 5s - loss: 7.3277\n",
            "Epoch 293/300\n",
            "2344/2344 - 5s - loss: 3.7588\n",
            "Epoch 294/300\n",
            "2344/2344 - 5s - loss: 4.0261\n",
            "Epoch 295/300\n",
            "2344/2344 - 5s - loss: 4.1808\n",
            "Epoch 296/300\n",
            "2344/2344 - 5s - loss: 3.9205\n",
            "Epoch 297/300\n",
            "2344/2344 - 5s - loss: 4.0679\n",
            "Epoch 298/300\n",
            "2344/2344 - 5s - loss: 4.2286\n",
            "Epoch 299/300\n",
            "2344/2344 - 5s - loss: 4.1623\n",
            "Epoch 300/300\n",
            "2344/2344 - 5s - loss: 4.1070\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f63380e69d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58PIikBcKam2"
      },
      "source": [
        "scaled_input = np.reshape(input_tensor,(n_samples,int(n_bs*n_paths*n_features)))\n",
        "scaled_input = scaler.transform(scaled_input)\n",
        "scaled_input = np.reshape(scaled_input,(scaled_input.shape[0],n_bs,n_paths,n_features))\n",
        "pred_vals = model.predict(scaled_input) +  np.min(true_cord_tensor)*0\n",
        "\n",
        "\n",
        "# pred_train = model.predict(scaled_df)\n",
        "# scaled_test =scaler.transform(X_test)\n",
        "# scaled_test = np.reshape(scaled_test,(scaled_test.shape[0],n_bs,n_paths,n_features))\n",
        "# pred_test = model.predict(scaled_test)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_t2mWBSvKam3",
        "outputId": "597084a9-e502-4f58-fe12-c585650fde83"
      },
      "source": [
        "print(X_test.shape)\n",
        "#pred_vals = scaler.inverse_transform(pred_vals)\n",
        "#pred_train = scaler.inverse_transform(pred_train)\n",
        "#pred_test = scaler.inverse_transform(pred_test)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(75000, 600)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pS1C-nsIKam3"
      },
      "source": [
        "#pred_vals = outScaler.inverse_transform(pred_vals)\n",
        "norm_error = np.linalg.norm(pred_vals - true_cord_tensor,axis = 1)\n",
        "# norm_error_train = np.linalg.norm(pred_train - y_train,axis = 1)\n",
        "# norm_error_test = np.linalg.norm(pred_test - y_test,axis = 1)\n",
        "# print(norm_error.shape)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "xqaxOOBFKam4",
        "outputId": "63c56249-8052-4131-b0d3-e092a29477fa"
      },
      "source": [
        "from statsmodels.distributions.empirical_distribution import ECDF\n",
        "\n",
        "cdf = ECDF(norm_error/1)\n",
        "plt.plot(cdf.x,cdf.y)\n",
        "plt.grid()\n",
        "plt.xlim([0,20])\n",
        "\n",
        "# cdf = ECDF(norm_error_train/1)\n",
        "# plt.plot(cdf.x,cdf.y)\n",
        "# plt.grid()\n",
        "# plt.xlim([0,20])\n",
        "\n",
        "\n",
        "# cdf = ECDF(norm_error_test/1)\n",
        "# plt.plot(cdf.x,cdf.y)\n",
        "# plt.grid()\n",
        "# plt.xlim([0,6])\n",
        "\n",
        "\n",
        "# mat = scipy.io.loadmat('overall_DNN.mat')\n",
        "# err_data = mat['norm_error']\n",
        "# err_data = (err_data).flatten()\n",
        "# cdf = ECDF(err_data*1)\n",
        "# plt.plot(cdf.x,cdf.y)\n",
        "\n",
        "\n",
        "# mat = scipy.io.loadmat('baselineerror.mat')\n",
        "# err_data = mat['err_data']\n",
        "# err_data = (err_data).flatten()\n",
        "# cdf = ECDF(err_data*1)\n",
        "# plt.plot(cdf.x,cdf.y)\n",
        "\n",
        "\n",
        "\n",
        "plt.xlabel('Error in meters')\n",
        "plt.ylabel('CDF')\n",
        "plt.legend(['CNN','NN','baseline'])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f62c8648bd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEGCAYAAACZ0MnKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZRcdZ338fe39yTd6WydtbNCSEiAkAQSBDQJIgZQMo7ogWEQFcjgyPPIER1QeNBhZs6IPi6PIw5kRgZkUAyoGBVB0TQgI0sSICtZCFk6IVsn6SWd3qq+zx91Oyk6vVTSdbu2z+ucOnWXX9369KW439ztd83dERERSUReqgOIiEjmUNEQEZGEqWiIiEjCVDRERCRhKhoiIpKwglQHOFmDBg3y008/PdUxenTkyBEGDBiQ6hg9Us7kyYSMoJzJlik5V65cecDdK3q7nIwrGiNGjGDFihWpjtGjqqoq5s+fn+oYPVLO5MmEjKCcyZYpOc1sezKWo8NTIiKSMBUNERFJmIqGiIgkLOPOaXSmtbWV6upqmpqaUh3lmPLycjZs2JC05ZWUlFBZWUlhYWHSlikicrKyomhUV1dTVlbGhAkTMLNUxwGgvr6esrKypCzL3ampqaG6upqJEycmZZkiIqcitMNTZvaQme0zs7VdzDcz+76ZbTGz1WY261S/q6mpiaFDh6ZNwUg2M2Po0KFptSclIrkpzHMaDwMLu5l/OTA5eC0G/r03X5atBaNdtv99IpIZQisa7v4CcLCbJouAH3vMy8AgMxsVVh4REem9VJ7TGAPsjBuvDqa927GhmS0mtjdCRUUFVVVV75lfXl5OfX19aEETtXfvXu644w5WrVpFeXk5w4cP5xvf+AazZ8/mm9/8JrfccgsAt99+O7NmzeK6667jlltuYfny5axevZri4mJqamqYN28ea9eeeFSvqanphL+9txoaGpK+zDBkQs5MyAjKmWyZkjNZMuJEuLsvAZYATJkyxTvefblhw4aknXQ+Ve7OZZddxg033MDPf/5z6uvr2bp1K3V1dQwfPpwHH3yQL3zhCxQVFVFUVERJSQllZWUUFhZSUFDAE088wec+9zmam5sxs07/npKSEmbOnJnU3JlyN2sm5MyEjJAZOd2d5VVVXHDR+4lEnbaI0xaNEok6rVEnEjfeFnUiUSfqx9/bIk7EnWiU4P3Edsdewfxj71En4hBtb3tsOu9tGwxv39nMqNHDgmlxn4vG/o6O07ubF4k67hz/3k7mOcF0P9624/vx4dj3RJP4sL1UFo1dwNi48cpgWkZavnw5hYWFx/YmAGbMmMG2bduoqKjgoosu4pFHHuHmm28+4bO33XYb3/3udzudJ5Is7k5rxDna5tQ0NNMSidLSFqU1EqW5LUprxI+Nt7S1Tzvepr19SyRKa5vTEokE8/yEtq0RpzUSpS0aa9sajU1vizgtkeiJhSDiQQGIBtODjdyzz6R2pXUizyA/z8iz2MujEYr37aYgzzCz987Pgzwz8s2wuOlmRn4wLy/uMxZ8pjAvr/N5dvwzFgx3fM8zMGLLsbjxN5L096eyaCwDbjWzx4G5QK27n3Bo6mT946/XsX53Xa/DxZs2eiBf++j0btusXbuW2bNndzn/jjvu4PLLL+ezn/3sCfPGjRvHxRdfzKOPPspHP/rRXueV9Oce29AebYnQ1BahuTVKU1uEptYoTa2R4BWl+T3zYhvplrYozZEoTS0RjrZGONoaW87R1rbgPbaM5tYILUFBaA4+d8xzz/X6b8jPM4ry8yjMN4oK8inKN4oK8ijMb38Zhfl5FOQbpYUFseE8OzYvPy82XpBvFOTFxmPTjYL8PHZu38bpp03qsm3sPTaelxfbMOfnHX/lHRuH/Lw88oMNcn6HtnmdDce1bd9Ix4ZPvCglE/bcAP45ScsJrWiY2U+B+cAwM6sGvgYUArj7A8DTwBXAFqAR+ExYWdLBpEmTmDt3Lj/5yU86nf+Vr3yFRYsWceWVV/ZxMulJWyTKkZYIR5rbONLcRkNzG0eaI8F7G0daYtM2bGqhqm4dR5rbaGyNxG3UIxxtidAYvI62xOaf6hEDMyjKz6NfUT79C/MpKcqnX2HsNaC4gKGl+ZQU5lOUn0dxYV7svSD2KiqIbYynT51MUX7esY18UUHeCeOF+Ubxe8aPtyvMzyM/L9wr+qqqdjN/fvr3aJ1rQisa7n5tD/Md+Hyyv7enPYKwTJ8+nSeffLLbNl/96le5+uqrmTdv3gnzJk+ezLnnnsvSpUvDipizmlojHDzSwqHGFg43tnKosYW6o23UHm2lrqmV2qOtNDTFNvwNzW3HhhuDYtDUGu35SwADSndXU1pcENuIBxvzAUUFDCstpn9RfvAqoH/R8fklhfmUFOZRXBB7LynIpziYVlKYT3HB8ffignwK861Xl2BXVe1i/vsmnPLnJbdlxInwTHDJJZfw1a9+lSVLlrB48WIAVq9eTW1t7bE2U6dOZdq0afz617/m/PPPP2EZd911l/Y0EtTUGmFfXTP76pvYV9/Mvrrgvf1V18ShxhZqj7Z2u9EvzDcGlhRSVlJAaUkBA4oKGFVeEhsuLmBAUexf76XFwXhxAaXFsUIQP720uICXX3qBBQsW9OFaEOl7KhpJYmb88pe/5LbbbuO+++6jqKiISZMm8b3vfe897e66664ur4CaPn06s2bNYtWqVX0ROe24O/XNbceKwf765mPDa7Y08eCml48VifqmthM+n59nVJQWM3xgMZWD+3FOZTnl/QoZ1L+IIQOKGNw/NjyofyHl/WKvfoX5SbtxUjdgSi5Q0Uii0aNHHzu8FN/3VPw9FzNmzCAaPf4v34cffvg9y/jFL34RftAUOtzYwtYDR9hec4RtBxrZeaiR3YeP8m5tE3vrmjrdKyguyGNgoTOuKMoZI8q4+PRhDB9YQkVZMcPLihleVsLwgcUM6V9EXsjH2UVynYqGJF006uw42Mhbe+rYuKeBbTVHeOfAEXYcbOTgkZZj7cxg5MASRg/qxzmVgxhRFttLGF5WEisGA4upKCthYEkBzz//PPPnX5jCv0pEQEVDeqmhuY0N79axfncdb+2pY8O79WzcU8/R1sixNqPLS5gwbAAfnj6CicMGMGlYKROGDaBycD9KCvNTmF5ETlbWFA13z+pjyp7EOzp7k2FbTSMrtx9i1Y5DrNp+iI17649dOjqofyFnjhzINXPGcubIgUwdVcbk4WX0K1JhEMkWWVE0SkpKqKmpydru0dufp1FSUtLn3119qJHlG/fzP1sO8Oo7B6kJDi+VFRdw7rhBLDxrJOdUljNtVDkjBhZn5foXkeOyomhUVlZSXV3N/v37Ux3lmKampqRu5Nuf3NcXNu6p5xevV7P8rX1s2tsAwJhB/Zh3RgXnTxzCrHGDmTy8VCedRXJQVhSNwsLCtHuiXVVVVdI7FwxTWyTKb9e8y3+9tI03dh6mMN+YM3EInzxvLAumDmfSsAHaixCR7CgacurcnT+s38t9z7zFln0NnFYxgLuvPJO/nlXJkAFFqY4nImlGRSOH7TzYyHdXNrP6wAomVQzggb+dxWXTRuqwk4h0SUUjB7k7P/7Ldu575i2ikQh3X3kmN1w4gcL8MJ/+KyLZQEUjxxxoaObLT7zJ8o37+cAZFSwa1cDH3z8p1bFEJEPon5Y55MXN+1n4vRd56e0a/mnRdB75zPkM7aefgIgkTnsaOaC5LcJ3fr+JB1/YyuThpfz3TXOYOnJgqmOJSAZS0chya3fV8g9Prmb9u3X8zdxx/J8rp+kObRE5ZSoaWaqpNcL3/7iZB55/myEDivmPT53Hh6aNSHUsEclwKhpZxt15Zu0e/uXpDVQfOsonZldy95XTKO9fmOpoIpIFVDSyyDsHjnDPr9by4uYDTB1Zxk9vvoD3nTY01bFEJIuoaGSBptYIP6x6mweq3qa4II+vfXQa118wngLddyEiSaaikeFe3Lyfu59ay/aaRhadO5q7rjiT4QP7vjdcEckNKhoZ6tCRFv7Pr9bym9XvMmFofx67aS4XnT4s1bFEJMupaGSgFzbt5/Yn3uRwYwu3f+gMbv7AJD0BT0T6hIpGBmlpi/KtZ9/iP158h8nDS3n4M+czfXR5qmOJSA5R0cgQuw4f5e8fW8WbOw/ztxeM4+4rp2nvQkT6nIpGBnh9xyFufGQFLW1R/v26WVx+9qhURxKRHKWikeZe3Lyfv3t0JRVlxfzXp89nUkVpqiOJSA5T0UhjL205wE2PrGDisAH8+LNzdCmtiKScikaaen7Tfm7+8QomDh3AT26+QI9eFZG0oKKRhl595yB/9+gKTq8o5bGb5jJYBUNE0oT6mUgza3fVcuPDrzG6vB8/vnGOCoaIpJVQi4aZLTSzjWa2xczu7GT+ODNbbmavm9lqM7sizDzpbuv+Bm546FUG9ivkv2+ay7DS4lRHEhF5j9CKhpnlA/cDlwPTgGvNbFqHZncDS919JnAN8MOw8qS7ffVNXP+jVwF49MY5jB7UL8WJREROFOaexhxgi7tvdfcW4HFgUYc2DrQ/d7Qc2B1inrTV1Brh5h+v5OCRFh7+zBxdVisiacvcPZwFm10NLHT3m4Lx64G57n5rXJtRwO+BwcAA4FJ3X9nJshYDiwEqKipmL126NJTMydTQ0EBpac8bf3fngTebeWVPhP81s5jZI/r22oREc6ZaJuTMhIygnMmWKTkXLFiw0t3P6/WC3D2UF3A18J9x49cDP+jQ5ovA7cHw+4D1QF53yz3jjDM8Eyxfvjyhdt9/bpOPv+M3fv/yzeEG6kKiOVMtE3JmQkZ35Uy2TMkJrPAkbNvDPDy1CxgbN14ZTIt3I7AUwN3/ApQAOdO/91/eruE7z23iYzPH8Ll5p6U6johIj8IsGq8Bk81sopkVETvRvaxDmx3ABwHM7ExiRWN/iJnSRn1TK1964k3GD+nPv3zsLMws1ZFERHoUWtFw9zbgVuBZYAOxq6TWmdm9ZnZV0Ox24GYzexP4KfDpYDcq6/3r795id+1Rvv3Jc+lfpHssRSQzhLq1cvengac7TLsnbng9cFGYGdLRnzcf4Cev7ODm909k9vjBqY4jIpIw3RHex5paI9z11BomDhvA7ZdNSXUcEZGTouMifezB57eyvaaRx26aq4coiUjG0Z5GH9pb18SDL7zN5WeN5KLTc+YiMRHJIioafej/PruRtohz5+VTUx1FROSUqGj0kbf21PHkqmpuuHA844cOSHUcEZFToqLRR771zEZKiwv4/ILTUx1FROSUqWj0gde2HeSPb+3jlnmnMai/no8hIplLRaMPfPv3G6koK+azF01MdRQRkV5R0QjZK1treHnrQW6Zdxr9inSJrYhkNhWNkP1g+RaGlRbxN3PGpTqKiEivqWiEaP3uOl7cfIAbL56kvQwRyQoqGiF65H+2UVKYx7VzxvbcWEQkA6hohKS+xXnqjV18bGalrpgSkayhohGSP+9qo7ktymcumpDqKCIiSaOiEQJ35/nqVmaNG8QZI8pSHUdEJGlUNEKwasch9hxxrjlfV0yJSHZR0QjBT17ZSUk+XHHOqFRHERFJKhWNJKttbOW3a3ZzwagCSov1uBIRyS4qGkn2i9eraWqNsmCcCoaIZB8VjSRyd3766g5mVJYzfqBu5hOR7KOikUSv7zzMpr0NXKsuQ0QkS6loJNETK6rpV5jPR2aMTnUUEZFQqGgkSVNrhN+u3s3Cs0bqBLiIZC0VjST501v7qGtq469njUl1FBGR0KhoJMmyN3YzrLSYC08bluooIiKhUdFIgvqmVv60cR8fOWcU+XmW6jgiIqFR0UiCZ9buoaUtykd1AlxEspyKRhL8ZvW7VA7ux6xxg1IdRUQkVCoavXS4sYWXthzgynNGYaZDUyKS3VQ0eun36/bSFnU+crYOTYlI9gu1aJjZQjPbaGZbzOzOLtp80szWm9k6M/tJmHnC8Pv1e6gc3I+zxgxMdRQRkdCFdheameUD9wMfAqqB18xsmbuvj2szGfgKcJG7HzKz4WHlCUNjSxsvbD7A38wZp0NTIpITwtzTmANscfet7t4CPA4s6tDmZuB+dz8E4O77QsyTdC9uPkBLW5QPTRuR6igiIn3C3D2cBZtdDSx095uC8euBue5+a1ybp4BNwEVAPvB1d3+mk2UtBhYDVFRUzF66dGkomU/Wj9Y0s2JvG/92SX8KOtyf0dDQQGlpaYqSJU45kycTMoJyJlum5FywYMFKdz+vt8tJdSdJBcBkYD5QCbxgZme7++H4Ru6+BFgCMGXKFJ8/f34fxzxRNOp86c/P8cFpo7j0klknzK+qqiIdcvZEOZMnEzKCciZbpuRMljAPT+0CxsaNVwbT4lUDy9y91d3fIbbXMTnETEnzRvVhDjS06NCUiOSUMIvGa8BkM5toZkXANcCyDm2eIraXgZkNA84AtoaYKWmeW7+X/Dxj/hkZde5eRKRXQisa7t4G3Ao8C2wAlrr7OjO718yuCpo9C9SY2XpgOfBld68JK1My/XHDPs6fMJjy/oWpjiIi0mdCPafh7k8DT3eYdk/csANfDF4ZY0dNIxv31nP3lWemOoqISJ/SHeGn4LkNewG49EydzxCR3KKicQr++NZeTh9eyoRhA1IdRUSkT6lonKS6plZe2XpQexkikpNUNE7S8xv30xZ1Lj1TV02JSO5R0ThJyzfuY3D/QmaOG5zqKCIifU5F4yS4Oy+/XcMFk4bqsa4ikpNUNE7COweOsLu2iYtOH5bqKCIiKdFt0TCzh+OGbwg9TZp76e3YfYcXq2iISI7qaU9jRtzwF8IMkgn+8vYBRpeXMH5o/1RHERFJiZ6KRjj9pmcgd+fVdw4xZ+IQPXBJRHJWT92IVJrZ9wGLGz7G3f93aMnSzPaaRg40NHPehCGpjiIikjI9FY0vxw2vCDNIunv1nYMAzJ2ooiEiuavbouHuj/RVkHT36raDDO5fyOnD0/8JXSIiYenxklszu8HMVpnZkeC1wsw+1Rfh0smKbQeZPV7nM0Qkt/V0ye0NwG3A7cBoYAzwD8AXgmd+54T99c1sq2nUoSkRyXk97Wl8DviYuy9391p3P+zufwI+Dnw+/HjpYeX22PmM2RPUdYiI5LaeisZAd9/WcWIwbWAYgdLRa9sOUVyQx1mjy1MdRUQkpXoqGkdPcV5WWbHtIDPGDqKoQL2uiEhu6+mS2zPNbHUn0w2YFEKetNPUGmHd7joWfyAn/lwRkW71VDRmACOAnR2mjwX2hJIozayurqUt6sxSV+giIj0envouUOvu2+NfQG0wL+ut2nEIgJnjBqU4iYhI6vVUNEa4+5qOE4NpE0JJlGZWbT/EhKH9GVpanOooIiIp11PR6O6f1/2SGSQduTuv7zysp/SJiAR6KhorzOzmjhPN7CZgZTiR0kf1oaPsr2/WoSkRkUBPJ8JvA35pZtdxvEicBxQBHwszWDp4Y+dhAGaO1Z6GiAj03GHhXuBCM1sAnBVM/m1wV3jWe2PnYYoL8pg6qizVUURE0kJPexoAuPtyYHnIWdLOGzsPc/aYcgrzdVOfiAgk0MttrmqNRFm7q5YZY3U+Q0SknYpGFzbuqae5LaqiISISR0WjC68fOwmuoiEi0k5Fowsrtx2koqyYysFZfzuKiEjCQi0aZrbQzDaa2RYzu7Obdh83Mzez88LMczJW7TjM7HGD9aQ+EZE4oRUNM8sH7gcuB6YB15rZtE7alQFfAF4JK8vJOnikhR0HGzlXN/WJiLxHmHsac4At7r7V3VuAx4FFnbT7J+A+oCnELCfljZ2xTgrP1fkMEZH3SOg+jVM0hvd2qV4NzI1vYGazgLHu/lsz+3JXCzKzxcBigIqKCqqqqpKfNs5Tm1sw4PDW1VTtOLXDUw0NDaHnTAblTJ5MyAjKmWyZkjNZwiwa3TKzPOA7wKd7auvuS4AlAFOmTPH58+eHmu2hra8yZWQTCy/9wCkvo6qqirBzJoNyJk8mZATlTLZMyZksYR6e2kXsYU3tKoNp7cqIdU1SZWbbgAuAZak+Ge7urK4+rENTIiKdCLNovAZMNrOJZlYEXAMsa5/p7rXuPszdJ7j7BOBl4Cp3XxFiph7tPHiUw42tnF1ZnsoYIiJpKbSi4e5twK3As8AGYKm7rzOze83sqrC+t7dW74rd1DejUnsaIiIdhXpOw92fBp7uMO2eLtrODzNLotbsqqUoP48zRqhnWxGRjnRHeAdrqmuZMrKMogKtGhGRjrRljBONOmuqa5kxVuczREQ6o6IRZ+uBI9Q3t3HOGJ3PEBHpjIpGnNXVwUlwXW4rItIpFY04a3fVUVKYx2kVA1IdRUQkLaloxFm7u5YzRw2kQI93FRHplLaOgWjU2bC7jumjB6Y6iohI2lLRCOw81Eh9cxvTR+vKKRGRrqhoBNbuqgPQnoaISDdUNAJrdtVSmG9MGak7wUVEuqKiEVi3u5YzRpRRXJCf6igiImlLRYP27tBrOUc924qIdEtFg1h36LVHWzlbd4KLiHRLRYPj3aFrT0NEpHsqGqg7dBGRRKloEOsOfeoodYcuItKTnN9KujtrdtVy9hgdmhIR6UnOF43tNY3UN7WpaIiIJCDni8ab1e0nwXXllIhIT3K+aKyurqWkMI8zRpSmOoqISNpT0ag+rO7QRUQSlNNbytZIlNXVtcwcOzjVUUREMkJOF41Ne+tpbosyY6xOgouIJCKni8bq6loAZugkuIhIQnK6aKzZVcvAkgLGD+2f6igiIhkht4tGdS1njSnHzFIdRUQkI+Rs0WhqjfDWnjrdnyEichJytmhseLeO1ohzrk6Ci4gkLGeLRvtJcO1piIgkLmeLxrrdtQwdUMSo8pJURxERyRihFg0zW2hmG81si5nd2cn8L5rZejNbbWZ/NLPxYeaJt3ZXHdN1ElxE5KSEVjTMLB+4H7gcmAZca2bTOjR7HTjP3c8BngS+GVaeeE2tETbtrWf66IF98XUiIlkjzD2NOcAWd9/q7i3A48Ci+AbuvtzdG4PRl4HKEPMcs253LW1RZ+ZYnc8QETkZ5u7hLNjsamChu98UjF8PzHX3W7to/wNgj7v/cyfzFgOLASoqKmYvXbq0V9l+904rP9vYwv9b0J/y4nAOTzU0NFBamv495ypn8mRCRlDOZMuUnAsWLFjp7uf1djkFyQjTW2b2t8B5wLzO5rv7EmAJwJQpU3z+/Pm9+r6lu1Yydkgtiz68oFfL6U5VVRW9zdkXlDN5MiEjKGeyZUrOZAmzaOwCxsaNVwbT3sPMLgXuAua5e3OIeY5Ztf0wcyYO6YuvEhHJKmGe03gNmGxmE82sCLgGWBbfwMxmAg8CV7n7vhCzHPNu7VH21DUxa5zOZ4iInKzQioa7twG3As8CG4Cl7r7OzO41s6uCZt8CSoEnzOwNM1vWxeKSZtX22ONdZ47TMzRERE5WqOc03P1p4OkO0+6JG740zO/vzOs7DlFckMeZo3S5rYjIycq5O8JX7TjE2WPKKSrIuT9dRKTXcmrL2dwWYe3uOmaN16EpEZFTkVNFY/3uOlraorqpT0TkFOVU0Vi1I3YSXHsaIiKnJqeKxus7DjG6vIQRA9WzrYjIqcixonFYl9qKiPRCzhSNvXVN7Dp8lJm6qU9E5JTlTNF4fcchQDf1iYj0Rg4VjcMU5edx1hjd1Ccicqpypmis2H6IaaMHUlyQn+ooIiIZKyeKRl1TK2/sPMxFpw9NdRQRkYyWE0Xjf7bUEIk6759ckeooIiIZLSeKxvOb9jOgKJ9ZOgkuItIrWV803J3nN+7jotOHqZNCEZFeyvqt6OZ9DeyubWL+lOGpjiIikvGyvmhUbYw9EHD+FJ3PEBHprawvGs+t38eZowYyelC/VEcREcl4WV009tc389r2g3xo2ohURxERyQpZXTSe27AXd7j8rJGpjiIikhWyumg8u24P44f2Z+rIslRHERHJCllbNA4daeGlLQf48PSRmFmq44iIZIWsLRq/fH0XrRHnYzPHpDqKiEjWyMqiEY06//3ydmaMHcSZo9SrrYhIsmRl0Xh+8362HjjCZy6ckOooIiJZJSuLxkN/foeKsmKuOHtUqqOIiGSVrCsaa3fV8uLmA3z6wgnqa0pEJMmybqv6wPNvU1pcwPXvG5/qKCIiWSerisamvfU8veZdrps7joElhamOIyKSdbKmaLg7X/vVOkqLC/i7eaelOo6ISFbKmqLxxMpq/rK1hi8vnMqQAUWpjiMikpWyomis313H15etY87EIVw3Z1yq44iIZK1Qi4aZLTSzjWa2xczu7GR+sZn9LJj/iplNONnv2LKvnk899AoDSwr5/jUzyctTlyEiImEJrWiYWT5wP3A5MA241symdWh2I3DI3U8Hvgvcl+jy99Y18cOqLSz6wUsAPHbzXEaWlyQlu4iIdK4gxGXPAba4+1YAM3scWASsj2uzCPh6MPwk8AMzM3f3rha6qyHK+f/yHPvrmwG4ZOpw7l00ncrB/UP4E0REJJ51s33u3YLNrgYWuvtNwfj1wFx3vzWuzdqgTXUw/nbQ5kCHZS0GFgMMGDF+9sfvfpCKfsbM4QWMKUvP0zINDQ2UlpamOkaPlDN5MiEjKGeyZUrOBQsWrHT383q7nDD3NJLG3ZcASwCmTJnij9z64RQn6llVVRXz589PdYweKWfyZEJGUM5ky5ScyRLmP9N3AWPjxiuDaZ22MbMCoByoCTGTiIj0QphF4zVgsplNNLMi4BpgWYc2y4AbguGrgT91dz5DRERSK7TDU+7eZma3As8C+cBD7r7OzO4FVrj7MuBHwKNmtgU4SKywiIhImgr1nIa7Pw083WHaPXHDTcAnwswgIiLJk56XHomISFpS0RARkYSpaIiISMJUNEREJGGh3REeFjOrBzamOkcChgEHemyVesqZPJmQEZQz2TIl5xR3L+vtQjLijvAONibjVviwmdkK5UyeTMiZCRlBOZMtk3ImYzk6PCUiIglT0RARkYRlYtFYkuoACVLO5MqEnJmQEZQz2XIqZ8adCBcRkdTJxD0NERFJERUNERFJWNoWDTNbaGYbzWyLmd3ZyfxiM/tZMP8VM5uQgoxjzWy5ma03s3Vm9oVO2sw3s5tgOcUAAAc5SURBVFozeyN43dPZsvog6zYzWxNkOOHSO4v5frA+V5vZrD7ONyVuHb1hZnVmdluHNilbl2b2kJntC5422T5tiJn9wcw2B++Du/jsDUGbzWZ2Q2dtQsz4LTN7K/hv+kszG9TFZ7v9ffRBzq+b2a64/7ZXdPHZbrcLfZDzZ3EZt5nZG118ti/XZ6fbodB+n+6edi9iXam/DUwCioA3gWkd2vw98EAwfA3wsxTkHAXMCobLgE2d5JwP/CYN1uk2YFg3868AfgcYcAHwSor/++8BxqfLugQ+AMwC1sZN+yZwZzB8J3BfJ58bAmwN3gcHw4P7MONlQEEwfF9nGRP5ffRBzq8DX0rgd9HtdiHsnB3mfxu4Jw3WZ6fbobB+n+m6pzEH2OLuW929BXgcWNShzSLgkWD4SeCDZmZ9mBF3f9fdVwXD9cAGYExfZkiiRcCPPeZlYJCZjUpRlg8Cb7v79hR9/wnc/QViz3yJF/8bfAT4q04++mHgD+5+0N0PAX8AFvZVRnf/vbu3BaMvE3uCZkp1sS4Tkch2IWm6yxlsaz4J/DSs709UN9uhUH6f6Vo0xgA748arOXFjfKxN8D9FLTC0T9J1Ijg8NhN4pZPZ7zOzN83sd2Y2vU+DHefA781spZkt7mR+Iuu8r1xD1/8zpsO6bDfC3d8NhvcAIzppk07r9bPE9iY709Pvoy/cGhxGe6iLQynptC7fD+x1981dzE/J+uywHQrl95muRSOjmFkp8HPgNnev6zB7FbHDLDOAfwOe6ut8gYvdfRZwOfB5M/tAinJ0y2KPBr4KeKKT2emyLk/gsX39tL1+3czuAtqAx7pokurfx78DpwHnAu8SO/STzq6l+72MPl+f3W2Hkvn7TNeisQsYGzdeGUzrtI2ZFQDlQE2fpItjZoXE/kM95u6/6Djf3evcvSEYfhooNLNhfRwTd98VvO8DfklsVz9eIuu8L1wOrHL3vR1npMu6jLO3/RBe8L6vkzYpX69m9mngI8B1wcbjBAn8PkLl7nvdPeLuUeA/uvj+lK9LOLa9+WvgZ1216ev12cV2KJTfZ7oWjdeAyWY2MfiX5zXAsg5tlgHtZ/qvBv7U1f8QYQmOa/4I2ODu3+mizcj2cy1mNofYOu/T4mZmA8ysrH2Y2MnRtR2aLQM+ZTEXALVxu7Z9qct/waXDuuwg/jd4A/CrTto8C1xmZoODQy6XBdP6hJktBP4BuMrdG7tok8jvI1Qdzp99rIvvT2S70BcuBd5y9+rOZvb1+uxmOxTO77Mvzu6f4hUBVxC7CuBt4K5g2r3EfvwAJcQOYWwBXgUmpSDjxcR2+VYDbwSvK4BbgFuCNrcC64hd6fEycGEKck4Kvv/NIEv7+ozPacD9wfpeA5yXgpwDiBWB8rhpabEuiRWyd4FWYsd9byR2Du2PwGbgOWBI0PY84D/jPvvZ4He6BfhMH2fcQuyYdfvvs/2Kw9HA0939Pvo456PB7241sY3dqI45g/ETtgt9mTOY/nD7bzKubSrXZ1fboVB+n+pGREREEpauh6dERCQNqWiIiEjCVDRERCRhKhoiIpIwFQ0REUmYioZkLDOL2Ht7xg2t11Mzu9fMLg1r+Z1831+Z2bS++j6RROmSW8lYZtbg7qU9tMl390hX44l+rq+Z2cPEevR98iQ+U+DHOycUCYX2NCTrBM8yuM/MVgGf6GT82uBZB2vN7L64zzWY2bfN7E3gfR2W+bCZXR23/H80s1XBcqZ2kuHTZvZU8ByDbWZ2q5l90cxeN7OXzWxI0O40M3sm6NjuRTObamYXEut/61vBHtRpnbWLy/WAmb0CfNPM5sXteb3efmeySLIUpDqASC/0s/c+BOdf3b29P6Aaj3UYh5l9o33czEYTu5t8NnCIWE+kf+XuTxG7I/0Vd789ge8+ECzv74EvATd10uYsYj2OlhC72/YOd59pZt8FPgV8D1hC7O7izWY2F/ihu19iZsuI29Mwsz92bAdcEnxPJbG74yNm9mvg8+7+UtCBXVMCf4tIwlQ0JJMddfdzu5jXsTO59vHzgSp33w9gZo8Re9jOU0CEWKdviWjvFG4lsc7rOrPcY883qDezWuDXwfQ1wDnBRv1C4Ak7/iiY4o4LSaDdE3GH0l4CvhP8Xb/wLvpHEjlVKhqSrY70MN6ZppM4j9EcvEfo+v+j5rjhaNx4NPhMHnC4m8LXrqd2x/42d/+Gmf2WWN9DL5nZh939rR6WL5IwndOQXPMqMM/MhplZPrFedZ9PRRCPPfPgHTP7BBx7TvuMYHY9sUd39tTuPczsNHdf4+73EesV9oTzLSK9oaIhmaxfh0tuv9HTBzzW3fudwHJivZCudPfOuozuK9cBNwYn39dx/PGljwNfDk5mn9ZNu45uC07wrybWO2tXT+oTOSW65FZERBKmPQ0REUmYioaIiCRMRUNERBKmoiEiIglT0RARkYSpaIiISMJUNEREJGH/H276bVj9Gz1hAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lePWwt2IKam4",
        "outputId": "556c19b2-87f4-4f7a-e8c1-696bebbf2317"
      },
      "source": [
        "print(np.mean(norm_error))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.765717379489687\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlaB2iBjKam5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff193d55-fc80-4ac4-8228-aac992467b9b"
      },
      "source": [
        "cdf = ECDF(norm_error/1)\n",
        "y = cdf.y\n",
        "indx = cdf.x[y>0.8]\n",
        "#print(\" {0}\":)\n",
        "print(\"The 80% percentile error in meters is \", indx[0], end='')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The 80% percentile error in meters is  3.2769808585927205"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jYOTvhAtvMb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}